{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48fa5964-ad12-476e-a53a-0aca42286521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "981098ca-b627-4d09-9766-13cc5d8d5ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_features = 520"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5251f59f-0760-4b87-8763-256d3559fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 1: Load data ===\n",
    "df = pd.read_excel('1. Dataset UJIIndoorloc.xlsx')  # Replace with your actual filename\n",
    "# Load test data\n",
    "DT = pd.read_excel('1. Datatest UJIIndoorloc.xlsx')  # unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38ea8bd-e082-4a91-a9b5-e252fadc255a",
   "metadata": {},
   "source": [
    "# Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e18d970c-738d-49f1-9634-9267bd3dfa0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\4252080208.py:12: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[rssi_columns] = df[rssi_columns].applymap(lambda x: x - shift if x < 0 else 0)\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\4252080208.py:16: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[rssi_columns] = df[rssi_columns].div(row_max, axis=0).fillna(0)\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\4252080208.py:21: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[rssi_columns] = df[rssi_columns].applymap(\n"
     ]
    }
   ],
   "source": [
    "# Sigmoid-Scaled Row Normalization\n",
    "\n",
    "# === Step 1: Select RSSI columns ===\n",
    "rssi_columns = df.columns[:raw_features]\n",
    "\n",
    "# === Step 2: Replace 100 (no signal) with 0 ===\n",
    "df[rssi_columns] = df[rssi_columns].replace(100, 0)\n",
    "\n",
    "# === Step 3: Shift RSSI to positive (only non-zero) ===\n",
    "rssi_min = df[rssi_columns][df[rssi_columns] < 0].min().min()\n",
    "shift = rssi_min - 1  # shift negative RSSI to positive\n",
    "df[rssi_columns] = df[rssi_columns].applymap(lambda x: x - shift if x < 0 else 0)\n",
    "\n",
    "# === Step 4: Row-wise normalization (exclude zeros) ===\n",
    "row_max = df[rssi_columns].replace(0, pd.NA).max(axis=1)\n",
    "df[rssi_columns] = df[rssi_columns].div(row_max, axis=0).fillna(0)\n",
    "\n",
    "# === Step 5: Apply sigmoid transformation to non-zero values ===\n",
    "alpha = 10  # steepness of sigmoid\n",
    "beta = 0.5  # midpoint\n",
    "df[rssi_columns] = df[rssi_columns].applymap(\n",
    "    lambda x: 1 / (1 + np.exp(-alpha * (x - beta))) if x > 0 else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8218625f-665f-4ad6-9187-581fec4e50e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\4082890510.py:9: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  DT[rssi_columns] = DT[rssi_columns].applymap(lambda x: x - shift if x < 0 else 0)\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\4082890510.py:18: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  DT[rssi_columns] = DT[rssi_columns].applymap(\n"
     ]
    }
   ],
   "source": [
    "# Sigmoid-Scaled Row Normalization\n",
    "\n",
    "# === Step 1: Replace 100 (no signal) with 0 ===\n",
    "DT[rssi_columns] = DT[rssi_columns].replace(100, 0)\n",
    "\n",
    "# === Step 2: Shift RSSI using dataset-wide min (from training) ===\n",
    "shift = rssi_min - 1\n",
    "DT[rssi_columns] = DT[rssi_columns].applymap(lambda x: x - shift if x < 0 else 0)\n",
    "\n",
    "# === Step 3: Row-wise normalization (exclude zeros) ===\n",
    "row_max = DT[rssi_columns].replace(0, np.nan).max(axis=1)\n",
    "DT[rssi_columns] = DT[rssi_columns].div(row_max, axis=0).fillna(0)\n",
    "\n",
    "# === Step 4: Apply sigmoid transformation ===\n",
    "alpha = 10  # steepness of the sigmoid\n",
    "beta = 0.5  # midpoint\n",
    "DT[rssi_columns] = DT[rssi_columns].applymap(\n",
    "    lambda x: 1 / (1 + np.exp(-alpha * (x - beta))) if x > 0 else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a023718-cc21-477d-802a-804726c57e95",
   "metadata": {},
   "source": [
    "# Marking Floor ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a3c1784-07a1-4cef-8e81-4600c4d96bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1629925467.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['FLOOR_ID'] = order_\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WAP001</th>\n",
       "      <th>WAP002</th>\n",
       "      <th>WAP003</th>\n",
       "      <th>WAP004</th>\n",
       "      <th>WAP005</th>\n",
       "      <th>WAP006</th>\n",
       "      <th>WAP007</th>\n",
       "      <th>WAP008</th>\n",
       "      <th>WAP009</th>\n",
       "      <th>WAP010</th>\n",
       "      <th>...</th>\n",
       "      <th>WAP517</th>\n",
       "      <th>WAP518</th>\n",
       "      <th>WAP519</th>\n",
       "      <th>WAP520</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>FLOOR</th>\n",
       "      <th>BUILDINGID</th>\n",
       "      <th>Order</th>\n",
       "      <th>FLOOR_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.298666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-7685.7235</td>\n",
       "      <td>4.864930e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.197816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-7685.7235</td>\n",
       "      <td>4.864930e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.242989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-7685.7235</td>\n",
       "      <td>4.864930e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.222700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-7685.7235</td>\n",
       "      <td>4.864930e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.252192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-7685.7235</td>\n",
       "      <td>4.864930e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19932</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.841131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-7309.5175</td>\n",
       "      <td>4.864813e+06</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>932</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19933</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.841131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-7309.5175</td>\n",
       "      <td>4.864813e+06</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>932</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19934</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.841131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-7309.5175</td>\n",
       "      <td>4.864813e+06</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>932</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19935</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.841131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-7309.5175</td>\n",
       "      <td>4.864813e+06</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>932</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19936</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.841131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-7309.5175</td>\n",
       "      <td>4.864813e+06</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>932</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19937 rows × 526 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       WAP001  WAP002  WAP003  WAP004  WAP005  WAP006    WAP007  WAP008  \\\n",
       "0         0.0     0.0       0       0     0.0     0.0  0.298666     0.0   \n",
       "1         0.0     0.0       0       0     0.0     0.0  0.197816     0.0   \n",
       "2         0.0     0.0       0       0     0.0     0.0  0.242989     0.0   \n",
       "3         0.0     0.0       0       0     0.0     0.0  0.222700     0.0   \n",
       "4         0.0     0.0       0       0     0.0     0.0  0.252192     0.0   \n",
       "...       ...     ...     ...     ...     ...     ...       ...     ...   \n",
       "19932     0.0     0.0       0       0     0.0     0.0  0.000000     0.0   \n",
       "19933     0.0     0.0       0       0     0.0     0.0  0.000000     0.0   \n",
       "19934     0.0     0.0       0       0     0.0     0.0  0.000000     0.0   \n",
       "19935     0.0     0.0       0       0     0.0     0.0  0.000000     0.0   \n",
       "19936     0.0     0.0       0       0     0.0     0.0  0.000000     0.0   \n",
       "\n",
       "       WAP009  WAP010  ...    WAP517  WAP518  WAP519  WAP520  LONGITUDE  \\\n",
       "0         0.0     0.0  ...  0.000000     0.0     0.0       0 -7685.7235   \n",
       "1         0.0     0.0  ...  0.000000     0.0     0.0       0 -7685.7235   \n",
       "2         0.0     0.0  ...  0.000000     0.0     0.0       0 -7685.7235   \n",
       "3         0.0     0.0  ...  0.000000     0.0     0.0       0 -7685.7235   \n",
       "4         0.0     0.0  ...  0.000000     0.0     0.0       0 -7685.7235   \n",
       "...       ...     ...  ...       ...     ...     ...     ...        ...   \n",
       "19932     0.0     0.0  ...  0.841131     0.0     0.0       0 -7309.5175   \n",
       "19933     0.0     0.0  ...  0.841131     0.0     0.0       0 -7309.5175   \n",
       "19934     0.0     0.0  ...  0.841131     0.0     0.0       0 -7309.5175   \n",
       "19935     0.0     0.0  ...  0.841131     0.0     0.0       0 -7309.5175   \n",
       "19936     0.0     0.0  ...  0.841131     0.0     0.0       0 -7309.5175   \n",
       "\n",
       "           LATITUDE  FLOOR  BUILDINGID  Order  FLOOR_ID  \n",
       "0      4.864930e+06      0           0      1         0  \n",
       "1      4.864930e+06      0           0      1         0  \n",
       "2      4.864930e+06      0           0      1         0  \n",
       "3      4.864930e+06      0           0      1         0  \n",
       "4      4.864930e+06      0           0      1         0  \n",
       "...             ...    ...         ...    ...       ...  \n",
       "19932  4.864813e+06      4           2    932        12  \n",
       "19933  4.864813e+06      4           2    932        12  \n",
       "19934  4.864813e+06      4           2    932        12  \n",
       "19935  4.864813e+06      4           2    932        12  \n",
       "19936  4.864813e+06      4           2    932        12  \n",
       "\n",
       "[19937 rows x 526 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialization\n",
    "order = 0\n",
    "building_id = 0\n",
    "floor_id = 0\n",
    "order_ = []\n",
    "\n",
    "# Loop through the sorted DataFrame\n",
    "for i in range(len(df)):\n",
    "    # Check if the current coordinates are the same as the previous coordinates\n",
    "    if df.iloc[i, -2] == building_id and df.iloc[i, -3] == floor_id:\n",
    "        # Append the same order if the coordinates are the same\n",
    "        order_.append(order)\n",
    "    else:\n",
    "        # If new coordinates, increment the order and update x, y, z\n",
    "        building_id = df.iloc[i, -2]\n",
    "        floor_id = df.iloc[i, -3]\n",
    "        order += 1\n",
    "        order_.append(order)\n",
    "\n",
    "# Adding the computed order back to the DataFrame\n",
    "df['FLOOR_ID'] = order_\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5edccab7-2b9e-4b11-b137-f15898c4077b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1386778111.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DT['FLOOR_ID'] = order_\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WAP001</th>\n",
       "      <th>WAP002</th>\n",
       "      <th>WAP003</th>\n",
       "      <th>WAP004</th>\n",
       "      <th>WAP005</th>\n",
       "      <th>WAP006</th>\n",
       "      <th>WAP007</th>\n",
       "      <th>WAP008</th>\n",
       "      <th>WAP009</th>\n",
       "      <th>WAP010</th>\n",
       "      <th>...</th>\n",
       "      <th>WAP516</th>\n",
       "      <th>WAP517</th>\n",
       "      <th>WAP518</th>\n",
       "      <th>WAP519</th>\n",
       "      <th>WAP520</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>FLOOR</th>\n",
       "      <th>BUILDINGID</th>\n",
       "      <th>FLOOR_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7680.015484</td>\n",
       "      <td>4.864931e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7639.400495</td>\n",
       "      <td>4.864913e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7632.777600</td>\n",
       "      <td>4.864965e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7607.076600</td>\n",
       "      <td>4.864978e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7592.953185</td>\n",
       "      <td>4.864984e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7352.064178</td>\n",
       "      <td>4.864835e+06</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7337.775408</td>\n",
       "      <td>4.864827e+06</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7324.144263</td>\n",
       "      <td>4.864818e+06</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7316.901654</td>\n",
       "      <td>4.864815e+06</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7346.611346</td>\n",
       "      <td>4.864756e+06</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1111 rows × 525 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      WAP001  WAP002  WAP003  WAP004  WAP005  WAP006  WAP007  WAP008  WAP009  \\\n",
       "0        0.0       0     0.0     0.0       0       0       0     0.0     0.0   \n",
       "1        0.0       0     0.0     0.0       0       0       0     0.0     0.0   \n",
       "2        0.0       0     0.0     0.0       0       0       0     0.0     0.0   \n",
       "3        0.0       0     0.0     0.0       0       0       0     0.0     0.0   \n",
       "4        0.0       0     0.0     0.0       0       0       0     0.0     0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "1106     0.0       0     0.0     0.0       0       0       0     0.0     0.0   \n",
       "1107     0.0       0     0.0     0.0       0       0       0     0.0     0.0   \n",
       "1108     0.0       0     0.0     0.0       0       0       0     0.0     0.0   \n",
       "1109     0.0       0     0.0     0.0       0       0       0     0.0     0.0   \n",
       "1110     0.0       0     0.0     0.0       0       0       0     0.0     0.0   \n",
       "\n",
       "      WAP010  ...  WAP516  WAP517  WAP518  WAP519  WAP520    LONGITUDE  \\\n",
       "0        0.0  ...       0       0       0       0     0.0 -7680.015484   \n",
       "1        0.0  ...       0       0       0       0     0.0 -7639.400495   \n",
       "2        0.0  ...       0       0       0       0     0.0 -7632.777600   \n",
       "3        0.0  ...       0       0       0       0     0.0 -7607.076600   \n",
       "4        0.0  ...       0       0       0       0     0.0 -7592.953185   \n",
       "...      ...  ...     ...     ...     ...     ...     ...          ...   \n",
       "1106     0.0  ...       0       0       0       0     0.0 -7352.064178   \n",
       "1107     0.0  ...       0       0       0       0     0.0 -7337.775408   \n",
       "1108     0.0  ...       0       0       0       0     0.0 -7324.144263   \n",
       "1109     0.0  ...       0       0       0       0     0.0 -7316.901654   \n",
       "1110     0.0  ...       0       0       0       0     0.0 -7346.611346   \n",
       "\n",
       "          LATITUDE  FLOOR  BUILDINGID  FLOOR_ID  \n",
       "0     4.864931e+06      0           0         0  \n",
       "1     4.864913e+06      0           0         0  \n",
       "2     4.864965e+06      0           0         0  \n",
       "3     4.864978e+06      0           0         0  \n",
       "4     4.864984e+06      0           0         0  \n",
       "...            ...    ...         ...       ...  \n",
       "1106  4.864835e+06      4           2        12  \n",
       "1107  4.864827e+06      4           2        12  \n",
       "1108  4.864818e+06      4           2        12  \n",
       "1109  4.864815e+06      4           2        12  \n",
       "1110  4.864756e+06      4           2        12  \n",
       "\n",
       "[1111 rows x 525 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialization\n",
    "order = 0\n",
    "building_id = 0\n",
    "floor_id = 0\n",
    "order_ = []\n",
    "\n",
    "# Loop through the sorted DataFrame\n",
    "for i in range(len(DT)):\n",
    "    # Check if the current coordinates are the same as the previous coordinates\n",
    "    if DT.iloc[i, -1] == building_id and DT.iloc[i, -2] == floor_id:\n",
    "        # Append the same order if the coordinates are the same\n",
    "        order_.append(order)\n",
    "    else:\n",
    "        # If new coordinates, increment the order and update x, y, z\n",
    "        building_id = DT.iloc[i, -1]\n",
    "        floor_id = DT.iloc[i, -2]\n",
    "        order += 1\n",
    "        order_.append(order)\n",
    "\n",
    "# Adding the computed order back to the DataFrame\n",
    "DT['FLOOR_ID'] = order_\n",
    "DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e55f99d-8839-426d-95ec-ba7883ee716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:520]\n",
    "X_target = DT.iloc[:,:520]\n",
    "\n",
    "y_building = df.iloc[:, -3]\n",
    "y_floor = df.iloc[:, -1]\n",
    "y_coordinate = df.iloc[:, [-6,-5]]\n",
    "y_target_building = DT.iloc[:,-2]\n",
    "y_target_floor = DT.iloc[:,-1]\n",
    "y_target_coordinate = DT.iloc[:, [-5,-4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c0770c-b27c-428d-9a9f-cc21b91c36ba",
   "metadata": {},
   "source": [
    "# Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b4142e8-748d-430a-8171-fccf933d9f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KMITL\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\KMITL\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "# Confidence weighting using Pearson correlation\n",
    "\n",
    "# Step 6.1: Compute Pearson correlations between each feature and target\n",
    "correlations = X.apply(lambda x: x.corr(y_floor), axis=0)\n",
    "\n",
    "# Step 6.2: Replace NaN with 0 (e.g. for constant columns)\n",
    "correlations = correlations.fillna(0)\n",
    "\n",
    "# Step 6.3: Compute confidence weights (e.g., absolute correlation)\n",
    "confidence_weights = correlations.abs()\n",
    "\n",
    "# Optional: scale weights to [0, 1]\n",
    "confidence_weights = confidence_weights / confidence_weights.max()\n",
    "\n",
    "# Step 6.4: Multiply each feature by its confidence weight\n",
    "X_weighted = X.mul(confidence_weights, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec2948bb-1162-4d98-9d33-6d8aa673f39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WAP001</th>\n",
       "      <th>WAP002</th>\n",
       "      <th>WAP005</th>\n",
       "      <th>WAP006</th>\n",
       "      <th>WAP007</th>\n",
       "      <th>WAP008</th>\n",
       "      <th>WAP009</th>\n",
       "      <th>WAP010</th>\n",
       "      <th>WAP011</th>\n",
       "      <th>WAP012</th>\n",
       "      <th>...</th>\n",
       "      <th>WAP510</th>\n",
       "      <th>WAP511</th>\n",
       "      <th>WAP512</th>\n",
       "      <th>WAP513</th>\n",
       "      <th>WAP514</th>\n",
       "      <th>WAP515</th>\n",
       "      <th>WAP516</th>\n",
       "      <th>WAP517</th>\n",
       "      <th>WAP518</th>\n",
       "      <th>WAP519</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.143402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.094980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.116669</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19932</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.544021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.810542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19933</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.544021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.810542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19934</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.544021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.810542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19935</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.544021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.810542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19936</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.544021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.810542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19937 rows × 465 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       WAP001  WAP002  WAP005  WAP006    WAP007  WAP008  WAP009  WAP010  \\\n",
       "0         0.0     0.0     0.0     0.0  0.143402     0.0     0.0     0.0   \n",
       "1         0.0     0.0     0.0     0.0  0.094980     0.0     0.0     0.0   \n",
       "2         0.0     0.0     0.0     0.0  0.116669     0.0     0.0     0.0   \n",
       "3         0.0     0.0     0.0     0.0  0.106927     0.0     0.0     0.0   \n",
       "4         0.0     0.0     0.0     0.0  0.121088     0.0     0.0     0.0   \n",
       "...       ...     ...     ...     ...       ...     ...     ...     ...   \n",
       "19932     0.0     0.0     0.0     0.0  0.000000     0.0     0.0     0.0   \n",
       "19933     0.0     0.0     0.0     0.0  0.000000     0.0     0.0     0.0   \n",
       "19934     0.0     0.0     0.0     0.0  0.000000     0.0     0.0     0.0   \n",
       "19935     0.0     0.0     0.0     0.0  0.000000     0.0     0.0     0.0   \n",
       "19936     0.0     0.0     0.0     0.0  0.000000     0.0     0.0     0.0   \n",
       "\n",
       "       WAP011  WAP012  ...  WAP510    WAP511  WAP512  WAP513  WAP514  WAP515  \\\n",
       "0         0.0     0.0  ...     0.0  0.000000     0.0     0.0     0.0     0.0   \n",
       "1         0.0     0.0  ...     0.0  0.000000     0.0     0.0     0.0     0.0   \n",
       "2         0.0     0.0  ...     0.0  0.000000     0.0     0.0     0.0     0.0   \n",
       "3         0.0     0.0  ...     0.0  0.000000     0.0     0.0     0.0     0.0   \n",
       "4         0.0     0.0  ...     0.0  0.000000     0.0     0.0     0.0     0.0   \n",
       "...       ...     ...  ...     ...       ...     ...     ...     ...     ...   \n",
       "19932     0.0     0.0  ...     0.0  0.544021     0.0     0.0     0.0     0.0   \n",
       "19933     0.0     0.0  ...     0.0  0.544021     0.0     0.0     0.0     0.0   \n",
       "19934     0.0     0.0  ...     0.0  0.544021     0.0     0.0     0.0     0.0   \n",
       "19935     0.0     0.0  ...     0.0  0.544021     0.0     0.0     0.0     0.0   \n",
       "19936     0.0     0.0  ...     0.0  0.544021     0.0     0.0     0.0     0.0   \n",
       "\n",
       "       WAP516    WAP517  WAP518  WAP519  \n",
       "0         0.0  0.000000     0.0     0.0  \n",
       "1         0.0  0.000000     0.0     0.0  \n",
       "2         0.0  0.000000     0.0     0.0  \n",
       "3         0.0  0.000000     0.0     0.0  \n",
       "4         0.0  0.000000     0.0     0.0  \n",
       "...       ...       ...     ...     ...  \n",
       "19932     0.0  0.810542     0.0     0.0  \n",
       "19933     0.0  0.810542     0.0     0.0  \n",
       "19934     0.0  0.810542     0.0     0.0  \n",
       "19935     0.0  0.810542     0.0     0.0  \n",
       "19936     0.0  0.810542     0.0     0.0  \n",
       "\n",
       "[19937 rows x 465 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confidence weighting using Pearson correlation\n",
    "\n",
    "# Step 6.5: Remove all-zero columns (optional cleanup)\n",
    "X_weighted_cleaned = X_weighted.loc[:, (X_weighted != 0).any(axis=0)]\n",
    "\n",
    "# Step 6.6: Update feature count\n",
    "raw_features = X_weighted_cleaned.shape[1]\n",
    "\n",
    "# Result: X_weighted_cleaned has transformed, normalized, and confidence-weighted features\n",
    "X_weighted_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21964dd8-03b7-4505-989c-d6d463a3c091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WAP001</th>\n",
       "      <th>WAP002</th>\n",
       "      <th>WAP005</th>\n",
       "      <th>WAP006</th>\n",
       "      <th>WAP007</th>\n",
       "      <th>WAP008</th>\n",
       "      <th>WAP009</th>\n",
       "      <th>WAP010</th>\n",
       "      <th>WAP011</th>\n",
       "      <th>WAP012</th>\n",
       "      <th>...</th>\n",
       "      <th>WAP510</th>\n",
       "      <th>WAP511</th>\n",
       "      <th>WAP512</th>\n",
       "      <th>WAP513</th>\n",
       "      <th>WAP514</th>\n",
       "      <th>WAP515</th>\n",
       "      <th>WAP516</th>\n",
       "      <th>WAP517</th>\n",
       "      <th>WAP518</th>\n",
       "      <th>WAP519</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.507841</td>\n",
       "      <td>0.507425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054563</td>\n",
       "      <td>0.044950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1111 rows × 465 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      WAP001  WAP002  WAP005  WAP006  WAP007  WAP008  WAP009  WAP010  \\\n",
       "0        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "1106     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1107     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1108     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1109     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1110     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "        WAP011    WAP012  ...  WAP510  WAP511  WAP512  WAP513  WAP514  WAP515  \\\n",
       "0     0.000000  0.000000  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.000000  0.000000  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.000000  0.000000  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.000000  0.000000  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.000000  0.000000  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...        ...       ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "1106  0.507841  0.507425  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1107  0.054563  0.044950  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1108  0.000000  0.000000  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1109  0.000000  0.000000  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1110  0.000000  0.000000  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      WAP516  WAP517  WAP518  WAP519  \n",
       "0        0.0     0.0     0.0     0.0  \n",
       "1        0.0     0.0     0.0     0.0  \n",
       "2        0.0     0.0     0.0     0.0  \n",
       "3        0.0     0.0     0.0     0.0  \n",
       "4        0.0     0.0     0.0     0.0  \n",
       "...      ...     ...     ...     ...  \n",
       "1106     0.0     0.0     0.0     0.0  \n",
       "1107     0.0     0.0     0.0     0.0  \n",
       "1108     0.0     0.0     0.0     0.0  \n",
       "1109     0.0     0.0     0.0     0.0  \n",
       "1110     0.0     0.0     0.0     0.0  \n",
       "\n",
       "[1111 rows x 465 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confidence weighting using Pearson correlation\n",
    "\n",
    "# Step 2: Apply the same weights to X_target\n",
    "X_target_weighted = X_target.mul(confidence_weights, axis=1)\n",
    "\n",
    "# Step 3: Keep only the columns that were kept in training\n",
    "X_target_weighted_cleaned = X_target_weighted[X_weighted_cleaned.columns]\n",
    "\n",
    "X_target_weighted_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82a81309-56ff-4162-9ed2-c0519692c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_weighted_cleaned\n",
    "X_target = X_target_weighted_cleaned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f57ee4f-3437-43bd-b372-eb16752b192d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "#raw_features = 992\n",
    "#df = pd.read_excel('2. Dataset Tampere.xlsx')\n",
    "#X = df.iloc[:, :raw_features]\n",
    "\n",
    "# Flatten all values into a single array\n",
    "all_values = X.values.flatten()\n",
    "\n",
    "# Remove NaNs and values equal to 100\n",
    "all_values = all_values[~pd.isnull(all_values)]\n",
    "all_values = all_values[all_values != 0]\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Histogram\n",
    "ax.hist(all_values, bins=80, color='lightcoral', edgecolor='black', zorder=1)\n",
    "\n",
    "# Grid behind\n",
    "ax.grid(True, linestyle='--', linewidth=0.6, zorder=1)\n",
    "\n",
    "# Titles and labels\n",
    "ax.set_title(\"UJIIndoorLoc\", fontsize=24)\n",
    "ax.set_xlabel(\"Transformed RSS\", fontsize=20)\n",
    "ax.set_ylabel(\"Frequency\", fontsize=20)\n",
    "\n",
    "# ✅ Adjust tick font sizes\n",
    "ax.tick_params(axis='both', labelsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Transformed UJIIndoorLoc distribution.svg', dpi = 600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2ffb80-6be9-4751-893f-0315b24e7181",
   "metadata": {},
   "source": [
    "# Building Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20eab841-155b-46f7-bd64-b0b98a88516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import random\n",
    "import numpy as np\n",
    "from optuna.exceptions import TrialPruned\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a0fec41-0a23-47ed-83c7-0db82ca85678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train-test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_building, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Fix random seeds for reproducibility ---\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a828e0ba-ba93-42fb-be71-c4d28e3f2194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time1 = time.time()\n",
    "\n",
    "# --- Custom weight function for KNN ---\n",
    "def knn_weight(d):\n",
    "    return 1 / (d + 0.000001) ** 2\n",
    "\n",
    "\n",
    "# --- Objective function for KNN ---\n",
    "def objective_wknn(trial):\n",
    "    n_neighbors = trial.suggest_int('n_neighbors', 1, 50)\n",
    "    p = trial.suggest_int('p', 1, 5)\n",
    "    metric = trial.suggest_categorical('metric', ['minkowski', 'euclidean', 'manhattan'])\n",
    "\n",
    "    knn = KNeighborsClassifier(\n",
    "        n_neighbors=n_neighbors,\n",
    "        weights=knn_weight,\n",
    "        p=p,\n",
    "        metric=metric\n",
    "    )\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        knn.fit(X_tr, y_tr)\n",
    "        y_pred = knn.predict(X_val)\n",
    "        score = accuracy_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "\n",
    "        trial.report(np.mean(scores), step=fold)\n",
    "        if trial.should_prune():\n",
    "            raise TrialPruned()\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# --- Objective function for RandomForest ---\n",
    "def objective_rf(trial):\n",
    "    max_depth = trial.suggest_categorical('max_depth', [None] + list(range(5, 51)))\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=n_estimators,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        rf.fit(X_tr, y_tr)\n",
    "        y_pred = rf.predict(X_val)\n",
    "        score = accuracy_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "\n",
    "        trial.report(np.mean(scores), step=fold)\n",
    "        if trial.should_prune():\n",
    "            raise TrialPruned()\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# --- Objective function for LightGBM (with pruning callback) ---\n",
    "def objective_lgb(trial):\n",
    "    param = {\n",
    "    'n_estimators': 10000,\n",
    "    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5, log=True),\n",
    "    'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "    'max_depth': trial.suggest_int('max_depth', 3, 30),\n",
    "    'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "    'random_state': SEED,\n",
    "    'n_jobs': -1,\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': len(np.unique(y_train)),\n",
    "    'verbosity': -1}\n",
    "\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    model = lgb.LGBMClassifier(**param)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='multi_error',\n",
    "            callbacks=[lgb.early_stopping(100)],\n",
    "        )\n",
    "\n",
    "        y_pred = model.predict(X_val)\n",
    "        score = accuracy_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "\n",
    "        # Report accuracy to Optuna for pruning\n",
    "        trial.report(np.mean(scores), step=fold)\n",
    "        if trial.should_prune():\n",
    "            raise TrialPruned()\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "# --- Objective function for XGBoost ---\n",
    "def objective_xgb(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'use_label_encoder': False,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'random_state': SEED,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**param)\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "        score = accuracy_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "\n",
    "        trial.report(np.mean(scores), step=fold)\n",
    "        if trial.should_prune():\n",
    "            raise TrialPruned()\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# --- Objective function for GaussianNB ---\n",
    "def objective_gnb(trial):\n",
    "    param = {\n",
    "        'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
    "    }\n",
    "\n",
    "    model = GaussianNB(**param)\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "        score = accuracy_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "\n",
    "        trial.report(np.mean(scores), step=fold)\n",
    "        if trial.should_prune():\n",
    "            raise TrialPruned()\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from optuna.exceptions import TrialPruned\n",
    "\n",
    "# --- Objective function for AdaBoost ---\n",
    "def objective_adaboost(trial):\n",
    "    base_estimator_params = {\n",
    "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "        'random_state': SEED\n",
    "    }\n",
    "\n",
    "    # AdaBoost-specific hyperparameters\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 1.0, log=True)\n",
    "\n",
    "    base_estimator = DecisionTreeClassifier(**base_estimator_params)\n",
    "\n",
    "    model = AdaBoostClassifier(\n",
    "        estimator=base_estimator,\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "        score = accuracy_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "\n",
    "        trial.report(np.mean(scores), step=fold)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9843d26a-e17b-469b-a69c-c38f02af21e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:01:06,359] A new study created in memory with name: no-name-2b9788f0-a3ac-4d0c-8741-03289e7070ec\n",
      "[I 2025-12-22 09:01:14,277] Trial 0 finished with value: 0.9976173975784708 and parameters: {'n_neighbors': 14, 'p': 3, 'metric': 'manhattan'}. Best is trial 0 with value: 0.9976173975784708.\n",
      "[I 2025-12-22 09:01:15,725] Trial 1 finished with value: 0.9954229222513519 and parameters: {'n_neighbors': 36, 'p': 5, 'metric': 'euclidean'}. Best is trial 0 with value: 0.9976173975784708.\n",
      "[I 2025-12-22 09:02:33,375] Trial 2 finished with value: 0.9946705514941152 and parameters: {'n_neighbors': 45, 'p': 4, 'metric': 'minkowski'}. Best is trial 0 with value: 0.9976173975784708.\n",
      "[I 2025-12-22 09:02:34,817] Trial 3 finished with value: 0.9961126167438815 and parameters: {'n_neighbors': 27, 'p': 4, 'metric': 'euclidean'}. Best is trial 0 with value: 0.9976173975784708.\n",
      "[I 2025-12-22 09:02:42,936] Trial 4 finished with value: 0.9970530949354709 and parameters: {'n_neighbors': 39, 'p': 3, 'metric': 'manhattan'}. Best is trial 0 with value: 0.9976173975784708.\n",
      "[I 2025-12-22 09:02:44,462] Trial 5 finished with value: 0.9964888316125867 and parameters: {'n_neighbors': 18, 'p': 4, 'metric': 'euclidean'}. Best is trial 0 with value: 0.9976173975784708.\n",
      "[I 2025-12-22 09:02:45,877] Trial 6 finished with value: 0.9973039179546461 and parameters: {'n_neighbors': 10, 'p': 4, 'metric': 'euclidean'}. Best is trial 0 with value: 0.9976173975784708.\n",
      "[I 2025-12-22 09:02:53,654] Trial 7 finished with value: 0.997993592787118 and parameters: {'n_neighbors': 7, 'p': 5, 'metric': 'manhattan'}. Best is trial 7 with value: 0.997993592787118.\n",
      "[I 2025-12-22 09:02:55,234] Trial 8 pruned. \n",
      "[I 2025-12-22 09:03:03,136] Trial 9 finished with value: 0.9969904186707639 and parameters: {'n_neighbors': 26, 'p': 4, 'metric': 'manhattan'}. Best is trial 7 with value: 0.997993592787118.\n",
      "[I 2025-12-22 09:03:10,902] Trial 10 finished with value: 0.9981817002214706 and parameters: {'n_neighbors': 1, 'p': 1, 'metric': 'manhattan'}. Best is trial 10 with value: 0.9981817002214706.\n",
      "[I 2025-12-22 09:03:18,649] Trial 11 finished with value: 0.9981817002214706 and parameters: {'n_neighbors': 1, 'p': 1, 'metric': 'manhattan'}. Best is trial 10 with value: 0.9981817002214706.\n",
      "[I 2025-12-22 09:03:26,376] Trial 12 finished with value: 0.9980562887118829 and parameters: {'n_neighbors': 5, 'p': 1, 'metric': 'manhattan'}. Best is trial 10 with value: 0.9981817002214706.\n",
      "[I 2025-12-22 09:03:34,074] Trial 13 finished with value: 0.9981817002214706 and parameters: {'n_neighbors': 1, 'p': 2, 'metric': 'manhattan'}. Best is trial 10 with value: 0.9981817002214706.\n",
      "[I 2025-12-22 09:03:41,822] Trial 14 finished with value: 0.9981817002214706 and parameters: {'n_neighbors': 1, 'p': 2, 'metric': 'manhattan'}. Best is trial 10 with value: 0.9981817002214706.\n",
      "[I 2025-12-22 09:03:44,962] Trial 15 pruned. \n",
      "[I 2025-12-22 09:03:52,960] Trial 16 finished with value: 0.9976800935032356 and parameters: {'n_neighbors': 11, 'p': 1, 'metric': 'minkowski'}. Best is trial 10 with value: 0.9981817002214706.\n",
      "[I 2025-12-22 09:03:56,076] Trial 17 pruned. \n",
      "[I 2025-12-22 09:04:03,844] Trial 18 finished with value: 0.9981189846366478 and parameters: {'n_neighbors': 6, 'p': 2, 'metric': 'manhattan'}. Best is trial 10 with value: 0.9981817002214706.\n",
      "[I 2025-12-22 09:04:11,563] Trial 19 finished with value: 0.9981817002214706 and parameters: {'n_neighbors': 1, 'p': 1, 'metric': 'minkowski'}. Best is trial 10 with value: 0.9981817002214706.\n",
      "[I 2025-12-22 09:04:13,137] Trial 20 pruned. \n",
      "[I 2025-12-22 09:04:20,868] Trial 21 finished with value: 0.9981817002214706 and parameters: {'n_neighbors': 1, 'p': 2, 'metric': 'manhattan'}. Best is trial 10 with value: 0.9981817002214706.\n",
      "[I 2025-12-22 09:04:28,612] Trial 22 finished with value: 0.9978681812775303 and parameters: {'n_neighbors': 9, 'p': 2, 'metric': 'manhattan'}. Best is trial 10 with value: 0.9981817002214706.\n",
      "[I 2025-12-22 09:04:36,347] Trial 23 finished with value: 0.9978681812775303 and parameters: {'n_neighbors': 13, 'p': 1, 'metric': 'manhattan'}. Best is trial 10 with value: 0.9981817002214706.\n",
      "[I 2025-12-22 09:04:44,093] Trial 24 finished with value: 0.9980562887118829 and parameters: {'n_neighbors': 4, 'p': 2, 'metric': 'manhattan'}. Best is trial 10 with value: 0.9981817002214706.\n",
      "[I 2025-12-22 09:04:45,668] Trial 25 pruned. \n",
      "[I 2025-12-22 09:04:53,393] Trial 26 finished with value: 0.9980562887118829 and parameters: {'n_neighbors': 4, 'p': 2, 'metric': 'manhattan'}. Best is trial 10 with value: 0.9981817002214706.\n",
      "[I 2025-12-22 09:04:54,993] Trial 27 pruned. \n",
      "[I 2025-12-22 09:04:55,253] Trial 28 pruned. \n",
      "[I 2025-12-22 09:05:10,807] Trial 29 pruned. \n",
      "[I 2025-12-22 09:05:13,921] Trial 30 pruned. \n",
      "[I 2025-12-22 09:05:21,816] Trial 31 finished with value: 0.9981817002214706 and parameters: {'n_neighbors': 1, 'p': 2, 'metric': 'manhattan'}. Best is trial 10 with value: 0.9981817002214706.\n",
      "[I 2025-12-22 09:05:29,567] Trial 32 finished with value: 0.9980562887118829 and parameters: {'n_neighbors': 3, 'p': 3, 'metric': 'manhattan'}. Best is trial 10 with value: 0.9981817002214706.\n",
      "[I 2025-12-22 09:05:35,767] Trial 33 pruned. \n",
      "[I 2025-12-22 09:05:43,484] Trial 34 finished with value: 0.9981817002214706 and parameters: {'n_neighbors': 1, 'p': 1, 'metric': 'manhattan'}. Best is trial 10 with value: 0.9981817002214706.\n",
      "[I 2025-12-22 09:05:43,750] Trial 35 pruned. \n",
      "[I 2025-12-22 09:05:45,317] Trial 36 pruned. \n",
      "[I 2025-12-22 09:05:45,581] Trial 37 pruned. \n",
      "[I 2025-12-22 09:05:47,168] Trial 38 pruned. \n",
      "[I 2025-12-22 09:06:02,674] Trial 39 pruned. \n",
      "[I 2025-12-22 09:06:05,784] Trial 40 pruned. \n",
      "[I 2025-12-22 09:06:13,492] Trial 41 finished with value: 0.9981817002214706 and parameters: {'n_neighbors': 1, 'p': 1, 'metric': 'minkowski'}. Best is trial 10 with value: 0.9981817002214706.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best KNN hyperparameters: {'n_neighbors': 1, 'p': 1, 'metric': 'manhattan'}\n",
      "KNN Cross-val Accuracies: [1.         0.99874608 0.99811912 0.99874608 0.99561129 0.99937304\n",
      " 0.99749216 0.9968652  0.99811912 0.99874529]\n",
      "KNN Mean CV Accuracy: 0.9981817395169189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['knn_building_UJI.joblib']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN\n",
    "# --- Run the optimization ---\n",
    "study1 = optuna.create_study(direction='maximize')\n",
    "study1.optimize(objective_wknn, n_trials=100, timeout=300)\n",
    "\n",
    "# --- Get best parameters ---\n",
    "best_params1 = study1.best_params\n",
    "print(\"Best KNN hyperparameters:\", best_params1)\n",
    "\n",
    "# --- Train final model ---\n",
    "knn_building_final = KNeighborsClassifier(\n",
    "    n_neighbors=best_params1['n_neighbors'],\n",
    "    weights=knn_weight,\n",
    "    p=best_params1['p'],\n",
    "    metric=best_params1['metric']\n",
    ")\n",
    "\n",
    "# --- Cross-validation accuracy ---\n",
    "cv_scores_knn = cross_val_score(knn_building_final, X_train, y_train, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "print(\"KNN Cross-val Accuracies:\", cv_scores_knn)\n",
    "print(\"KNN Mean CV Accuracy:\", cv_scores_knn.mean())\n",
    "\n",
    "# --- Train on full training set (optional, if you want to save the model) ---\n",
    "knn_building_final.fit(X_train, y_train)\n",
    "joblib.dump(knn_building_final, 'knn_building_UJI.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5269a659-0f71-4e18-9fa7-275c12075e6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:06:25,292] A new study created in memory with name: no-name-17acb02d-b65c-4a64-af42-83305bc58e2d\n",
      "[I 2025-12-22 09:06:31,955] Trial 0 finished with value: 0.9980562887118829 and parameters: {'max_depth': 45, 'n_estimators': 74, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:06:35,038] Trial 1 finished with value: 0.9910338929568825 and parameters: {'max_depth': 10, 'n_estimators': 224, 'min_samples_split': 2, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:06:53,587] Trial 2 finished with value: 0.9675215842861089 and parameters: {'max_depth': 9, 'n_estimators': 446, 'min_samples_split': 9, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:06:55,509] Trial 3 finished with value: 0.9963634790831728 and parameters: {'max_depth': 10, 'n_estimators': 134, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:07:00,430] Trial 4 finished with value: 0.9963633414627673 and parameters: {'max_depth': 31, 'n_estimators': 51, 'min_samples_split': 5, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:07:27,602] Trial 5 finished with value: 0.9966768800667655 and parameters: {'max_depth': 35, 'n_estimators': 387, 'min_samples_split': 4, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:07:31,897] Trial 6 pruned. \n",
      "[I 2025-12-22 09:07:59,782] Trial 7 finished with value: 0.9963633807828831 and parameters: {'max_depth': 37, 'n_estimators': 408, 'min_samples_split': 8, 'min_samples_leaf': 9, 'max_features': None}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:08:03,074] Trial 8 finished with value: 0.9969904186707638 and parameters: {'max_depth': 39, 'n_estimators': 269, 'min_samples_split': 2, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:08:04,951] Trial 9 finished with value: 0.9976174172385285 and parameters: {'max_depth': 27, 'n_estimators': 114, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:08:07,417] Trial 10 finished with value: 0.9978055246728811 and parameters: {'max_depth': 45, 'n_estimators': 184, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:08:09,792] Trial 11 finished with value: 0.9978055246728811 and parameters: {'max_depth': 45, 'n_estimators': 173, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:08:11,204] Trial 12 finished with value: 0.9977428287481163 and parameters: {'max_depth': 45, 'n_estimators': 62, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:08:11,740] Trial 13 pruned. \n",
      "[I 2025-12-22 09:08:13,827] Trial 14 finished with value: 0.9974293294642338 and parameters: {'max_depth': 33, 'n_estimators': 123, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:08:17,926] Trial 15 pruned. \n",
      "[I 2025-12-22 09:08:20,259] Trial 16 finished with value: 0.9977428287481163 and parameters: {'max_depth': 38, 'n_estimators': 165, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:08:21,356] Trial 17 pruned. \n",
      "[I 2025-12-22 09:08:24,311] Trial 18 finished with value: 0.9978055246728811 and parameters: {'max_depth': 47, 'n_estimators': 227, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:08:25,080] Trial 19 pruned. \n",
      "[I 2025-12-22 09:08:30,727] Trial 20 finished with value: 0.9976174172385285 and parameters: {'max_depth': 32, 'n_estimators': 485, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:08:33,115] Trial 21 finished with value: 0.9978055246728811 and parameters: {'max_depth': 45, 'n_estimators': 171, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:08:35,404] Trial 22 finished with value: 0.9978055246728811 and parameters: {'max_depth': 45, 'n_estimators': 163, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:08:35,706] Trial 23 pruned. \n",
      "[I 2025-12-22 09:08:38,662] Trial 24 finished with value: 0.9974293491242918 and parameters: {'max_depth': 45, 'n_estimators': 229, 'min_samples_split': 8, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:08:41,038] Trial 25 pruned. \n",
      "[I 2025-12-22 09:08:43,291] Trial 26 finished with value: 0.9976174172385285 and parameters: {'max_depth': 30, 'n_estimators': 144, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:08:43,592] Trial 27 pruned. \n",
      "[I 2025-12-22 09:08:47,860] Trial 28 finished with value: 0.9974920450490566 and parameters: {'max_depth': 34, 'n_estimators': 312, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:08:50,801] Trial 29 pruned. \n",
      "[I 2025-12-22 09:08:54,373] Trial 30 finished with value: 0.9978055246728811 and parameters: {'max_depth': 48, 'n_estimators': 237, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:08:57,671] Trial 31 finished with value: 0.9978055246728811 and parameters: {'max_depth': 47, 'n_estimators': 252, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:08:58,246] Trial 32 pruned. \n",
      "[I 2025-12-22 09:09:02,037] Trial 33 finished with value: 0.9974920253889987 and parameters: {'max_depth': 29, 'n_estimators': 295, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:09:02,488] Trial 34 pruned. \n",
      "[I 2025-12-22 09:09:03,825] Trial 35 pruned. \n",
      "[I 2025-12-22 09:09:04,315] Trial 36 pruned. \n",
      "[I 2025-12-22 09:09:07,747] Trial 37 pruned. \n",
      "[I 2025-12-22 09:09:08,608] Trial 38 pruned. \n",
      "[I 2025-12-22 09:09:11,763] Trial 39 pruned. \n",
      "[I 2025-12-22 09:09:12,000] Trial 40 pruned. \n",
      "[I 2025-12-22 09:09:14,464] Trial 41 finished with value: 0.9978055246728811 and parameters: {'max_depth': 45, 'n_estimators': 178, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:09:16,485] Trial 42 finished with value: 0.9973039179546461 and parameters: {'max_depth': 22, 'n_estimators': 135, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:09:19,250] Trial 43 finished with value: 0.9978055246728811 and parameters: {'max_depth': 43, 'n_estimators': 192, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:09:22,866] Trial 44 finished with value: 0.9978055246728811 and parameters: {'max_depth': None, 'n_estimators': 288, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:09:24,483] Trial 45 finished with value: 0.9975547016537056 and parameters: {'max_depth': 24, 'n_estimators': 102, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:09:27,764] Trial 46 pruned. \n",
      "[I 2025-12-22 09:09:30,229] Trial 47 finished with value: 0.996865026821234 and parameters: {'max_depth': 25, 'n_estimators': 159, 'min_samples_split': 7, 'min_samples_leaf': 6, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:09:30,629] Trial 48 pruned. \n",
      "[I 2025-12-22 09:09:34,172] Trial 49 pruned. \n",
      "[I 2025-12-22 09:09:34,455] Trial 50 pruned. \n",
      "[I 2025-12-22 09:09:37,109] Trial 51 finished with value: 0.9978055246728811 and parameters: {'max_depth': 45, 'n_estimators': 178, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:09:39,420] Trial 52 finished with value: 0.9978055246728811 and parameters: {'max_depth': 45, 'n_estimators': 155, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:09:39,908] Trial 53 pruned. \n",
      "[I 2025-12-22 09:09:42,498] Trial 54 finished with value: 0.9976174172385285 and parameters: {'max_depth': 23, 'n_estimators': 195, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:09:44,159] Trial 55 finished with value: 0.9976801131632934 and parameters: {'max_depth': 36, 'n_estimators': 106, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:09:46,181] Trial 56 finished with value: 0.9978055246728811 and parameters: {'max_depth': 45, 'n_estimators': 137, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:09:46,927] Trial 57 pruned. \n",
      "[I 2025-12-22 09:09:49,137] Trial 58 pruned. \n",
      "[I 2025-12-22 09:09:52,371] Trial 59 finished with value: 0.9976174368985864 and parameters: {'max_depth': 47, 'n_estimators': 265, 'min_samples_split': 6, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:09:52,753] Trial 60 pruned. \n",
      "[I 2025-12-22 09:09:56,286] Trial 61 finished with value: 0.9978055246728811 and parameters: {'max_depth': 31, 'n_estimators': 239, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:09:59,260] Trial 62 finished with value: 0.9974920450490566 and parameters: {'max_depth': 48, 'n_estimators': 191, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:09:59,860] Trial 63 pruned. \n",
      "[I 2025-12-22 09:10:03,190] Trial 64 finished with value: 0.9974920450490566 and parameters: {'max_depth': 48, 'n_estimators': 206, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:10:03,948] Trial 65 pruned. \n",
      "[I 2025-12-22 09:10:07,606] Trial 66 pruned. \n",
      "[I 2025-12-22 09:10:09,764] Trial 67 finished with value: 0.9977428287481163 and parameters: {'max_depth': 39, 'n_estimators': 142, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:10:11,471] Trial 68 finished with value: 0.9974920253889987 and parameters: {'max_depth': 35, 'n_estimators': 90, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:10:11,754] Trial 69 pruned. \n",
      "[I 2025-12-22 09:10:17,097] Trial 70 finished with value: 0.9976801131632934 and parameters: {'max_depth': 33, 'n_estimators': 458, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:10:20,492] Trial 71 finished with value: 0.9976174172385285 and parameters: {'max_depth': 27, 'n_estimators': 259, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:10:21,144] Trial 72 pruned. \n",
      "[I 2025-12-22 09:10:24,614] Trial 73 finished with value: 0.9978055246728811 and parameters: {'max_depth': 47, 'n_estimators': 279, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:10:27,532] Trial 74 finished with value: 0.9976801131632934 and parameters: {'max_depth': 37, 'n_estimators': 222, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:10:28,962] Trial 75 pruned. \n",
      "[I 2025-12-22 09:10:32,668] Trial 76 finished with value: 0.9977428287481163 and parameters: {'max_depth': 38, 'n_estimators': 306, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:10:36,904] Trial 77 finished with value: 0.9978055246728811 and parameters: {'max_depth': 46, 'n_estimators': 346, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:10:39,393] Trial 78 finished with value: 0.9976174172385285 and parameters: {'max_depth': 32, 'n_estimators': 189, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:10:39,916] Trial 79 pruned. \n",
      "[I 2025-12-22 09:10:42,488] Trial 80 pruned. \n",
      "[I 2025-12-22 09:10:44,964] Trial 81 finished with value: 0.9978055246728811 and parameters: {'max_depth': 45, 'n_estimators': 180, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:10:45,393] Trial 82 pruned. \n",
      "[I 2025-12-22 09:10:48,324] Trial 83 finished with value: 0.9972412416899392 and parameters: {'max_depth': 18, 'n_estimators': 237, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:10:51,244] Trial 84 finished with value: 0.997303957274762 and parameters: {'max_depth': 45, 'n_estimators': 216, 'min_samples_split': 9, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:10:53,112] Trial 85 finished with value: 0.9976174172385285 and parameters: {'max_depth': 30, 'n_estimators': 124, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:10:53,651] Trial 86 pruned. \n",
      "[I 2025-12-22 09:10:54,130] Trial 87 pruned. \n",
      "[I 2025-12-22 09:10:57,404] Trial 88 finished with value: 0.9976174172385285 and parameters: {'max_depth': 28, 'n_estimators': 270, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:11:00,000] Trial 89 pruned. \n",
      "[I 2025-12-22 09:11:02,210] Trial 90 finished with value: 0.9976174172385285 and parameters: {'max_depth': 34, 'n_estimators': 152, 'min_samples_split': 8, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:11:04,765] Trial 91 finished with value: 0.9978055246728811 and parameters: {'max_depth': 49, 'n_estimators': 186, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:11:08,086] Trial 92 finished with value: 0.9978055246728811 and parameters: {'max_depth': 42, 'n_estimators': 251, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:11:11,065] Trial 93 finished with value: 0.9978055246728811 and parameters: {'max_depth': 43, 'n_estimators': 228, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:11:13,698] Trial 94 finished with value: 0.9978055246728811 and parameters: {'max_depth': 43, 'n_estimators': 197, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:11:16,535] Trial 95 finished with value: 0.9978055246728811 and parameters: {'max_depth': 50, 'n_estimators': 211, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:11:19,304] Trial 96 finished with value: 0.9976801328233513 and parameters: {'max_depth': 45, 'n_estimators': 159, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.9980562887118829.\n",
      "[I 2025-12-22 09:11:19,650] Trial 97 pruned. \n",
      "[I 2025-12-22 09:11:22,601] Trial 98 pruned. \n",
      "[I 2025-12-22 09:11:23,080] Trial 99 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest hyperparameters: {'max_depth': 45, 'n_estimators': 74, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': None}\n",
      "RF Cross-val Accuracies: [0.99749216 0.99874608 0.9968652  0.99874608 0.99561129 0.99937304\n",
      " 0.99623824 0.9968652  0.99561129 0.99874529]\n",
      "RF Mean CV Accuracy: 0.9974293884197403\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rf_building_pred_UJI.joblib']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "# --- Run the optimization ---\n",
    "study2 = optuna.create_study(direction='maximize')\n",
    "study2.optimize(objective_rf, n_trials=100, timeout=300)\n",
    "\n",
    "# --- Train final model with best hyperparameters ---\n",
    "best_params2 = study2.best_params\n",
    "print(\"Best Random Forest hyperparameters:\", best_params2)\n",
    "\n",
    "rf_building_final = RandomForestClassifier(\n",
    "    **best_params2,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# --- Cross-validation accuracy ---\n",
    "cv_scores_rf = cross_val_score(rf_building_final, X_train, y_train, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "print(\"RF Cross-val Accuracies:\", cv_scores_rf)\n",
    "print(\"RF Mean CV Accuracy:\", cv_scores_rf.mean())\n",
    "\n",
    "# --- Train on full training set (optional, if you want to save the model) ---\n",
    "rf_building_final.fit(X_train, y_train)\n",
    "joblib.dump(rf_building_final, 'rf_building_pred_UJI.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "301abec3-99be-409d-90e1-aa72bac6fd6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:11:39,407] A new study created in memory with name: no-name-5893638d-f5d9-4697-a001-ad581033624c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's multi_error: 0.00250784\tvalid_0's multi_logloss: 0.147705\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[89]\tvalid_0's multi_error: 0.000940439\tvalid_0's multi_logloss: 0.00533039\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's multi_error: 0.00125392\tvalid_0's multi_logloss: 0.0064329\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalid_0's multi_error: 0.0015674\tvalid_0's multi_logloss: 0.0079243\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:12:10,659] Trial 0 finished with value: 0.9984325035805881 and parameters: {'learning_rate': 0.05578769143281918, 'num_leaves': 80, 'max_depth': 26, 'min_child_samples': 35}. Best is trial 0 with value: 0.9984325035805881.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[75]\tvalid_0's multi_error: 0.00156789\tvalid_0's multi_logloss: 0.00728177\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[175]\tvalid_0's multi_error: 0.00250784\tvalid_0's multi_logloss: 0.0854071\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[534]\tvalid_0's multi_error: 0.000940439\tvalid_0's multi_logloss: 0.00453757\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[154]\tvalid_0's multi_error: 0.00376176\tvalid_0's multi_logloss: 0.112952\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[184]\tvalid_0's multi_error: 0.00282132\tvalid_0's multi_logloss: 0.0793313\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[425]\tvalid_0's multi_error: 0.00156789\tvalid_0's multi_logloss: 0.00714786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:13:30,403] Trial 1 finished with value: 0.9976801524834095 and parameters: {'learning_rate': 0.010567435071746279, 'num_leaves': 93, 'max_depth': 25, 'min_child_samples': 76}. Best is trial 0 with value: 0.9984325035805881.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[237]\tvalid_0's multi_error: 0.00250784\tvalid_0's multi_logloss: 0.00848125\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[217]\tvalid_0's multi_error: 0.00250784\tvalid_0's multi_logloss: 0.0104695\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[191]\tvalid_0's multi_error: 0.00219436\tvalid_0's multi_logloss: 0.0143694\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[289]\tvalid_0's multi_error: 0.0015674\tvalid_0's multi_logloss: 0.00530958\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:14:11,867] Trial 2 finished with value: 0.9975546426735319 and parameters: {'learning_rate': 0.018835756470339235, 'num_leaves': 58, 'max_depth': 12, 'min_child_samples': 38}. Best is trial 0 with value: 0.9984325035805881.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's multi_error: 0.00344936\tvalid_0's multi_logloss: 0.349567\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's multi_error: 0.0031348\tvalid_0's multi_logloss: 0.0318443\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's multi_error: 0.000940439\tvalid_0's multi_logloss: 0.00505128\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's multi_error: 0.00125392\tvalid_0's multi_logloss: 0.00722903\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's multi_error: 0.0015674\tvalid_0's multi_logloss: 0.00463732\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:14:19,201] Trial 3 finished with value: 0.9983071117310583 and parameters: {'learning_rate': 0.41789290050075306, 'num_leaves': 35, 'max_depth': 29, 'min_child_samples': 69}. Best is trial 0 with value: 0.9984325035805881.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's multi_error: 0.00156789\tvalid_0's multi_logloss: 0.0203423\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid_0's multi_error: 0.00219436\tvalid_0's multi_logloss: 0.0138893\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[99]\tvalid_0's multi_error: 0.00250784\tvalid_0's multi_logloss: 0.0122826\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[185]\tvalid_0's multi_error: 0.00125392\tvalid_0's multi_logloss: 0.00371731\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[177]\tvalid_0's multi_error: 0.00188088\tvalid_0's multi_logloss: 0.00571698\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:14:29,985] Trial 4 finished with value: 0.9981190239567637 and parameters: {'learning_rate': 0.07352559767674131, 'num_leaves': 95, 'max_depth': 4, 'min_child_samples': 19}. Best is trial 0 with value: 0.9984325035805881.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[174]\tvalid_0's multi_error: 0.00156789\tvalid_0's multi_logloss: 0.00407107\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[78]\tvalid_0's multi_error: 0.00250784\tvalid_0's multi_logloss: 0.0613096\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[214]\tvalid_0's multi_error: 0.000940439\tvalid_0's multi_logloss: 0.00418177\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[194]\tvalid_0's multi_error: 0.00125392\tvalid_0's multi_logloss: 0.00421275\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[190]\tvalid_0's multi_error: 0.0015674\tvalid_0's multi_logloss: 0.00567119\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:15:13,219] Trial 5 finished with value: 0.9984325035805881 and parameters: {'learning_rate': 0.02724170837436886, 'num_leaves': 75, 'max_depth': 14, 'min_child_samples': 50}. Best is trial 0 with value: 0.9984325035805881.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[190]\tvalid_0's multi_error: 0.00156789\tvalid_0's multi_logloss: 0.00483455\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[55]\tvalid_0's multi_error: 0.00250784\tvalid_0's multi_logloss: 0.298239\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:15:30,130] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[186]\tvalid_0's multi_error: 0.00282132\tvalid_0's multi_logloss: 0.0317646\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's multi_error: 0.00219436\tvalid_0's multi_logloss: 0.0109958\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's multi_error: 0.000940439\tvalid_0's multi_logloss: 0.003678\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[48]\tvalid_0's multi_error: 0.00125392\tvalid_0's multi_logloss: 0.00365544\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[71]\tvalid_0's multi_error: 0.00188088\tvalid_0's multi_logloss: 0.00473423\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:15:37,449] Trial 7 finished with value: 0.9984325035805881 and parameters: {'learning_rate': 0.21560451228311736, 'num_leaves': 27, 'max_depth': 5, 'min_child_samples': 15}. Best is trial 0 with value: 0.9984325035805881.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[44]\tvalid_0's multi_error: 0.00156789\tvalid_0's multi_logloss: 0.00419664\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's multi_error: 0.00219436\tvalid_0's multi_logloss: 0.00758711\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's multi_error: 0.000940439\tvalid_0's multi_logloss: 0.00418865\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's multi_error: 0.00125392\tvalid_0's multi_logloss: 0.00414535\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid_0's multi_error: 0.0015674\tvalid_0's multi_logloss: 0.00528366\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:15:48,665] Trial 8 finished with value: 0.998495199505353 and parameters: {'learning_rate': 0.16114592325856952, 'num_leaves': 57, 'max_depth': 11, 'min_child_samples': 64}. Best is trial 8 with value: 0.998495199505353.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's multi_error: 0.00156789\tvalid_0's multi_logloss: 0.00461244\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[265]\tvalid_0's multi_error: 0.00219436\tvalid_0's multi_logloss: 0.0156933\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:15:56,410] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[260]\tvalid_0's multi_error: 0.00282132\tvalid_0's multi_logloss: 0.0178516\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\tvalid_0's multi_error: 0.00250784\tvalid_0's multi_logloss: 0.0289176\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid_0's multi_error: 0.000940439\tvalid_0's multi_logloss: 0.00408959\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's multi_error: 0.00125392\tvalid_0's multi_logloss: 0.00400837\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[59]\tvalid_0's multi_error: 0.0015674\tvalid_0's multi_logloss: 0.0047846\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:16:06,168] Trial 10 finished with value: 0.9984325035805881 and parameters: {'learning_rate': 0.141699076661194, 'num_leaves': 51, 'max_depth': 9, 'min_child_samples': 97}. Best is trial 8 with value: 0.998495199505353.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's multi_error: 0.00156789\tvalid_0's multi_logloss: 0.00470922\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[32]\tvalid_0's multi_error: 0.00250784\tvalid_0's multi_logloss: 0.0753741\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[92]\tvalid_0's multi_error: 0.000940439\tvalid_0's multi_logloss: 0.00425326\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[83]\tvalid_0's multi_error: 0.00125392\tvalid_0's multi_logloss: 0.00492402\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[105]\tvalid_0's multi_error: 0.0015674\tvalid_0's multi_logloss: 0.00484849\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:16:33,688] Trial 11 finished with value: 0.9984325035805881 and parameters: {'learning_rate': 0.05958526825719909, 'num_leaves': 67, 'max_depth': 20, 'min_child_samples': 43}. Best is trial 8 with value: 0.998495199505353.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[78]\tvalid_0's multi_error: 0.00156789\tvalid_0's multi_logloss: 0.00572645\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[42]\tvalid_0's multi_error: 0.00219436\tvalid_0's multi_logloss: 0.0463965\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[96]\tvalid_0's multi_error: 0.000940439\tvalid_0's multi_logloss: 0.0044562\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[88]\tvalid_0's multi_error: 0.00125392\tvalid_0's multi_logloss: 0.00495753\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalid_0's multi_error: 0.0015674\tvalid_0's multi_logloss: 0.00676047\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:16:54,842] Trial 12 finished with value: 0.998495199505353 and parameters: {'learning_rate': 0.05633957226558867, 'num_leaves': 46, 'max_depth': 18, 'min_child_samples': 69}. Best is trial 8 with value: 0.998495199505353.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[82]\tvalid_0's multi_error: 0.00156789\tvalid_0's multi_logloss: 0.00634292\n",
      "Best LightGBM hyperparameters: {'learning_rate': 0.16114592325856952, 'num_leaves': 57, 'max_depth': 11, 'min_child_samples': 64}\n",
      "LGB Cross-val Accuracies: [1.         0.99874608 0.99874608 0.99874608 0.99561129 0.99937304\n",
      " 0.99811912 0.9968652  0.99811912 0.99874529]\n",
      "LGB Mean CV Accuracy: 0.9983071313664487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lgb_building_pred_UJI.joblib']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Optuna study\n",
    "study3 = optuna.create_study(direction='maximize')\n",
    "study3.optimize(objective_lgb, n_trials=100, timeout=300)\n",
    "\n",
    "# Best params\n",
    "best_params3 = study3.best_params\n",
    "print(\"Best LightGBM hyperparameters:\", best_params3)\n",
    "\n",
    "best_params3.update({\n",
    "    'random_state': SEED,\n",
    "    'n_jobs': -1,\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': len(np.unique(y_train))\n",
    "})\n",
    "\n",
    "# Final model\n",
    "lgb_building_final = lgb.LGBMClassifier(**best_params3)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores_lgb = cross_val_score(lgb_building_final, X_train, y_train, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "print(\"LGB Cross-val Accuracies:\", cv_scores_lgb)\n",
    "print(\"LGB Mean CV Accuracy:\", cv_scores_lgb.mean())\n",
    "\n",
    "# Train final model\n",
    "lgb_building_final.fit(X_train, y_train)\n",
    "\n",
    "# Save\n",
    "joblib.dump(lgb_building_final, 'lgb_building_pred_UJI.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4fdcf87-9900-42c8-8aac-88b31664df34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:17:13,916] A new study created in memory with name: no-name-78639073-2d5c-45fd-b73f-0c346703f0de\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:17:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:17:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:17:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:17:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:17:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:17:28,243] Trial 0 finished with value: 0.9942316996808189 and parameters: {'n_estimators': 88, 'learning_rate': 0.015222116548474628, 'max_depth': 6, 'min_child_weight': 3.958361220822593, 'gamma': 0.8632732770264151, 'subsample': 0.9982493862845282, 'colsample_bytree': 0.7885706086291799}. Best is trial 0 with value: 0.9942316996808189.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:17:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:17:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:17:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:17:34] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:17:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:17:37,920] Trial 1 finished with value: 0.9971158695004674 and parameters: {'n_estimators': 135, 'learning_rate': 0.39087505030095965, 'max_depth': 10, 'min_child_weight': 7.973364779474677, 'gamma': 2.2298600375040287, 'subsample': 0.602710358243707, 'colsample_bytree': 0.7062263000364091}. Best is trial 1 with value: 0.9971158695004674.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:17:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:17:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:17:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:17:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:17:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:17:48,378] Trial 2 finished with value: 0.9939808963217015 and parameters: {'n_estimators': 73, 'learning_rate': 0.013778596385894206, 'max_depth': 5, 'min_child_weight': 3.3875048920276636, 'gamma': 3.347192767283447, 'subsample': 0.6908853478925547, 'colsample_bytree': 0.6007343154492346}. Best is trial 1 with value: 0.9971158695004674.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:17:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:17:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:17:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:17:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:18:03,157] Trial 3 finished with value: 0.9970531539156445 and parameters: {'n_estimators': 112, 'learning_rate': 0.0679765719487203, 'max_depth': 5, 'min_child_weight': 6.829543090398676, 'gamma': 2.1969118481687797, 'subsample': 0.6260917545281162, 'colsample_bytree': 0.9088729719770297}. Best is trial 1 with value: 0.9971158695004674.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:18:13,554] Trial 4 finished with value: 0.9971785457651743 and parameters: {'n_estimators': 133, 'learning_rate': 0.21157938473030205, 'max_depth': 5, 'min_child_weight': 1.2627529188276174, 'gamma': 3.4452576660171657, 'subsample': 0.61876170524632, 'colsample_bytree': 0.7429810895528665}. Best is trial 4 with value: 0.9971785457651743.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:23] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:18:26,196] Trial 5 finished with value: 0.9976801328233513 and parameters: {'n_estimators': 108, 'learning_rate': 0.15515453121030454, 'max_depth': 6, 'min_child_weight': 3.3755647992750615, 'gamma': 0.3292316144100971, 'subsample': 0.8999301936943044, 'colsample_bytree': 0.9686984984642404}. Best is trial 5 with value: 0.9976801328233513.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:18:33,170] Trial 6 finished with value: 0.9974293491242918 and parameters: {'n_estimators': 57, 'learning_rate': 0.2544231535698366, 'max_depth': 10, 'min_child_weight': 6.93985815491061, 'gamma': 1.883741332674496, 'subsample': 0.6434212674302484, 'colsample_bytree': 0.604924057244046}. Best is trial 5 with value: 0.9976801328233513.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:33] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:18:34,493] Trial 7 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:34] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:18:36,060] Trial 8 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:18:37,570] Trial 9 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:18:53,894] Trial 10 finished with value: 0.9977428287481163 and parameters: {'n_estimators': 186, 'learning_rate': 0.11197268327281866, 'max_depth': 3, 'min_child_weight': 5.035948529392061, 'gamma': 0.3119709464782064, 'subsample': 0.8634630331081131, 'colsample_bytree': 0.5007622506357574}. Best is trial 10 with value: 0.9977428287481163.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:18:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:19:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:19:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:19:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:19:10,647] Trial 11 finished with value: 0.9977428287481163 and parameters: {'n_estimators': 194, 'learning_rate': 0.1090193712433825, 'max_depth': 3, 'min_child_weight': 5.005317471304361, 'gamma': 0.26179812138551556, 'subsample': 0.8573893364257253, 'colsample_bytree': 0.5060062553435806}. Best is trial 10 with value: 0.9977428287481163.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:19:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:19:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:19:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:19:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:19:24] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:19:27,986] Trial 12 finished with value: 0.9977428287481163 and parameters: {'n_estimators': 199, 'learning_rate': 0.09487533624331834, 'max_depth': 3, 'min_child_weight': 5.288117843153084, 'gamma': 0.04921392615408887, 'subsample': 0.8224328735360261, 'colsample_bytree': 0.5048660623560935}. Best is trial 10 with value: 0.9977428287481163.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:19:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:19:31,495] Trial 13 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:19:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:19:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:19:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:19:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:19:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:19:48,208] Trial 14 finished with value: 0.9976801328233513 and parameters: {'n_estimators': 167, 'learning_rate': 0.09058685898904419, 'max_depth': 4, 'min_child_weight': 4.721114872474787, 'gamma': 1.0976711671285189, 'subsample': 0.9041186549012916, 'colsample_bytree': 0.6015174138745822}. Best is trial 10 with value: 0.9977428287481163.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:19:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:19:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:19:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:19:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:20:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:20:02,732] Trial 15 finished with value: 0.9976174368985864 and parameters: {'n_estimators': 174, 'learning_rate': 0.13883953730267537, 'max_depth': 4, 'min_child_weight': 6.721552893134916, 'gamma': 0.5240629367933498, 'subsample': 0.8641578802062642, 'colsample_bytree': 0.6657484000283401}. Best is trial 10 with value: 0.9977428287481163.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:20:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:20:07,662] Trial 16 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:20:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:20:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:20:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:20:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:20:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:20:21,608] Trial 17 finished with value: 0.9973666531995269 and parameters: {'n_estimators': 182, 'learning_rate': 0.12785115012656062, 'max_depth': 4, 'min_child_weight': 6.43800576171736, 'gamma': 3.1612500241875763, 'subsample': 0.7787136746447069, 'colsample_bytree': 0.6540864141461564}. Best is trial 10 with value: 0.9977428287481163.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:20:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:20:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:20:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:20:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:20:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:20:40,047] Trial 18 finished with value: 0.9971158695004674 and parameters: {'n_estimators': 145, 'learning_rate': 0.05098038733158831, 'max_depth': 7, 'min_child_weight': 7.867558424144432, 'gamma': 4.69181540429087, 'subsample': 0.8465859491517164, 'colsample_bytree': 0.5633281452076462}. Best is trial 10 with value: 0.9977428287481163.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:20:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:20:43,072] Trial 19 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:20:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:20:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:20:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:20:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:20:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:20:54,455] Trial 20 finished with value: 0.9975547409738216 and parameters: {'n_estimators': 180, 'learning_rate': 0.46751363979978633, 'max_depth': 4, 'min_child_weight': 6.036193039848471, 'gamma': 0.8172640396023252, 'subsample': 0.7613436492573181, 'colsample_bytree': 0.8055970967178644}. Best is trial 10 with value: 0.9977428287481163.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:20:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:20:58] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:21:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:21:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:21:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:21:12,302] Trial 21 finished with value: 0.9977428287481163 and parameters: {'n_estimators': 198, 'learning_rate': 0.08844594178470482, 'max_depth': 3, 'min_child_weight': 5.2613426064053455, 'gamma': 0.0056082661951987745, 'subsample': 0.8097863867717887, 'colsample_bytree': 0.5030605341835197}. Best is trial 10 with value: 0.9977428287481163.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:21:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:21:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:21:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:21:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:21:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:21:29,121] Trial 22 finished with value: 0.9977428287481163 and parameters: {'n_estimators': 199, 'learning_rate': 0.10590557016498818, 'max_depth': 3, 'min_child_weight': 4.5385181958379, 'gamma': 0.5024145782240237, 'subsample': 0.8363705126245021, 'colsample_bytree': 0.5447000685295151}. Best is trial 10 with value: 0.9977428287481163.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:21:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:21:33] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:21:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:21:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:21:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:21:49,038] Trial 23 finished with value: 0.9976801328233513 and parameters: {'n_estimators': 189, 'learning_rate': 0.06691858238879661, 'max_depth': 4, 'min_child_weight': 5.4167461599294455, 'gamma': 0.39833339401455287, 'subsample': 0.9014680759235416, 'colsample_bytree': 0.5077245275242339}. Best is trial 10 with value: 0.9977428287481163.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:21:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:21:51,593] Trial 24 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:21:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:21:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:21:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:22:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:22:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:22:06,631] Trial 25 finished with value: 0.9977428287481163 and parameters: {'n_estimators': 161, 'learning_rate': 0.10558277934211818, 'max_depth': 3, 'min_child_weight': 4.031092323399548, 'gamma': 0.7381182033178113, 'subsample': 0.8753197877154526, 'colsample_bytree': 0.6298640517452693}. Best is trial 10 with value: 0.9977428287481163.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:22:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:22:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:22:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:22:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:22:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:22:21,125] Trial 26 finished with value: 0.9976174368985864 and parameters: {'n_estimators': 186, 'learning_rate': 0.1838536236026317, 'max_depth': 4, 'min_child_weight': 6.019992330443218, 'gamma': 0.07082893797906925, 'subsample': 0.8189437228504642, 'colsample_bytree': 0.5383342347088814}. Best is trial 10 with value: 0.9977428287481163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost hyperparameters: {'n_estimators': 186, 'learning_rate': 0.11197268327281866, 'max_depth': 3, 'min_child_weight': 5.035948529392061, 'gamma': 0.3119709464782064, 'subsample': 0.8634630331081131, 'colsample_bytree': 0.5007622506357574}\n",
      "XGB Cross-val Accuracies: [0.99749216 0.99874608 0.99561129 0.99937304 0.99749216 0.99937304\n",
      " 0.9968652  0.9968652  0.9968652  0.99749059]\n",
      "XGB Mean CV Accuracy: 0.9976173975291356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:22:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgb_building_pred_UJI.joblib']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoost\n",
    "# --- Run optimization ---\n",
    "study4 = optuna.create_study(direction='maximize')\n",
    "study4.optimize(objective_xgb, n_trials=100, timeout=300)\n",
    "\n",
    "# --- Best parameters ---\n",
    "best_params4 = study4.best_params\n",
    "print(\"Best XGBoost hyperparameters:\", best_params4)\n",
    "\n",
    "# --- Train final model with best params ---\n",
    "xgb_building_final = XGBClassifier(**best_params4, use_label_encoder=False, eval_metric='mlogloss', random_state=SEED, n_jobs=-1)\n",
    "\n",
    "# --- Cross-validation accuracy ---\n",
    "cv_scores_xgb = cross_val_score(xgb_building_final, X_train, y_train, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "print(\"XGB Cross-val Accuracies:\", cv_scores_xgb)\n",
    "print(\"XGB Mean CV Accuracy:\", cv_scores_xgb.mean())\n",
    "\n",
    "# --- Train on full training set (optional, if you want to save the model) ---\n",
    "xgb_building_final.fit(X_train, y_train)\n",
    "joblib.dump(xgb_building_final, 'xgb_building_pred_UJI.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed96a6f0-7c66-485f-839c-618247433214",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:22:39,621] A new study created in memory with name: no-name-a03a815d-db65-4776-a1d7-5012b3b50752\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:40,597] Trial 0 finished with value: 0.9957991371200571 and parameters: {'var_smoothing': 2.1953424644592985e-12}. Best is trial 0 with value: 0.9957991371200571.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:41,773] Trial 1 finished with value: 0.9957991371200571 and parameters: {'var_smoothing': 1.496785417972162e-09}. Best is trial 0 with value: 0.9957991371200571.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:42,731] Trial 2 finished with value: 0.9957991371200571 and parameters: {'var_smoothing': 3.719743681534384e-12}. Best is trial 0 with value: 0.9957991371200571.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:43,672] Trial 3 finished with value: 0.9979309165224111 and parameters: {'var_smoothing': 1.0857602273601716e-07}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:44,590] Trial 4 finished with value: 0.9979309165224111 and parameters: {'var_smoothing': 2.5421742469352025e-08}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:45,510] Trial 5 finished with value: 0.9957991371200571 and parameters: {'var_smoothing': 1.4551907034655367e-10}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:46,422] Trial 6 finished with value: 0.9979309165224111 and parameters: {'var_smoothing': 5.098521796665467e-08}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:47,321] Trial 7 finished with value: 0.9979309165224111 and parameters: {'var_smoothing': 2.8973962166177498e-08}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:47,528] Trial 8 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:47,707] Trial 9 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:48,637] Trial 10 finished with value: 0.9978055246728813 and parameters: {'var_smoothing': 9.816490945211767e-07}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:49,579] Trial 11 finished with value: 0.9978682205976461 and parameters: {'var_smoothing': 6.329977387269422e-07}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:50,508] Trial 12 finished with value: 0.9979309165224111 and parameters: {'var_smoothing': 1.0033936345480769e-07}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:50,691] Trial 13 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:51,764] Trial 14 finished with value: 0.9979309165224111 and parameters: {'var_smoothing': 1.5035566710938545e-07}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:51,946] Trial 15 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:52,129] Trial 16 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:52,312] Trial 17 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:53,223] Trial 18 finished with value: 0.9979309165224111 and parameters: {'var_smoothing': 1.8089231673076742e-07}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:53,425] Trial 19 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:53,608] Trial 20 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:54,530] Trial 21 finished with value: 0.9979309165224111 and parameters: {'var_smoothing': 3.635486613202127e-08}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:55,449] Trial 22 finished with value: 0.9979309165224111 and parameters: {'var_smoothing': 5.08423553303678e-08}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:56,355] Trial 23 finished with value: 0.9979309165224111 and parameters: {'var_smoothing': 3.35060099972125e-07}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:56,541] Trial 24 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:57,087] Trial 25 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:58,003] Trial 26 finished with value: 0.9979309165224111 and parameters: {'var_smoothing': 7.09742296923357e-08}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:58,914] Trial 27 finished with value: 0.9979309165224111 and parameters: {'var_smoothing': 2.78723855522015e-07}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:59,098] Trial 28 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:22:59,295] Trial 29 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:23:00,215] Trial 30 finished with value: 0.9979309165224111 and parameters: {'var_smoothing': 2.379698936293828e-08}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:23:01,287] Trial 31 finished with value: 0.9979309165224111 and parameters: {'var_smoothing': 3.3267617992443164e-08}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:23:02,225] Trial 32 finished with value: 0.9979309165224111 and parameters: {'var_smoothing': 7.3691931909024e-08}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:23:03,142] Trial 33 finished with value: 0.9979309165224111 and parameters: {'var_smoothing': 3.5480192813856933e-07}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:23:03,706] Trial 34 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:23:03,899] Trial 35 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:23:04,082] Trial 36 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:23:05,000] Trial 37 finished with value: 0.9979309165224111 and parameters: {'var_smoothing': 1.2891257620332463e-07}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:23:05,908] Trial 38 finished with value: 0.9979309165224111 and parameters: {'var_smoothing': 5.776941616345408e-07}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:23:06,090] Trial 39 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:23:06,646] Trial 40 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:23:07,561] Trial 41 finished with value: 0.9979309165224111 and parameters: {'var_smoothing': 7.864472633464102e-08}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:23:08,508] Trial 42 finished with value: 0.9979309165224111 and parameters: {'var_smoothing': 1.3045453007259335e-07}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:23:09,420] Trial 43 finished with value: 0.9979309165224111 and parameters: {'var_smoothing': 4.253766213721814e-08}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:23:09,604] Trial 44 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:23:10,681] Trial 45 finished with value: 0.9978055246728813 and parameters: {'var_smoothing': 7.027292080971511e-07}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:23:11,587] Trial 46 finished with value: 0.9979309165224111 and parameters: {'var_smoothing': 1.7753198242775025e-07}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:23:11,784] Trial 47 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:23:12,696] Trial 48 finished with value: 0.9979309165224111 and parameters: {'var_smoothing': 1.0422159698787381e-07}. Best is trial 3 with value: 0.9979309165224111.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\777657298.py:166: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:23:12,885] Trial 49 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Naive Bayes hyperparameters: {'var_smoothing': 1.0857602273601716e-07}\n",
      "GNB Cross-val Accuracies: [1.         0.99874608 0.99874608 0.99811912 0.99561129 0.99937304\n",
      " 0.99811912 0.99623824 0.99749216 0.99874529]\n",
      "GNB Mean CV Accuracy: 0.9981190435921541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['gnb_building_pred_UJI.joblib']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gaussian Naive Bayes\n",
    "# --- Run the optimization ---\n",
    "study7 = optuna.create_study(direction='maximize')\n",
    "study7.optimize(objective_gnb, n_trials=50, timeout=200)\n",
    "\n",
    "# --- Train final model with best hyperparameters ---\n",
    "best_params7 = study7.best_params\n",
    "print(\"Best Naive Bayes hyperparameters:\", best_params7)\n",
    "\n",
    "gnb_building_final = GaussianNB(**best_params7)\n",
    "\n",
    "# --- Cross-validation accuracy ---\n",
    "cv_scores_gnb = cross_val_score(gnb_building_final, X_train, y_train, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "print(\"GNB Cross-val Accuracies:\", cv_scores_gnb)\n",
    "print(\"GNB Mean CV Accuracy:\", cv_scores_gnb.mean())\n",
    "\n",
    "# --- Train on full training set (optional, if you want to save the model) ---\n",
    "gnb_building_final.fit(X_train, y_train)\n",
    "joblib.dump(gnb_building_final, 'gnb_building_pred_UJI.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "067afa44-045b-4062-86e4-1a115f21783b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:23:16,087] A new study created in memory with name: no-name-d283c608-875a-4fd0-972f-ca69c5881559\n",
      "[I 2025-12-22 09:24:20,207] Trial 0 finished with value: 0.9199317402788385 and parameters: {'criterion': 'entropy', 'max_depth': 4, 'min_samples_split': 17, 'min_samples_leaf': 3, 'n_estimators': 77, 'learning_rate': 0.016456285975838102}. Best is trial 0 with value: 0.9199317402788385.\n",
      "[I 2025-12-22 09:26:43,818] Trial 1 finished with value: 0.9974293687843498 and parameters: {'criterion': 'entropy', 'max_depth': 3, 'min_samples_split': 17, 'min_samples_leaf': 19, 'n_estimators': 197, 'learning_rate': 0.6335393631690206}. Best is trial 1 with value: 0.9974293687843498.\n",
      "[I 2025-12-22 09:32:00,277] Trial 2 finished with value: 0.9982444158062934 and parameters: {'criterion': 'log_loss', 'max_depth': 6, 'min_samples_split': 2, 'min_samples_leaf': 2, 'n_estimators': 271, 'learning_rate': 0.4635982177250387}. Best is trial 2 with value: 0.9982444158062934.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'criterion': 'log_loss', 'max_depth': 6, 'min_samples_split': 2, 'min_samples_leaf': 2, 'n_estimators': 271, 'learning_rate': 0.4635982177250387}\n",
      "AdaBoost Cross-val Accuracies: [1.         0.99874608 0.99874608 0.99874608 0.99561129 0.99937304\n",
      " 0.99811912 0.9968652  0.99811912 0.99874529]\n",
      "AdaBoost Mean CV Accuracy: 0.9983071313664487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['adaboost_building_pred_UJI.joblib']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import joblib\n",
    "\n",
    "# --- Run the optimization ---\n",
    "study_ab = optuna.create_study(direction='maximize')\n",
    "study_ab.optimize(objective_adaboost, n_trials=100, timeout=300)\n",
    "\n",
    "# --- Train final model with best hyperparameters ---\n",
    "best_params_ab = study_ab.best_params\n",
    "print(\"Best hyperparameters:\", best_params_ab)\n",
    "\n",
    "# Extract AdaBoost-specific parameters\n",
    "ada_params = {\n",
    "    'n_estimators': best_params_ab.pop('n_estimators'),\n",
    "    'learning_rate': best_params_ab.pop('learning_rate'),\n",
    "    'random_state': SEED\n",
    "}\n",
    "\n",
    "# Remaining params go to base estimator (DecisionTree)\n",
    "base_estimator = DecisionTreeClassifier(**best_params_ab, random_state=SEED)\n",
    "\n",
    "# Build final AdaBoost model\n",
    "ab_building_final = AdaBoostClassifier(\n",
    "    estimator=base_estimator,\n",
    "    **ada_params\n",
    ")\n",
    "\n",
    "# --- Cross-validation accuracy ---\n",
    "cv_scores_ab = cross_val_score(ab_building_final, X_train, y_train, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "print(\"AdaBoost Cross-val Accuracies:\", cv_scores_ab)\n",
    "print(\"AdaBoost Mean CV Accuracy:\", cv_scores_ab.mean())\n",
    "\n",
    "# --- Train on full training set (optional, if you want to save the model) ---\n",
    "ab_building_final.fit(X_train, y_train)\n",
    "joblib.dump(ab_building_final, 'adaboost_building_pred_UJI.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6cdcd2c2-2386-4456-a54c-54a207959c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "        (\"KNN\", knn_building_final),\n",
    "        (\"RF\", rf_building_final),\n",
    "        (\"LGB\", lgb_building_final),\n",
    "        (\"XGB\", xgb_building_final),\n",
    "        (\"NB\", gnb_building_final),\n",
    "        (\"AB\", ab_building_final)\n",
    "    ]\n",
    "\n",
    "accuracy_knn = cv_scores_knn.mean()\n",
    "accuracy_rf = cv_scores_rf.mean()\n",
    "accuracy_lgb = cv_scores_lgb.mean()\n",
    "accuracy_xgb = cv_scores_xgb.mean()\n",
    "accuracy_gnb = cv_scores_gnb.mean()\n",
    "accuracy_ab = cv_scores_ab.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66576aa2-9d7b-4792-811c-8b88839e6f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ENSEMBLE MEMORY FOOTPRINT =====\n",
      "KNN: Disk = 56.71 MB | RAM = 56.71 MB\n",
      "RF: Disk = 0.65 MB | RAM = 0.64 MB\n",
      "LGB: Disk = 1.44 MB | RAM = 1.44 MB\n",
      "XGB: Disk = 0.50 MB | RAM = 0.50 MB\n",
      "NB: Disk = 0.03 MB | RAM = 0.03 MB\n",
      "AB: Disk = 0.67 MB | RAM = 0.66 MB\n",
      "------------------------------------\n",
      "TOTAL ENSEMBLE DISK SIZE: 60.00 MB\n",
      "TOTAL ENSEMBLE RAM SIZE : 59.98 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pickle\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "# ===== Utility functions =====\n",
    "def model_disk_size_mb(model):\n",
    "    \"\"\"Serialized model size on disk (deployment footprint)\"\"\"\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".joblib\") as tmp:\n",
    "        joblib.dump(model, tmp.name)\n",
    "        size_mb = os.path.getsize(tmp.name) / (1024 ** 2)\n",
    "    os.remove(tmp.name)\n",
    "    return size_mb\n",
    "\n",
    "\n",
    "def model_ram_size_mb(model):\n",
    "    \"\"\"In-memory size (RAM footprint)\"\"\"\n",
    "    return sys.getsizeof(pickle.dumps(model)) / (1024 ** 2)\n",
    "\n",
    "\n",
    "# ===== Compute footprint =====\n",
    "disk_sizes = {}\n",
    "ram_sizes = {}\n",
    "\n",
    "for name, model in models:\n",
    "    disk_sizes[name] = model_disk_size_mb(model)\n",
    "    ram_sizes[name] = model_ram_size_mb(model)\n",
    "\n",
    "# ===== Total ensemble footprint =====\n",
    "total_disk_mb = sum(disk_sizes.values())\n",
    "total_ram_mb = sum(ram_sizes.values())\n",
    "\n",
    "print(\"===== ENSEMBLE MEMORY FOOTPRINT =====\")\n",
    "for name in disk_sizes:\n",
    "    print(f\"{name}: Disk = {disk_sizes[name]:.2f} MB | RAM = {ram_sizes[name]:.2f} MB\")\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(f\"TOTAL ENSEMBLE DISK SIZE: {total_disk_mb:.2f} MB\")\n",
    "print(f\"TOTAL ENSEMBLE RAM SIZE : {total_ram_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "741946c5-3504-4317-9822-d2002e682b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IEO-CVV\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def ieo_cvv_ensemble_selection(models, X_train, y_train, X_test, y_test, cv=10, patience=3):\n",
    "    print(\"Initializing Iterative Ensemble Optimization with CV Voting (IEO-CVV)...\")\n",
    "\n",
    "    # --- Step 1: Compute cross-val scores and predictions ---\n",
    "    model_scores = {}\n",
    "    model_preds = {}\n",
    "\n",
    "    for name, model in models:\n",
    "        print(f\"Evaluating {name}...\")\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "        acc = cv_scores.mean()\n",
    "        model_scores[name] = acc\n",
    "\n",
    "        y_pred_cv = cross_val_predict(model, X_train, y_train, cv=cv, method='predict', n_jobs=-1)\n",
    "        model_preds[name] = y_pred_cv\n",
    "\n",
    "        print(f\"{name} CV Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Sort models by CV accuracy\n",
    "    sorted_models = sorted(models, key=lambda m: model_scores[m[0]], reverse=True)\n",
    "\n",
    "    # --- Ensemble voting function (CV-based predictions) ---\n",
    "    def ensemble_predict_cv(selected_model_names):\n",
    "        preds = np.array([model_preds[name] for name in selected_model_names])\n",
    "        weights = np.array([model_scores[name] for name in selected_model_names])\n",
    "        weighted_preds = []\n",
    "\n",
    "        for i in range(preds.shape[1]):\n",
    "            classes = np.unique(preds[:, i])\n",
    "            vote_score = {c: 0.0 for c in classes}\n",
    "            for j, c in enumerate(preds[:, i]):\n",
    "                vote_score[c] += weights[j]\n",
    "            final_class = max(vote_score, key=vote_score.get)\n",
    "            weighted_preds.append(final_class)\n",
    "\n",
    "        return np.array(weighted_preds)\n",
    "\n",
    "    # --- Step 2: Initialize ensemble with top 2 models ---\n",
    "    ensemble = sorted_models[:2]\n",
    "    ensemble_names = [name for name, _ in ensemble]\n",
    "    best_acc = accuracy_score(y_train, ensemble_predict_cv(ensemble_names))\n",
    "    print(f\"Initial Ensemble: {[n for n, _ in ensemble]} - Train Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "    # --- Step 3: Iterative forward selection with backward pruning ---\n",
    "    patience_counter = 0\n",
    "    remaining_models = sorted_models[2:]\n",
    "\n",
    "    while remaining_models and patience_counter < patience:\n",
    "        improved = False\n",
    "\n",
    "        name, model = remaining_models.pop(0)\n",
    "        temp_ensemble = ensemble + [(name, model)]\n",
    "        temp_names = [n for n, _ in temp_ensemble]\n",
    "\n",
    "        y_pred_temp = ensemble_predict_cv(temp_names)\n",
    "        new_acc = accuracy_score(y_train, y_pred_temp)\n",
    "\n",
    "        if new_acc > best_acc:\n",
    "            ensemble = temp_ensemble\n",
    "            best_acc = new_acc\n",
    "            improved = True\n",
    "            patience_counter = 0\n",
    "            print(f\"Added {name} - Improved Train Accuracy: {new_acc:.4f}\")\n",
    "\n",
    "            # --- Backward pruning ---\n",
    "            pruned = True\n",
    "            while pruned and len(ensemble) > 1:\n",
    "                pruned = False\n",
    "                for n, m in ensemble:\n",
    "                    if n == name:\n",
    "                        continue  # Don't remove just-added model\n",
    "                    temp_ensemble_pruned = [(x, y) for x, y in ensemble if x != n]\n",
    "                    temp_names_pruned = [x for x, _ in temp_ensemble_pruned]\n",
    "                    y_pred_pruned = ensemble_predict_cv(temp_names_pruned)\n",
    "                    pruned_acc = accuracy_score(y_train, y_pred_pruned)\n",
    "\n",
    "                    if pruned_acc >= best_acc:\n",
    "                        print(f\"Pruned {n} - Accuracy maintained/improved: {pruned_acc:.4f}\")\n",
    "                        ensemble = temp_ensemble_pruned\n",
    "                        best_acc = pruned_acc\n",
    "                        pruned = True\n",
    "                        break  # Restart pruning loop\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Ignored {name} - No improvement: {new_acc:.4f} (Patience: {patience_counter}/{patience})\")\n",
    "\n",
    "    # --- Final weights based on normalized CV accuracies ---\n",
    "    final_names = [n for n, _ in ensemble]\n",
    "    weights = {name: model_scores[name] for name in final_names}\n",
    "    total = sum(weights.values())\n",
    "    weights = {k: v / total for k, v in weights.items()}\n",
    "\n",
    "    print(\"Final Ensemble Members:\", final_names)\n",
    "    print(\"Final Ensemble Train Accuracy:\", best_acc)\n",
    "\n",
    "    # --- Step 4: Retrain final models and test ensemble ---\n",
    "    print(\"\\nRetraining final models and evaluating on test set...\")\n",
    "    for name, model in ensemble:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    def ensemble_predict_test(selected_models, X):\n",
    "        preds = np.array([m.predict(X).ravel() for _, m in selected_models])\n",
    "        weights = np.array([model_scores[name] for name, _ in selected_models])\n",
    "        weighted_preds = []\n",
    "        for i in range(X.shape[0]):\n",
    "            classes = np.unique(preds[:, i])\n",
    "            vote_score = {c: 0.0 for c in classes}\n",
    "            for j, c in enumerate(preds[:, i]):\n",
    "                vote_score[c] += weights[j]\n",
    "            final_class = max(vote_score, key=vote_score.get)\n",
    "            weighted_preds.append(final_class)\n",
    "        return np.array(weighted_preds)\n",
    "\n",
    "    y_pred_test = ensemble_predict_test(ensemble, X_test)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    print(\"Final Ensemble Test Accuracy:\", test_acc)\n",
    "\n",
    "    return ensemble, best_acc, weights, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8cd30934-dc48-447e-8ce6-2f068b42a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(ensemble, X, model_scores):\n",
    "    preds = np.array([m.predict(X).ravel() for _, m in ensemble])  # predictions of all models\n",
    "    weights = np.array([model_scores[name] for name, _ in ensemble])  # accuracy-based weights\n",
    "\n",
    "    weighted_preds = []\n",
    "    for i in range(X.shape[0]):\n",
    "        classes, counts = np.unique(preds[:, i], return_counts=True)\n",
    "        # Weighted voting\n",
    "        vote_score = {c: 0 for c in classes}\n",
    "        for j, c in enumerate(preds[:, i]):\n",
    "            vote_score[c] += weights[j]\n",
    "        weighted_preds.append(max(vote_score, key=vote_score.get))\n",
    "\n",
    "    return np.array(weighted_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f73a15ef-b345-49a2-91e8-468b29474acf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Iterative Ensemble Optimization with CV Voting (IEO-CVV)...\n",
      "Evaluating KNN...\n",
      "KNN CV Accuracy: 0.9979\n",
      "Evaluating RF...\n",
      "RF CV Accuracy: 0.9977\n",
      "Evaluating LGB...\n",
      "LGB CV Accuracy: 0.9977\n",
      "Evaluating XGB...\n",
      "XGB CV Accuracy: 0.9976\n",
      "Evaluating NB...\n",
      "NB CV Accuracy: 0.9976\n",
      "Evaluating AB...\n",
      "AB CV Accuracy: 0.9982\n",
      "Initial Ensemble: ['AB', 'KNN'] - Train Accuracy: 0.9982\n",
      "Ignored RF - No improvement: 0.9981 (Patience: 1/3)\n",
      "Ignored LGB - No improvement: 0.9980 (Patience: 2/3)\n",
      "Added NB - Improved Train Accuracy: 0.9983\n",
      "Ignored XGB - No improvement: 0.9981 (Patience: 1/3)\n",
      "Final Ensemble Members: ['AB', 'KNN', 'NB']\n",
      "Final Ensemble Train Accuracy: 0.9983071038936611\n",
      "\n",
      "Retraining final models and evaluating on test set...\n",
      "Final Ensemble Test Accuracy: 0.9972417251755266\n",
      "Training time: 2237.53 seconds\n"
     ]
    }
   ],
   "source": [
    "ensemble2, best_acc2, ensemble_weights2, test_acc2 = ieo_cvv_ensemble_selection(models, X_train, y_train, X_test, y_test)\n",
    "\n",
    "end_time1 = time.time()\n",
    "\n",
    "training_time1 = end_time1 - start_time1\n",
    "print(f\"Training time: {training_time1:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3fbb4497-ebef-4d73-8702-4cd57e9c3c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.88 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time2 = time.time()\n",
    "\n",
    "y_building_predictions2 = ensemble_predict(ensemble2, X_target, ensemble_weights2)\n",
    "\n",
    "end_time2 = time.time()\n",
    "training_time2 = end_time2 - start_time2\n",
    "print(f\"Training time: {training_time2:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b9bf19fb-12c5-4b3d-912d-8c46ed0f2200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building using Ensemble Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy_building_ensemble = accuracy_score(y_target_building, y_building_predictions2)\n",
    "print(\"Building using Ensemble Accuracy:\", accuracy_building_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2caad2-e0f1-462e-bebb-e574bfabf74f",
   "metadata": {},
   "source": [
    "# Floor Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "de0ec602-f5ff-4c26-a565-86fdd8553317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train-test split ---\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y_floor, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Fix random seeds for reproducibility ---\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b8dba08-fb33-4570-878b-d3b15cb0b291",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time3 = time.time()\n",
    "\n",
    "# --- Custom weight function for KNN ---\n",
    "def knn_weight(d):\n",
    "    return 1 / (d + 0.000001) ** 2\n",
    "\n",
    "\n",
    "# --- Objective function for KNN ---\n",
    "def objective_wknn2(trial):\n",
    "    n_neighbors = trial.suggest_int('n_neighbors', 1, 50)\n",
    "    p = trial.suggest_int('p', 1, 5)\n",
    "    metric = trial.suggest_categorical('metric', ['minkowski', 'euclidean', 'manhattan'])\n",
    "\n",
    "    knn = KNeighborsClassifier(\n",
    "        n_neighbors=n_neighbors,\n",
    "        weights=knn_weight,\n",
    "        p=p,\n",
    "        metric=metric\n",
    "    )\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train2, y_train2)):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        knn.fit(X_tr, y_tr)\n",
    "        y_pred = knn.predict(X_val)\n",
    "        score = accuracy_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "\n",
    "        trial.report(np.mean(scores), step=fold)\n",
    "        if trial.should_prune():\n",
    "            raise TrialPruned()\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# --- Objective function for RandomForest ---\n",
    "def objective_rf2(trial):\n",
    "    max_depth = trial.suggest_categorical('max_depth', [None] + list(range(5, 51)))\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=n_estimators,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train2, y_train2)):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        rf.fit(X_tr, y_tr)\n",
    "        y_pred = rf.predict(X_val)\n",
    "        score = accuracy_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "\n",
    "        trial.report(np.mean(scores), step=fold)\n",
    "        if trial.should_prune():\n",
    "            raise TrialPruned()\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# --- Objective function for LightGBM (with pruning callback) ---\n",
    "def objective_lgb2(trial):\n",
    "    param = {\n",
    "    'n_estimators': 10000,\n",
    "    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5, log=True),\n",
    "    'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "    'max_depth': trial.suggest_int('max_depth', 3, 30),\n",
    "    'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "    'random_state': SEED,\n",
    "    'n_jobs': -1,\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': len(np.unique(y_train)),\n",
    "    'verbosity': -1}\n",
    "\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    model = lgb.LGBMClassifier(**param)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train2, y_train2)):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='multi_error',\n",
    "            callbacks=[lgb.early_stopping(100)],\n",
    "        )\n",
    "\n",
    "        y_pred = model.predict(X_val)\n",
    "        score = accuracy_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "\n",
    "        # Report accuracy to Optuna for pruning\n",
    "        trial.report(np.mean(scores), step=fold)\n",
    "        if trial.should_prune():\n",
    "            raise TrialPruned()\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Objective function for XGBoost ---\n",
    "def objective_xgb2(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'use_label_encoder': False,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'random_state': SEED,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**param)\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train2, y_train2)):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "        score = accuracy_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "\n",
    "        trial.report(np.mean(scores), step=fold)\n",
    "        if trial.should_prune():\n",
    "            raise TrialPruned()\n",
    "    return np.mean(scores)\n",
    "\n",
    "# --- Objective function for GaussianNB ---\n",
    "def objective_gnb2(trial):\n",
    "    param = {\n",
    "        'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
    "    }\n",
    "\n",
    "    model = GaussianNB(**param)\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train2, y_train2)):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "        score = accuracy_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "\n",
    "        trial.report(np.mean(scores), step=fold)\n",
    "        if trial.should_prune():\n",
    "            raise TrialPruned()\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from optuna.exceptions import TrialPruned\n",
    "\n",
    "# --- Objective function for AdaBoost ---\n",
    "def objective_adaboost2(trial):\n",
    "    base_estimator_params = {\n",
    "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "        'random_state': SEED\n",
    "    }\n",
    "\n",
    "    # AdaBoost-specific hyperparameters\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 1.0, log=True)\n",
    "\n",
    "    base_estimator = DecisionTreeClassifier(**base_estimator_params)\n",
    "\n",
    "    model = AdaBoostClassifier(\n",
    "        estimator=base_estimator,\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train2, y_train2)):\n",
    "        X_tr, X_val = X_train2.iloc[train_idx], X_train2.iloc[val_idx]\n",
    "        y_tr, y_val = y_train2.iloc[train_idx], y_train2.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "        score = accuracy_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "\n",
    "        trial.report(np.mean(scores), step=fold)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "02f17fb5-0e66-4066-a1be-fa382df05dea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:34:50,699] A new study created in memory with name: no-name-a7b727f3-01a8-410a-b8c3-e4064e276b6a\n",
      "[I 2025-12-22 09:34:52,110] Trial 0 finished with value: 0.9963006651980603 and parameters: {'n_neighbors': 29, 'p': 2, 'metric': 'minkowski'}. Best is trial 0 with value: 0.9963006651980603.\n",
      "[I 2025-12-22 09:34:59,974] Trial 1 finished with value: 0.9973038589744725 and parameters: {'n_neighbors': 36, 'p': 2, 'metric': 'manhattan'}. Best is trial 1 with value: 0.9973038589744725.\n",
      "[I 2025-12-22 09:35:07,670] Trial 2 finished with value: 0.9973665548992374 and parameters: {'n_neighbors': 19, 'p': 3, 'metric': 'manhattan'}. Best is trial 2 with value: 0.9973665548992374.\n",
      "[I 2025-12-22 09:35:15,396] Trial 3 finished with value: 0.9973665548992374 and parameters: {'n_neighbors': 19, 'p': 4, 'metric': 'manhattan'}. Best is trial 2 with value: 0.9973665548992374.\n",
      "[I 2025-12-22 09:36:32,896] Trial 4 finished with value: 0.9975547016537056 and parameters: {'n_neighbors': 9, 'p': 3, 'metric': 'minkowski'}. Best is trial 4 with value: 0.9975547016537056.\n",
      "[I 2025-12-22 09:36:33,155] Trial 5 pruned. \n",
      "[I 2025-12-22 09:36:40,933] Trial 6 finished with value: 0.9972411630497076 and parameters: {'n_neighbors': 29, 'p': 3, 'metric': 'manhattan'}. Best is trial 4 with value: 0.9975547016537056.\n",
      "[I 2025-12-22 09:36:48,678] Trial 7 finished with value: 0.9974919467487672 and parameters: {'n_neighbors': 18, 'p': 5, 'metric': 'manhattan'}. Best is trial 4 with value: 0.9975547016537056.\n",
      "[I 2025-12-22 09:36:56,439] Trial 8 finished with value: 0.9972411630497076 and parameters: {'n_neighbors': 29, 'p': 2, 'metric': 'manhattan'}. Best is trial 4 with value: 0.9975547016537056.\n",
      "[I 2025-12-22 09:36:58,003] Trial 9 pruned. \n",
      "[I 2025-12-22 09:37:13,548] Trial 10 pruned. \n",
      "[I 2025-12-22 09:37:29,188] Trial 11 pruned. \n",
      "[I 2025-12-22 09:37:29,458] Trial 12 pruned. \n",
      "[I 2025-12-22 09:37:45,094] Trial 13 pruned. \n",
      "[I 2025-12-22 09:38:00,637] Trial 14 pruned. \n",
      "[I 2025-12-22 09:38:00,926] Trial 15 pruned. \n",
      "[I 2025-12-22 09:38:08,655] Trial 16 finished with value: 0.9981816609013547 and parameters: {'n_neighbors': 2, 'p': 1, 'metric': 'minkowski'}. Best is trial 16 with value: 0.9981816609013547.\n",
      "[I 2025-12-22 09:38:16,540] Trial 17 finished with value: 0.9978054656927074 and parameters: {'n_neighbors': 4, 'p': 1, 'metric': 'minkowski'}. Best is trial 16 with value: 0.9981816609013547.\n",
      "[I 2025-12-22 09:38:24,227] Trial 18 finished with value: 0.9981816609013547 and parameters: {'n_neighbors': 2, 'p': 1, 'metric': 'minkowski'}. Best is trial 16 with value: 0.9981816609013547.\n",
      "[I 2025-12-22 09:38:31,915] Trial 19 finished with value: 0.9981816609013547 and parameters: {'n_neighbors': 1, 'p': 1, 'metric': 'minkowski'}. Best is trial 16 with value: 0.9981816609013547.\n",
      "[I 2025-12-22 09:38:33,508] Trial 20 pruned. \n",
      "[I 2025-12-22 09:38:41,203] Trial 21 finished with value: 0.9981816609013547 and parameters: {'n_neighbors': 1, 'p': 1, 'metric': 'minkowski'}. Best is trial 16 with value: 0.9981816609013547.\n",
      "[I 2025-12-22 09:38:42,434] Trial 22 finished with value: 0.9978054853527654 and parameters: {'n_neighbors': 1, 'p': 2, 'metric': 'minkowski'}. Best is trial 16 with value: 0.9981816609013547.\n",
      "[I 2025-12-22 09:38:50,179] Trial 23 finished with value: 0.9980562493917671 and parameters: {'n_neighbors': 8, 'p': 1, 'metric': 'minkowski'}. Best is trial 16 with value: 0.9981816609013547.\n",
      "[I 2025-12-22 09:38:50,450] Trial 24 pruned. \n",
      "[I 2025-12-22 09:38:58,219] Trial 25 finished with value: 0.9979308575422372 and parameters: {'n_neighbors': 13, 'p': 1, 'metric': 'minkowski'}. Best is trial 16 with value: 0.9981816609013547.\n",
      "[I 2025-12-22 09:38:59,515] Trial 26 finished with value: 0.9978054853527654 and parameters: {'n_neighbors': 1, 'p': 1, 'metric': 'euclidean'}. Best is trial 16 with value: 0.9981816609013547.\n",
      "[I 2025-12-22 09:38:59,787] Trial 27 pruned. \n",
      "[I 2025-12-22 09:39:07,489] Trial 28 finished with value: 0.9980562493917671 and parameters: {'n_neighbors': 6, 'p': 1, 'metric': 'minkowski'}. Best is trial 16 with value: 0.9981816609013547.\n",
      "[I 2025-12-22 09:39:07,772] Trial 29 pruned. \n",
      "[I 2025-12-22 09:39:09,331] Trial 30 pruned. \n",
      "[I 2025-12-22 09:39:17,189] Trial 31 finished with value: 0.9978054656927074 and parameters: {'n_neighbors': 4, 'p': 1, 'metric': 'minkowski'}. Best is trial 16 with value: 0.9981816609013547.\n",
      "[I 2025-12-22 09:39:24,864] Trial 32 finished with value: 0.9981816609013547 and parameters: {'n_neighbors': 1, 'p': 1, 'metric': 'minkowski'}. Best is trial 16 with value: 0.9981816609013547.\n",
      "[I 2025-12-22 09:39:26,168] Trial 33 finished with value: 0.9978054853527654 and parameters: {'n_neighbors': 1, 'p': 2, 'metric': 'minkowski'}. Best is trial 16 with value: 0.9981816609013547.\n",
      "[I 2025-12-22 09:39:26,454] Trial 34 pruned. \n",
      "[I 2025-12-22 09:39:34,348] Trial 35 finished with value: 0.9972411630497076 and parameters: {'n_neighbors': 37, 'p': 1, 'metric': 'minkowski'}. Best is trial 16 with value: 0.9981816609013547.\n",
      "[I 2025-12-22 09:39:34,607] Trial 36 pruned. \n",
      "[I 2025-12-22 09:39:36,172] Trial 37 pruned. \n",
      "[I 2025-12-22 09:39:36,447] Trial 38 pruned. \n",
      "[I 2025-12-22 09:39:44,145] Trial 39 finished with value: 0.9978054656927074 and parameters: {'n_neighbors': 4, 'p': 2, 'metric': 'manhattan'}. Best is trial 16 with value: 0.9981816609013547.\n",
      "[I 2025-12-22 09:39:59,723] Trial 40 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best KNN hyperparameters: {'n_neighbors': 2, 'p': 1, 'metric': 'minkowski'}\n",
      "KNN Cross-val Accuracies: [0.99498433 0.99247649 0.99373041 0.99749216 0.9968652  0.99373041\n",
      " 0.99435737 0.99435737 0.99373041 0.99623588]\n",
      "KNN Mean CV Accuracy: 0.9947960022498161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['knn_floor_UJI.joblib']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN\n",
    "# --- Run the optimization ---\n",
    "study1 = optuna.create_study(direction='maximize')\n",
    "study1.optimize(objective_wknn2, n_trials=100, timeout=300)\n",
    "\n",
    "# --- Get best parameters ---\n",
    "best_params1 = study1.best_params\n",
    "print(\"Best KNN hyperparameters:\", best_params1)\n",
    "\n",
    "# --- Train final model ---\n",
    "knn_floor_final = KNeighborsClassifier(\n",
    "    n_neighbors=best_params1['n_neighbors'],\n",
    "    weights=knn_weight,\n",
    "    p=best_params1['p'],\n",
    "    metric=best_params1['metric']\n",
    ")\n",
    "\n",
    "# --- Cross-validation accuracy ---\n",
    "cv_scores_knn = cross_val_score(knn_floor_final, X_train2, y_train2, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "print(\"KNN Cross-val Accuracies:\", cv_scores_knn)\n",
    "print(\"KNN Mean CV Accuracy:\", cv_scores_knn.mean())\n",
    "\n",
    "# --- Train on full training set ---\n",
    "knn_floor_final.fit(X_train2, y_train2)\n",
    "joblib.dump(knn_floor_final, 'knn_floor_UJI.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "01d278c5-d00e-40bc-b8d3-539536346310",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:40:11,252] A new study created in memory with name: no-name-aad1131d-c512-4b9f-93ea-e3c0d4731d80\n",
      "[I 2025-12-22 09:40:16,924] Trial 0 finished with value: 0.9887765644245354 and parameters: {'max_depth': 8, 'n_estimators': 481, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.9887765644245354.\n",
      "[I 2025-12-22 09:40:38,759] Trial 1 finished with value: 0.9969276637658252 and parameters: {'max_depth': 36, 'n_estimators': 312, 'min_samples_split': 5, 'min_samples_leaf': 8, 'max_features': None}. Best is trial 1 with value: 0.9969276637658252.\n",
      "[I 2025-12-22 09:40:43,976] Trial 2 finished with value: 0.9977427304478267 and parameters: {'max_depth': 31, 'n_estimators': 373, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9977427304478267.\n",
      "[I 2025-12-22 09:40:52,173] Trial 3 finished with value: 0.9925387917518194 and parameters: {'max_depth': 15, 'n_estimators': 139, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 2 with value: 0.9977427304478267.\n",
      "[I 2025-12-22 09:41:12,962] Trial 4 finished with value: 0.9972411433896496 and parameters: {'max_depth': 36, 'n_estimators': 288, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 2 with value: 0.9977427304478267.\n",
      "[I 2025-12-22 09:41:13,463] Trial 5 pruned. \n",
      "[I 2025-12-22 09:41:13,943] Trial 6 pruned. \n",
      "[I 2025-12-22 09:41:14,917] Trial 7 pruned. \n",
      "[I 2025-12-22 09:41:16,115] Trial 8 pruned. \n",
      "[I 2025-12-22 09:41:17,031] Trial 9 pruned. \n",
      "[I 2025-12-22 09:41:18,720] Trial 10 finished with value: 0.997617318938239 and parameters: {'max_depth': 48, 'n_estimators': 71, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9977427304478267.\n",
      "[I 2025-12-22 09:41:20,440] Trial 11 finished with value: 0.9976800345230619 and parameters: {'max_depth': 48, 'n_estimators': 86, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9977427304478267.\n",
      "[I 2025-12-22 09:41:21,326] Trial 12 pruned. \n",
      "[I 2025-12-22 09:41:24,845] Trial 13 finished with value: 0.9978054263725916 and parameters: {'max_depth': 48, 'n_estimators': 235, 'min_samples_split': 10, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.9978054263725916.\n",
      "[I 2025-12-22 09:41:25,475] Trial 14 pruned. \n",
      "[I 2025-12-22 09:41:29,069] Trial 15 finished with value: 0.9974292115038864 and parameters: {'max_depth': None, 'n_estimators': 248, 'min_samples_split': 4, 'min_samples_leaf': 6, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.9978054263725916.\n",
      "[I 2025-12-22 09:41:34,120] Trial 16 finished with value: 0.9978054263725916 and parameters: {'max_depth': 49, 'n_estimators': 352, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.9978054263725916.\n",
      "[I 2025-12-22 09:41:38,908] Trial 17 finished with value: 0.9974292115038864 and parameters: {'max_depth': 49, 'n_estimators': 321, 'min_samples_split': 9, 'min_samples_leaf': 6, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.9978054263725916.\n",
      "[I 2025-12-22 09:41:39,569] Trial 18 pruned. \n",
      "[I 2025-12-22 09:41:41,288] Trial 19 pruned. \n",
      "[I 2025-12-22 09:41:47,173] Trial 20 finished with value: 0.9975546230134741 and parameters: {'max_depth': 33, 'n_estimators': 429, 'min_samples_split': 8, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.9978054263725916.\n",
      "[I 2025-12-22 09:41:48,200] Trial 21 pruned. \n",
      "[I 2025-12-22 09:41:49,134] Trial 22 pruned. \n",
      "[I 2025-12-22 09:41:53,438] Trial 23 finished with value: 0.9978054263725916 and parameters: {'max_depth': 49, 'n_estimators': 289, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.9978054263725916.\n",
      "[I 2025-12-22 09:41:57,699] Trial 24 finished with value: 0.9974919270887093 and parameters: {'max_depth': 49, 'n_estimators': 297, 'min_samples_split': 3, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.9978054263725916.\n",
      "[I 2025-12-22 09:42:01,709] Trial 25 finished with value: 0.9978054263725916 and parameters: {'max_depth': 35, 'n_estimators': 266, 'min_samples_split': 9, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.9978054263725916.\n",
      "[I 2025-12-22 09:42:05,193] Trial 26 finished with value: 0.9978054263725916 and parameters: {'max_depth': 49, 'n_estimators': 225, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.9978054263725916.\n",
      "[I 2025-12-22 09:42:09,823] Trial 27 finished with value: 0.9978054263725916 and parameters: {'max_depth': 40, 'n_estimators': 324, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.9978054263725916.\n",
      "[I 2025-12-22 09:42:11,684] Trial 28 pruned. \n",
      "[I 2025-12-22 09:42:14,744] Trial 29 finished with value: 0.9978054263725916 and parameters: {'max_depth': 47, 'n_estimators': 218, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.9978054263725916.\n",
      "[I 2025-12-22 09:42:19,660] Trial 30 finished with value: 0.9978054263725916 and parameters: {'max_depth': 44, 'n_estimators': 340, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.9978054263725916.\n",
      "[I 2025-12-22 09:42:23,542] Trial 31 finished with value: 0.9978054263725916 and parameters: {'max_depth': 35, 'n_estimators': 269, 'min_samples_split': 9, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.9978054263725916.\n",
      "[I 2025-12-22 09:42:24,267] Trial 32 pruned. \n",
      "[I 2025-12-22 09:42:25,078] Trial 33 pruned. \n",
      "[I 2025-12-22 09:42:25,816] Trial 34 pruned. \n",
      "[I 2025-12-22 09:42:29,451] Trial 35 pruned. \n",
      "[I 2025-12-22 09:42:29,916] Trial 36 pruned. \n",
      "[I 2025-12-22 09:42:30,502] Trial 37 pruned. \n",
      "[I 2025-12-22 09:42:49,290] Trial 38 finished with value: 0.9981816609013547 and parameters: {'max_depth': 50, 'n_estimators': 250, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 38 with value: 0.9981816609013547.\n",
      "[I 2025-12-22 09:43:07,825] Trial 39 finished with value: 0.9981816609013547 and parameters: {'max_depth': 50, 'n_estimators': 246, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 38 with value: 0.9981816609013547.\n",
      "[I 2025-12-22 09:43:21,490] Trial 40 finished with value: 0.9982443568261197 and parameters: {'max_depth': 50, 'n_estimators': 179, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 40 with value: 0.9982443568261197.\n",
      "[I 2025-12-22 09:43:35,325] Trial 41 finished with value: 0.9982443568261197 and parameters: {'max_depth': 50, 'n_estimators': 181, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 40 with value: 0.9982443568261197.\n",
      "[I 2025-12-22 09:43:46,951] Trial 42 finished with value: 0.9973038393144146 and parameters: {'max_depth': 25, 'n_estimators': 161, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 40 with value: 0.9982443568261197.\n",
      "[I 2025-12-22 09:43:56,885] Trial 43 finished with value: 0.9981189649765898 and parameters: {'max_depth': 50, 'n_estimators': 122, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 40 with value: 0.9982443568261197.\n",
      "[I 2025-12-22 09:43:57,930] Trial 44 pruned. \n",
      "[I 2025-12-22 09:44:07,978] Trial 45 finished with value: 0.998056269051825 and parameters: {'max_depth': 50, 'n_estimators': 121, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 40 with value: 0.9982443568261197.\n",
      "[I 2025-12-22 09:44:09,711] Trial 46 pruned. \n",
      "[I 2025-12-22 09:44:23,747] Trial 47 finished with value: 0.9982443568261197 and parameters: {'max_depth': 50, 'n_estimators': 185, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 40 with value: 0.9982443568261197.\n",
      "[I 2025-12-22 09:44:37,285] Trial 48 finished with value: 0.9981189649765898 and parameters: {'max_depth': 50, 'n_estimators': 179, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 40 with value: 0.9982443568261197.\n",
      "[I 2025-12-22 09:44:52,421] Trial 49 finished with value: 0.9981189649765898 and parameters: {'max_depth': 50, 'n_estimators': 198, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 40 with value: 0.9982443568261197.\n",
      "[I 2025-12-22 09:45:05,174] Trial 50 finished with value: 0.9982443568261197 and parameters: {'max_depth': 45, 'n_estimators': 167, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 40 with value: 0.9982443568261197.\n",
      "[I 2025-12-22 09:45:18,016] Trial 51 finished with value: 0.9973665548992374 and parameters: {'max_depth': 26, 'n_estimators': 174, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 40 with value: 0.9982443568261197.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest hyperparameters: {'max_depth': 50, 'n_estimators': 179, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': None}\n",
      "RF Cross-val Accuracies: [0.98808777 0.99247649 0.98808777 0.98808777 0.98808777 0.98746082\n",
      " 0.98871473 0.9862069  0.98934169 0.98619824]\n",
      "RF Mean CV Accuracy: 0.9882749967550728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rf_floor_pred_UJI.joblib']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "# --- Run the optimization ---\n",
    "study2 = optuna.create_study(direction='maximize')\n",
    "study2.optimize(objective_rf2, n_trials=100, timeout=300)\n",
    "\n",
    "# --- Train final model with best hyperparameters ---\n",
    "best_params2 = study2.best_params\n",
    "print(\"Best Random Forest hyperparameters:\", best_params2)\n",
    "\n",
    "rf_floor_final = RandomForestClassifier(\n",
    "    **best_params2,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# --- Cross-validation accuracy ---\n",
    "cv_scores_rf = cross_val_score(rf_floor_final, X_train2, y_train2, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "print(\"RF Cross-val Accuracies:\", cv_scores_rf)\n",
    "print(\"RF Mean CV Accuracy:\", cv_scores_rf.mean())\n",
    "\n",
    "# --- Train on full training set ---\n",
    "rf_floor_final.fit(X_train2, y_train2)\n",
    "joblib.dump(rf_floor_final, 'rf_floor_pred_UJI.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "34af0ca2-6e03-44b2-9bcf-50f9c65b3e40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:46:17,131] A new study created in memory with name: no-name-f5b06cfb-68c6-4fe9-a70d-0a3a34e4c530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[134]\tvalid_0's multi_error: 0.0015674\tvalid_0's multi_logloss: 0.138363\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[269]\tvalid_0's multi_error: 0.00250784\tvalid_0's multi_logloss: 0.0265777\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[121]\tvalid_0's multi_error: 0.00344828\tvalid_0's multi_logloss: 0.169471\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's multi_error: 0.00125392\tvalid_0's multi_logloss: 0.135051\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:46:52,921] Trial 0 finished with value: 0.9976173582583548 and parameters: {'learning_rate': 0.010475404404691522, 'num_leaves': 47, 'max_depth': 29, 'min_child_samples': 39}. Best is trial 0 with value: 0.9976173582583548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[107]\tvalid_0's multi_error: 0.00313578\tvalid_0's multi_logloss: 0.202043\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[412]\tvalid_0's multi_error: 0.00188088\tvalid_0's multi_logloss: 0.023644\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[413]\tvalid_0's multi_error: 0.00250784\tvalid_0's multi_logloss: 0.0256032\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[496]\tvalid_0's multi_error: 0.0031348\tvalid_0's multi_logloss: 0.0189442\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[590]\tvalid_0's multi_error: 0.00125392\tvalid_0's multi_logloss: 0.0107823\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:47:17,393] Trial 1 finished with value: 0.997491927088709 and parameters: {'learning_rate': 0.012312558724836489, 'num_leaves': 67, 'max_depth': 4, 'min_child_samples': 28}. Best is trial 0 with value: 0.9976173582583548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[571]\tvalid_0's multi_error: 0.00376294\tvalid_0's multi_logloss: 0.015723\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[87]\tvalid_0's multi_error: 0.00031348\tvalid_0's multi_logloss: 0.00170157\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's multi_error: 0.00282132\tvalid_0's multi_logloss: 0.0362717\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid_0's multi_error: 0.00188088\tvalid_0's multi_logloss: 0.00866098\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[60]\tvalid_0's multi_error: 0.000626959\tvalid_0's multi_logloss: 0.00352483\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:47:35,484] Trial 2 finished with value: 0.9983697486756494 and parameters: {'learning_rate': 0.09745295001906112, 'num_leaves': 68, 'max_depth': 22, 'min_child_samples': 89}. Best is trial 2 with value: 0.9983697486756494.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[46]\tvalid_0's multi_error: 0.00250862\tvalid_0's multi_logloss: 0.00898693\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[62]\tvalid_0's multi_error: 0.00031348\tvalid_0's multi_logloss: 0.00164252\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's multi_error: 0.0031348\tvalid_0's multi_logloss: 0.0297808\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[65]\tvalid_0's multi_error: 0.00219436\tvalid_0's multi_logloss: 0.00480928\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[125]\tvalid_0's multi_error: 0.000626959\tvalid_0's multi_logloss: 0.00184277\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:47:50,716] Trial 3 finished with value: 0.9982443568261197 and parameters: {'learning_rate': 0.11615040736882115, 'num_leaves': 49, 'max_depth': 12, 'min_child_samples': 7}. Best is trial 2 with value: 0.9983697486756494.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[55]\tvalid_0's multi_error: 0.00250862\tvalid_0's multi_logloss: 0.00619092\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[64]\tvalid_0's multi_error: 0.0015674\tvalid_0's multi_logloss: 0.082437\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[127]\tvalid_0's multi_error: 0.0031348\tvalid_0's multi_logloss: 0.0161388\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[201]\tvalid_0's multi_error: 0.00250784\tvalid_0's multi_logloss: 0.00605186\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[388]\tvalid_0's multi_error: 0.000940439\tvalid_0's multi_logloss: 0.00272774\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:48:19,548] Trial 4 finished with value: 0.9976800345230619 and parameters: {'learning_rate': 0.028253454729263085, 'num_leaves': 43, 'max_depth': 12, 'min_child_samples': 6}. Best is trial 2 with value: 0.9983697486756494.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's multi_error: 0.00344936\tvalid_0's multi_logloss: 0.422874\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's multi_error: 0.000626959\tvalid_0's multi_logloss: 0.00787814\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's multi_error: 0.00250784\tvalid_0's multi_logloss: 0.0368026\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid_0's multi_error: 0.00188088\tvalid_0's multi_logloss: 0.00474218\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's multi_error: 0.000626959\tvalid_0's multi_logloss: 0.00693854\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:48:37,258] Trial 5 finished with value: 0.9983697486756494 and parameters: {'learning_rate': 0.157351281119234, 'num_leaves': 99, 'max_depth': 26, 'min_child_samples': 56}. Best is trial 2 with value: 0.9983697486756494.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's multi_error: 0.00250862\tvalid_0's multi_logloss: 0.0100318\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's multi_error: 0.000626959\tvalid_0's multi_logloss: 0.00602458\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's multi_error: 0.0031348\tvalid_0's multi_logloss: 0.024515\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[30]\tvalid_0's multi_error: 0.00219436\tvalid_0's multi_logloss: 0.00448305\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[61]\tvalid_0's multi_error: 0.000626959\tvalid_0's multi_logloss: 0.00211161\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:48:51,612] Trial 6 finished with value: 0.9981816609013547 and parameters: {'learning_rate': 0.23112981959985265, 'num_leaves': 74, 'max_depth': 28, 'min_child_samples': 8}. Best is trial 2 with value: 0.9983697486756494.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's multi_error: 0.00250862\tvalid_0's multi_logloss: 0.00968427\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[202]\tvalid_0's multi_error: 0.00031348\tvalid_0's multi_logloss: 0.00209996\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[93]\tvalid_0's multi_error: 0.00282132\tvalid_0's multi_logloss: 0.0224109\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[125]\tvalid_0's multi_error: 0.00188088\tvalid_0's multi_logloss: 0.00905884\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[209]\tvalid_0's multi_error: 0.000626959\tvalid_0's multi_logloss: 0.00271964\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:49:14,074] Trial 7 finished with value: 0.9983697486756494 and parameters: {'learning_rate': 0.03420960455198466, 'num_leaves': 26, 'max_depth': 27, 'min_child_samples': 100}. Best is trial 2 with value: 0.9983697486756494.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[125]\tvalid_0's multi_error: 0.00250862\tvalid_0's multi_logloss: 0.0100357\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's multi_error: 0.00031348\tvalid_0's multi_logloss: 0.00175707\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[30]\tvalid_0's multi_error: 0.00250784\tvalid_0's multi_logloss: 0.0316523\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[55]\tvalid_0's multi_error: 0.00188088\tvalid_0's multi_logloss: 0.00581399\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[51]\tvalid_0's multi_error: 0.000626959\tvalid_0's multi_logloss: 0.00505022\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:49:31,816] Trial 8 finished with value: 0.9984324446004142 and parameters: {'learning_rate': 0.0901502558225774, 'num_leaves': 52, 'max_depth': 20, 'min_child_samples': 51}. Best is trial 8 with value: 0.9984324446004142.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[55]\tvalid_0's multi_error: 0.00250862\tvalid_0's multi_logloss: 0.00707862\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[82]\tvalid_0's multi_error: 0.00031348\tvalid_0's multi_logloss: 0.00154421\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid_0's multi_error: 0.00250784\tvalid_0's multi_logloss: 0.0474931\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid_0's multi_error: 0.00188088\tvalid_0's multi_logloss: 0.00594595\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid_0's multi_error: 0.000626959\tvalid_0's multi_logloss: 0.00519361\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:49:49,031] Trial 9 finished with value: 0.9984324446004142 and parameters: {'learning_rate': 0.10651294041775781, 'num_leaves': 59, 'max_depth': 20, 'min_child_samples': 55}. Best is trial 8 with value: 0.9984324446004142.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's multi_error: 0.00250862\tvalid_0's multi_logloss: 0.00747059\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's multi_error: 0.00031348\tvalid_0's multi_logloss: 0.00535812\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's multi_error: 0.00282132\tvalid_0's multi_logloss: 0.0234803\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's multi_error: 0.00188088\tvalid_0's multi_logloss: 0.00472504\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's multi_error: 0.000626959\tvalid_0's multi_logloss: 0.00362322\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:49:54,854] Trial 10 finished with value: 0.9983697486756494 and parameters: {'learning_rate': 0.47849555256461995, 'num_leaves': 25, 'max_depth': 15, 'min_child_samples': 73}. Best is trial 8 with value: 0.9984324446004142.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's multi_error: 0.00250862\tvalid_0's multi_logloss: 0.00724341\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's multi_error: 0.00031348\tvalid_0's multi_logloss: 0.00169768\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[48]\tvalid_0's multi_error: 0.00282132\tvalid_0's multi_logloss: 0.0324784\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[88]\tvalid_0's multi_error: 0.00188088\tvalid_0's multi_logloss: 0.00589624\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[81]\tvalid_0's multi_error: 0.000626959\tvalid_0's multi_logloss: 0.00555503\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:50:26,287] Trial 11 finished with value: 0.9983697486756494 and parameters: {'learning_rate': 0.05653866520365691, 'num_leaves': 84, 'max_depth': 20, 'min_child_samples': 58}. Best is trial 8 with value: 0.9984324446004142.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[85]\tvalid_0's multi_error: 0.00250862\tvalid_0's multi_logloss: 0.00746563\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's multi_error: 0.00031348\tvalid_0's multi_logloss: 0.00381681\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's multi_error: 0.00250784\tvalid_0's multi_logloss: 0.0475387\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's multi_error: 0.00188088\tvalid_0's multi_logloss: 0.00636068\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's multi_error: 0.000626959\tvalid_0's multi_logloss: 0.00504688\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:50:36,237] Trial 12 finished with value: 0.9984324446004142 and parameters: {'learning_rate': 0.24753005551253426, 'num_leaves': 54, 'max_depth': 20, 'min_child_samples': 38}. Best is trial 8 with value: 0.9984324446004142.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's multi_error: 0.00250862\tvalid_0's multi_logloss: 0.00637901\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[150]\tvalid_0's multi_error: 0.00031348\tvalid_0's multi_logloss: 0.00167463\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's multi_error: 0.00282132\tvalid_0's multi_logloss: 0.0433747\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[93]\tvalid_0's multi_error: 0.00188088\tvalid_0's multi_logloss: 0.00561416\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[141]\tvalid_0's multi_error: 0.000626959\tvalid_0's multi_logloss: 0.00242105\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:50:55,525] Trial 13 finished with value: 0.9983697486756494 and parameters: {'learning_rate': 0.05518896809253837, 'num_leaves': 38, 'max_depth': 16, 'min_child_samples': 71}. Best is trial 8 with value: 0.9984324446004142.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[92]\tvalid_0's multi_error: 0.00250862\tvalid_0's multi_logloss: 0.00692572\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:50:59,724] Trial 14 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[57]\tvalid_0's multi_error: 0.000626959\tvalid_0's multi_logloss: 0.00648697\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[339]\tvalid_0's multi_error: 0.00031348\tvalid_0's multi_logloss: 0.00238446\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[131]\tvalid_0's multi_error: 0.00250784\tvalid_0's multi_logloss: 0.0259369\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[154]\tvalid_0's multi_error: 0.00344828\tvalid_0's multi_logloss: 0.01964\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[319]\tvalid_0's multi_error: 0.000626959\tvalid_0's multi_logloss: 0.00316815\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:51:16,759] Trial 15 finished with value: 0.9978681026372985 and parameters: {'learning_rate': 0.03178769721871055, 'num_leaves': 79, 'max_depth': 5, 'min_child_samples': 44}. Best is trial 8 with value: 0.9984324446004142.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[164]\tvalid_0's multi_error: 0.00376294\tvalid_0's multi_logloss: 0.0178237\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's multi_error: 0.00031348\tvalid_0's multi_logloss: 0.00289249\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's multi_error: 0.00250784\tvalid_0's multi_logloss: 0.0551039\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[63]\tvalid_0's multi_error: 0.00188088\tvalid_0's multi_logloss: 0.00364941\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's multi_error: 0.000626959\tvalid_0's multi_logloss: 0.00338484\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:51:26,870] Trial 16 finished with value: 0.9984324446004142 and parameters: {'learning_rate': 0.16560871840565755, 'num_leaves': 36, 'max_depth': 11, 'min_child_samples': 23}. Best is trial 8 with value: 0.9984324446004142.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's multi_error: 0.00250862\tvalid_0's multi_logloss: 0.00621761\n",
      "Best LightGBM hyperparameters: {'learning_rate': 0.0901502558225774, 'num_leaves': 52, 'max_depth': 20, 'min_child_samples': 51}\n",
      "LGB Cross-val Accuracies: [0.9968652  0.99561129 0.99623824 0.99561129 0.99623824 0.99373041\n",
      " 0.99498433 0.99561129 0.99310345 0.99623588]\n",
      "LGB Mean CV Accuracy: 0.995422961497465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lgb_floor_pred_UJI.joblib']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Optuna study\n",
    "study3 = optuna.create_study(direction='maximize')\n",
    "study3.optimize(objective_lgb2, n_trials=100, timeout=300)\n",
    "\n",
    "# Best params\n",
    "best_params3 = study3.best_params\n",
    "print(\"Best LightGBM hyperparameters:\", best_params3)\n",
    "\n",
    "best_params3.update({\n",
    "    'random_state': SEED,\n",
    "    'n_jobs': -1,\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': len(np.unique(y_train))\n",
    "})\n",
    "\n",
    "# Final model\n",
    "lgb_floor_final = lgb.LGBMClassifier(**best_params3)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores_lgb = cross_val_score(lgb_floor_final, X_train2, y_train2, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "print(\"LGB Cross-val Accuracies:\", cv_scores_lgb)\n",
    "print(\"LGB Mean CV Accuracy:\", cv_scores_lgb.mean())\n",
    "\n",
    "# Train final model\n",
    "lgb_floor_final.fit(X_train2, y_train2)\n",
    "\n",
    "# Save\n",
    "joblib.dump(lgb_floor_final, 'lgb_floor_pred_UJI.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5056e1de-08cc-4df8-aeef-5213000fccc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:52:49,867] A new study created in memory with name: no-name-3a241a1d-3bce-4a03-aca6-b33e28470ff7\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:52:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:52:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:52:56] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:52:58] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:53:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:53:04,603] Trial 0 finished with value: 0.9972411237295915 and parameters: {'n_estimators': 175, 'learning_rate': 0.12207342725299752, 'max_depth': 9, 'min_child_weight': 7.460882136724882, 'gamma': 1.774203499157565, 'subsample': 0.5068036106898854, 'colsample_bytree': 0.5980249576960881}. Best is trial 0 with value: 0.9972411237295915.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:53:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:53:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:53:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:53:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:53:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:53:20,172] Trial 1 finished with value: 0.9970530556153548 and parameters: {'n_estimators': 78, 'learning_rate': 0.04145237999662607, 'max_depth': 9, 'min_child_weight': 6.454500367021479, 'gamma': 1.830260454952744, 'subsample': 0.5371854531433431, 'colsample_bytree': 0.9536717261932746}. Best is trial 0 with value: 0.9972411237295915.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:53:20] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:53:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:53:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:53:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:53:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:53:32,173] Trial 2 finished with value: 0.997115731880062 and parameters: {'n_estimators': 194, 'learning_rate': 0.36974528985713223, 'max_depth': 6, 'min_child_weight': 8.292252923917939, 'gamma': 2.1685712694596453, 'subsample': 0.6563879057484314, 'colsample_bytree': 0.981876590557575}. Best is trial 0 with value: 0.9972411237295915.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:53:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:53:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:53:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:53:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:53:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:53:47,486] Trial 3 finished with value: 0.9977427107877688 and parameters: {'n_estimators': 176, 'learning_rate': 0.15060489564454488, 'max_depth': 3, 'min_child_weight': 3.8398202876998093, 'gamma': 0.06565232246651898, 'subsample': 0.5771966470046568, 'colsample_bytree': 0.907064960012731}. Best is trial 3 with value: 0.9977427107877688.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:53:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:53:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:53:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:53:56] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:53:59] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:54:02,731] Trial 4 finished with value: 0.997053055615355 and parameters: {'n_estimators': 77, 'learning_rate': 0.04077499119680866, 'max_depth': 8, 'min_child_weight': 1.522524979875043, 'gamma': 3.6326251164873784, 'subsample': 0.593458343912034, 'colsample_bytree': 0.948633293202434}. Best is trial 3 with value: 0.9977427107877688.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:54:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:54:04,548] Trial 5 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:54:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:54:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:54:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:54:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:54:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:54:14,303] Trial 6 finished with value: 0.997617318938239 and parameters: {'n_estimators': 129, 'learning_rate': 0.3118021272607557, 'max_depth': 4, 'min_child_weight': 6.893062337114447, 'gamma': 1.5798980688916386, 'subsample': 0.8787905803810496, 'colsample_bytree': 0.6147172954457816}. Best is trial 3 with value: 0.9977427107877688.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:54:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:54:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:54:20] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:54:23] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:54:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:54:28,427] Trial 7 finished with value: 0.9974919270887093 and parameters: {'n_estimators': 111, 'learning_rate': 0.08830415244772089, 'max_depth': 4, 'min_child_weight': 3.2211249176830012, 'gamma': 0.8533823872577673, 'subsample': 0.8808622181427768, 'colsample_bytree': 0.7776278886732213}. Best is trial 3 with value: 0.9977427107877688.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:54:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:54:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:54:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:54:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:54:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:54:49,893] Trial 8 finished with value: 0.9973665352391794 and parameters: {'n_estimators': 187, 'learning_rate': 0.05478563739711284, 'max_depth': 8, 'min_child_weight': 4.226593690651791, 'gamma': 4.382202937944876, 'subsample': 0.8118586761021112, 'colsample_bytree': 0.9142485933232793}. Best is trial 3 with value: 0.9977427107877688.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:54:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:54:51,121] Trial 9 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:54:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:54:55,225] Trial 10 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:54:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:54:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:54:59] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:55:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:55:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:55:04,469] Trial 11 finished with value: 0.9975546033534162 and parameters: {'n_estimators': 138, 'learning_rate': 0.30772197468213, 'max_depth': 3, 'min_child_weight': 4.5665540726999625, 'gamma': 3.228100567170139, 'subsample': 0.7882007683001316, 'colsample_bytree': 0.692858873085344}. Best is trial 3 with value: 0.9977427107877688.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:55:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:55:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:55:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:55:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:55:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:55:15,135] Trial 12 finished with value: 0.9976800148630038 and parameters: {'n_estimators': 120, 'learning_rate': 0.18042678726023637, 'max_depth': 4, 'min_child_weight': 6.638752140268114, 'gamma': 1.0407758704654153, 'subsample': 0.9995359656076035, 'colsample_bytree': 0.7063731717916768}. Best is trial 3 with value: 0.9977427107877688.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:55:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:55:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:55:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:55:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:55:24] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:55:26,154] Trial 13 finished with value: 0.9976800148630038 and parameters: {'n_estimators': 100, 'learning_rate': 0.18625074913918235, 'max_depth': 4, 'min_child_weight': 4.652119540643148, 'gamma': 0.019219973085150777, 'subsample': 0.9772265917372569, 'colsample_bytree': 0.8520892960261391}. Best is trial 3 with value: 0.9977427107877688.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:55:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:55:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:55:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:55:34] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:55:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:55:39,466] Trial 14 finished with value: 0.9977427107877688 and parameters: {'n_estimators': 160, 'learning_rate': 0.17387723804489308, 'max_depth': 5, 'min_child_weight': 3.2420632339030986, 'gamma': 0.8710985254369604, 'subsample': 0.6887660483484935, 'colsample_bytree': 0.7213527825537367}. Best is trial 3 with value: 0.9977427107877688.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:55:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:55:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:55:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:55:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:55:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:55:54,121] Trial 15 finished with value: 0.9977427107877688 and parameters: {'n_estimators': 160, 'learning_rate': 0.16157682083922897, 'max_depth': 5, 'min_child_weight': 1.1138368512834873, 'gamma': 0.5806028235816203, 'subsample': 0.6759921085432389, 'colsample_bytree': 0.8654784367245786}. Best is trial 3 with value: 0.9977427107877688.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:55:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:55:59,319] Trial 16 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:55:59] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:56:09,177] Trial 17 finished with value: 0.9976800148630038 and parameters: {'n_estimators': 145, 'learning_rate': 0.46532896181688493, 'max_depth': 5, 'min_child_weight': 3.7119850123063904, 'gamma': 1.2889299131597336, 'subsample': 0.7359030190663853, 'colsample_bytree': 0.7187118845593645}. Best is trial 3 with value: 0.9977427107877688.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:56:24,780] Trial 18 finished with value: 0.9977427107877688 and parameters: {'n_estimators': 176, 'learning_rate': 0.13425652410001637, 'max_depth': 3, 'min_child_weight': 2.2290659462118576, 'gamma': 0.37081489163149495, 'subsample': 0.6349777533473115, 'colsample_bytree': 0.6655294658547094}. Best is trial 3 with value: 0.9977427107877688.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:33] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:56:39,311] Trial 19 finished with value: 0.997617318938239 and parameters: {'n_estimators': 199, 'learning_rate': 0.23686929796419173, 'max_depth': 10, 'min_child_weight': 5.373706719118093, 'gamma': 1.3599526021712534, 'subsample': 0.7240984781303985, 'colsample_bytree': 0.8403019908556771}. Best is trial 3 with value: 0.9977427107877688.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:56:42,020] Trial 20 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:56:55,315] Trial 21 finished with value: 0.9977427107877688 and parameters: {'n_estimators': 159, 'learning_rate': 0.2126720167023629, 'max_depth': 5, 'min_child_weight': 2.2214681191017047, 'gamma': 0.5133030324542684, 'subsample': 0.6733269762062993, 'colsample_bytree': 0.8870586705387078}. Best is trial 3 with value: 0.9977427107877688.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:56:58] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:57:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:57:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:57:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:57:12,310] Trial 22 finished with value: 0.9977427107877688 and parameters: {'n_estimators': 147, 'learning_rate': 0.10831492892375276, 'max_depth': 5, 'min_child_weight': 1.042602797195222, 'gamma': 0.39070147271770095, 'subsample': 0.7660137857039909, 'colsample_bytree': 0.8544877904798326}. Best is trial 3 with value: 0.9977427107877688.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:57:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:57:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:57:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:57:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:57:24] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:57:27,091] Trial 23 finished with value: 0.997617318938239 and parameters: {'n_estimators': 166, 'learning_rate': 0.1570568659688693, 'max_depth': 7, 'min_child_weight': 1.6362857301324218, 'gamma': 0.9679576986648383, 'subsample': 0.6939883847241105, 'colsample_bytree': 0.9003821325349903}. Best is trial 3 with value: 0.9977427107877688.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:57:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:57:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:57:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:57:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:57:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:57:41,555] Trial 24 finished with value: 0.9977427107877688 and parameters: {'n_estimators': 184, 'learning_rate': 0.2531845032501875, 'max_depth': 4, 'min_child_weight': 3.4581087646468758, 'gamma': 0.050926058320951206, 'subsample': 0.6352331918357959, 'colsample_bytree': 0.7970334014601804}. Best is trial 3 with value: 0.9977427107877688.\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:57:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:57:44,163] Trial 25 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:57:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:57:47,734] Trial 26 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:57:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:57:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:57:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:57:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:57:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-12-22 09:57:59,131] Trial 27 finished with value: 0.9976800148630038 and parameters: {'n_estimators': 169, 'learning_rate': 0.4917266757783348, 'max_depth': 6, 'min_child_weight': 2.761655722656681, 'gamma': 0.32021551327131725, 'subsample': 0.6066044293448568, 'colsample_bytree': 0.8196332361244304}. Best is trial 3 with value: 0.9977427107877688.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost hyperparameters: {'n_estimators': 176, 'learning_rate': 0.15060489564454488, 'max_depth': 3, 'min_child_weight': 3.8398202876998093, 'gamma': 0.06565232246651898, 'subsample': 0.5771966470046568, 'colsample_bytree': 0.907064960012731}\n",
      "XGB Cross-val Accuracies: [0.99561129 0.99184953 0.99310345 0.99623824 0.99247649 0.99498433\n",
      " 0.99373041 0.99498433 0.99310345 0.99372647]\n",
      "XGB Mean CV Accuracy: 0.9939807978980738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [09:58:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgb_floor_pred_UJI.joblib']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoost\n",
    "# --- Run optimization ---\n",
    "study4 = optuna.create_study(direction='maximize')\n",
    "study4.optimize(objective_xgb2, n_trials=100, timeout=300)\n",
    "\n",
    "# --- Best parameters ---\n",
    "best_params4 = study4.best_params\n",
    "print(\"Best XGBoost hyperparameters:\", best_params4)\n",
    "\n",
    "# --- Train final model with best params ---\n",
    "xgb_floor_final = XGBClassifier(**best_params4, use_label_encoder=False, eval_metric='mlogloss', random_state=SEED, n_jobs=-1)\n",
    "\n",
    "# --- Cross-validation accuracy ---\n",
    "cv_scores_xgb = cross_val_score(xgb_floor_final, X_train2, y_train2, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "print(\"XGB Cross-val Accuracies:\", cv_scores_xgb)\n",
    "print(\"XGB Mean CV Accuracy:\", cv_scores_xgb.mean())\n",
    "\n",
    "# --- Train on full training set ---\n",
    "xgb_floor_final.fit(X_train2, y_train2)\n",
    "joblib.dump(xgb_floor_final, 'xgb_floor_pred_UJI.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3fc09250-e4df-4680-8f64-a1d5fde45864",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:59:03,982] A new study created in memory with name: no-name-713c70e3-ecf5-4886-96da-8211dabbbba2\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:04,959] Trial 0 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 1.0441799570671247e-07}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:05,892] Trial 1 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 9.912425691990198e-07}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:06,809] Trial 2 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 9.506684104268001e-08}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:07,730] Trial 3 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 1.0289182740503482e-07}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:08,655] Trial 4 finished with value: 0.996049802858769 and parameters: {'var_smoothing': 3.652610093499098e-12}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:08,843] Trial 5 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:09,785] Trial 6 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 4.301842911607464e-08}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:09,994] Trial 7 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:10,192] Trial 8 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:10,387] Trial 9 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:10,581] Trial 10 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:11,521] Trial 11 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 4.4472076557039585e-07}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:12,620] Trial 12 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 5.927311497526784e-07}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:13,593] Trial 13 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 9.110916134564359e-07}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:13,793] Trial 14 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:13,987] Trial 15 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:14,930] Trial 16 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 1.4367708794220752e-07}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:15,122] Trial 17 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:15,314] Trial 18 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:16,251] Trial 19 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 2.6692250185508545e-07}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:16,453] Trial 20 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:17,391] Trial 21 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 6.535857193871865e-08}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:18,327] Trial 22 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 2.0417713061270873e-07}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:19,269] Trial 23 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 9.246191244207753e-07}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:20,230] Trial 24 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 9.022467857384948e-08}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:21,202] Trial 25 finished with value: 0.9976173779184127 and parameters: {'var_smoothing': 2.7450247112551078e-08}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:21,406] Trial 26 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:22,560] Trial 27 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 2.512808018916161e-07}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:22,780] Trial 28 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:23,738] Trial 29 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 7.120018434480891e-08}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:24,671] Trial 30 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 1.3475880814332815e-07}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:24,865] Trial 31 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:25,830] Trial 32 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 4.2040530387356234e-07}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:26,789] Trial 33 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 3.816938151651024e-08}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:26,993] Trial 34 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:27,936] Trial 35 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 4.3582493667625474e-08}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:28,133] Trial 36 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:29,073] Trial 37 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 1.1797741006047421e-07}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:30,027] Trial 38 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 3.4949750903964126e-07}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:30,227] Trial 39 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:31,151] Trial 40 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 9.465438807350758e-07}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:32,245] Trial 41 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 4.5814399507149655e-08}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:33,216] Trial 42 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 4.963482677972962e-07}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:33,407] Trial 43 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:34,353] Trial 44 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 1.8326819796124573e-07}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:34,546] Trial 45 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:34,739] Trial 46 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:34,931] Trial 47 pruned. \n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:35,871] Trial 48 finished with value: 0.9976173779184127 and parameters: {'var_smoothing': 2.663604357295384e-08}. Best is trial 0 with value: 0.9981189453165319.\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_28532\\1647267290.py:192: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'var_smoothing': trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
      "[I 2025-12-22 09:59:36,813] Trial 49 finished with value: 0.9981189453165319 and parameters: {'var_smoothing': 7.952593549047552e-08}. Best is trial 0 with value: 0.9981189453165319.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Naive Bayes hyperparameters: {'var_smoothing': 1.0441799570671247e-07}\n",
      "GNB Cross-val Accuracies: [0.72664577 0.70909091 0.71347962 0.73981191 0.71661442 0.72100313\n",
      " 0.7153605  0.74420063 0.70783699 0.72459222]\n",
      "GNB Mean CV Accuracy: 0.7218636107975441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['gnb_floor_pred_UJI.joblib']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gaussian Naive Bayes\n",
    "# --- Run the optimization ---\n",
    "study7 = optuna.create_study(direction='maximize')\n",
    "study7.optimize(objective_gnb2, n_trials=50, timeout=200)\n",
    "\n",
    "# --- Train final model with best hyperparameters ---\n",
    "best_params7 = study7.best_params\n",
    "print(\"Best Naive Bayes hyperparameters:\", best_params7)\n",
    "\n",
    "gnb_floor_final = GaussianNB(**best_params7)\n",
    "\n",
    "# --- Cross-validation accuracy ---\n",
    "cv_scores_gnb = cross_val_score(gnb_floor_final, X_train2, y_train2, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "print(\"GNB Cross-val Accuracies:\", cv_scores_gnb)\n",
    "print(\"GNB Mean CV Accuracy:\", cv_scores_gnb.mean())\n",
    "\n",
    "# --- Train on full training set ---\n",
    "gnb_floor_final.fit(X_train2, y_train2)\n",
    "joblib.dump(gnb_floor_final, 'gnb_floor_pred_UJI.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "44199816-2c18-406b-8473-f521b6ca6605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 09:59:40,282] A new study created in memory with name: no-name-0a56eba8-9399-4f3e-bf49-0235bdcb3fa7\n",
      "[I 2025-12-22 10:01:47,435] Trial 0 finished with value: 0.9638849257488762 and parameters: {'criterion': 'entropy', 'max_depth': 4, 'min_samples_split': 6, 'min_samples_leaf': 10, 'n_estimators': 98, 'learning_rate': 0.32266071736848795}. Best is trial 0 with value: 0.9638849257488762.\n",
      "[I 2025-12-22 10:13:13,035] Trial 1 finished with value: 0.9900308171408181 and parameters: {'criterion': 'entropy', 'max_depth': 8, 'min_samples_split': 12, 'min_samples_leaf': 3, 'n_estimators': 298, 'learning_rate': 0.7567225290631958}. Best is trial 1 with value: 0.9900308171408181.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'criterion': 'entropy', 'max_depth': 8, 'min_samples_split': 12, 'min_samples_leaf': 3, 'n_estimators': 298, 'learning_rate': 0.7567225290631958}\n",
      "AdaBoost Cross-val Accuracies: [0.99373041 0.99247649 0.98996865 0.99184953 0.99247649 0.99247649\n",
      " 0.99435737 0.99310345 0.99498433 0.98870765]\n",
      "AdaBoost Mean CV Accuracy: 0.9924130851193544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['adaboost_floor_pred_UJI.joblib']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import joblib\n",
    "\n",
    "# --- Run the optimization ---\n",
    "study_ab = optuna.create_study(direction='maximize')\n",
    "study_ab.optimize(objective_adaboost2, n_trials=100, timeout=300)\n",
    "\n",
    "# --- Train final model with best hyperparameters ---\n",
    "best_params_ab = study_ab.best_params\n",
    "print(\"Best hyperparameters:\", best_params_ab)\n",
    "\n",
    "# Extract AdaBoost-specific parameters\n",
    "ada_params = {\n",
    "    'n_estimators': best_params_ab.pop('n_estimators'),\n",
    "    'learning_rate': best_params_ab.pop('learning_rate'),\n",
    "    'random_state': SEED\n",
    "}\n",
    "\n",
    "# Remaining params go to base estimator (DecisionTree)\n",
    "base_estimator = DecisionTreeClassifier(**best_params_ab, random_state=SEED)\n",
    "\n",
    "# Build final AdaBoost model\n",
    "ab_floor_final = AdaBoostClassifier(\n",
    "    estimator=base_estimator,\n",
    "    **ada_params\n",
    ")\n",
    "\n",
    "# --- Cross-validation accuracy ---\n",
    "cv_scores_ab = cross_val_score(ab_floor_final, X_train2, y_train2, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "print(\"AdaBoost Cross-val Accuracies:\", cv_scores_ab)\n",
    "print(\"AdaBoost Mean CV Accuracy:\", cv_scores_ab.mean())\n",
    "\n",
    "# --- Train on full training set ---\n",
    "ab_floor_final.fit(X_train2, y_train2)\n",
    "joblib.dump(ab_floor_final, 'adaboost_floor_pred_UJI.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ab83e201-eda6-4ace-ac9a-85ee9f3782d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models2 = [\n",
    "        (\"KNN\", knn_floor_final),\n",
    "        (\"RF\", rf_floor_final),\n",
    "        (\"LGB\", lgb_floor_final),\n",
    "        (\"XGB\", xgb_floor_final),\n",
    "        (\"NB\", gnb_floor_final),\n",
    "        (\"AB\", ab_floor_final)\n",
    "    ]\n",
    "\n",
    "accuracy_knn = cv_scores_knn.mean()\n",
    "accuracy_rf = cv_scores_rf.mean()\n",
    "accuracy_lgb = cv_scores_lgb.mean()\n",
    "accuracy_xgb = cv_scores_xgb.mean()\n",
    "accuracy_gnb = cv_scores_gnb.mean()\n",
    "accuracy_ab = cv_scores_ab.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5994d136-1105-416c-b46a-4f8fc9d9e698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ENSEMBLE MEMORY FOOTPRINT =====\n",
      "KNN: Disk = 56.71 MB | RAM = 56.71 MB\n",
      "RF: Disk = 18.86 MB | RAM = 18.85 MB\n",
      "LGB: Disk = 6.61 MB | RAM = 6.61 MB\n",
      "XGB: Disk = 2.10 MB | RAM = 2.10 MB\n",
      "NB: Disk = 0.10 MB | RAM = 0.10 MB\n",
      "AB: Disk = 10.64 MB | RAM = 10.63 MB\n",
      "------------------------------------\n",
      "TOTAL ENSEMBLE DISK SIZE: 95.02 MB\n",
      "TOTAL ENSEMBLE RAM SIZE : 94.99 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pickle\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "# ===== Utility functions =====\n",
    "def model_disk_size_mb(model):\n",
    "    \"\"\"Serialized model size on disk (deployment footprint)\"\"\"\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".joblib\") as tmp:\n",
    "        joblib.dump(model, tmp.name)\n",
    "        size_mb = os.path.getsize(tmp.name) / (1024 ** 2)\n",
    "    os.remove(tmp.name)\n",
    "    return size_mb\n",
    "\n",
    "\n",
    "def model_ram_size_mb(model):\n",
    "    \"\"\"In-memory size (RAM footprint)\"\"\"\n",
    "    return sys.getsizeof(pickle.dumps(model)) / (1024 ** 2)\n",
    "\n",
    "\n",
    "# ===== Compute footprint =====\n",
    "disk_sizes = {}\n",
    "ram_sizes = {}\n",
    "\n",
    "for name, model in models2:\n",
    "    disk_sizes[name] = model_disk_size_mb(model)\n",
    "    ram_sizes[name] = model_ram_size_mb(model)\n",
    "\n",
    "# ===== Total ensemble footprint =====\n",
    "total_disk_mb = sum(disk_sizes.values())\n",
    "total_ram_mb = sum(ram_sizes.values())\n",
    "\n",
    "print(\"===== ENSEMBLE MEMORY FOOTPRINT =====\")\n",
    "for name in disk_sizes:\n",
    "    print(f\"{name}: Disk = {disk_sizes[name]:.2f} MB | RAM = {ram_sizes[name]:.2f} MB\")\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(f\"TOTAL ENSEMBLE DISK SIZE: {total_disk_mb:.2f} MB\")\n",
    "print(f\"TOTAL ENSEMBLE RAM SIZE : {total_ram_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d71a670d-627b-422f-81fe-77799d8a331c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Iterative Ensemble Optimization with CV Voting (IEO-CVV)...\n",
      "Evaluating KNN...\n",
      "KNN CV Accuracy: 0.9888\n",
      "Evaluating RF...\n",
      "RF CV Accuracy: 0.9872\n",
      "Evaluating LGB...\n",
      "LGB CV Accuracy: 0.9959\n",
      "Evaluating XGB...\n",
      "XGB CV Accuracy: 0.9955\n",
      "Evaluating NB...\n",
      "NB CV Accuracy: 0.7891\n",
      "Evaluating AB...\n",
      "AB CV Accuracy: 0.9925\n",
      "Initial Ensemble: ['LGB', 'XGB'] - Train Accuracy: 0.9959\n",
      "Added AB - Improved Train Accuracy: 0.9962\n",
      "Added KNN - Improved Train Accuracy: 0.9962\n",
      "Pruned XGB - Accuracy maintained/improved: 0.9963\n",
      "Ignored RF - No improvement: 0.9962 (Patience: 1/3)\n",
      "Ignored NB - No improvement: 0.9963 (Patience: 2/3)\n",
      "Final Ensemble Members: ['LGB', 'AB', 'KNN']\n",
      "Final Ensemble Train Accuracy: 0.9963007085083704\n",
      "\n",
      "Retraining final models and evaluating on test set...\n",
      "Final Ensemble Test Accuracy: 0.995987963891675\n",
      "Training time: 2516.49 seconds\n"
     ]
    }
   ],
   "source": [
    "ensemble_f2, best_acc_f2, ensemble_weights_f2, test_acc_f2 = ieo_cvv_ensemble_selection(models2, X_train2, y_train2, X_test2, y_test2)\n",
    "\n",
    "end_time3 = time.time()\n",
    "training_time3 = end_time3 - start_time3\n",
    "print(f\"Training time: {training_time3:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "879063db-c296-454a-a18f-3b8af3fd3d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.77 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time4 = time.time()\n",
    "\n",
    "y_floor_predictions2 = ensemble_predict(ensemble_f2, X_target, ensemble_weights_f2)\n",
    "\n",
    "end_time4 = time.time()\n",
    "training_time4 = end_time4 - start_time4\n",
    "print(f\"Training time: {training_time4:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4d8c2c02-83e3-42b0-b57f-35a3aadfbc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building using Ensemble Accuracy: 0.9198919891989199\n"
     ]
    }
   ],
   "source": [
    "accuracy_floor_ensemble = accuracy_score(y_target_floor, y_floor_predictions2)\n",
    "print(\"Building using Ensemble Accuracy:\", accuracy_floor_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14899b36-51f8-4b5c-b6c3-08ddd872b73b",
   "metadata": {},
   "source": [
    "# Location Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7309a31c-b5e2-4119-bca2-4846a85f978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances,make_scorer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "db20f0fd-ecb0-4ffa-a16c-4ff4499e9eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data split ---\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y_coordinate, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "def mean_euclidean_distance(y_true, y_pred):\n",
    "    return np.mean(np.linalg.norm(y_pred - y_true, axis=1))\n",
    "\n",
    "euclidean_scorer = make_scorer(mean_euclidean_distance, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e496b429-3580-4057-af15-527ca887e630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "start_time5 = time.time()\n",
    "\n",
    "def objective_rfr(trial):\n",
    "    max_depth = trial.suggest_categorical('max_depth', [None] + list(range(5, 51)))\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
    "\n",
    "    rf = RandomForestRegressor(\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=n_estimators,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(cv.split(X_train3)):\n",
    "        X_train_fold = X_train3.iloc[train_idx]\n",
    "        X_valid_fold = X_train3.iloc[valid_idx]\n",
    "        y_train_fold = y_train3.iloc[train_idx]\n",
    "        y_valid_fold = y_train3.iloc[valid_idx]\n",
    "\n",
    "        rf.fit(X_train_fold, y_train_fold)\n",
    "        preds = rf.predict(X_valid_fold)\n",
    "\n",
    "        # Compute mean Euclidean distance manually\n",
    "        fold_score = np.mean(np.linalg.norm(preds - y_valid_fold.to_numpy(), axis=1))\n",
    "        scores.append(fold_score)\n",
    "\n",
    "        # Report intermediate mean score for pruning\n",
    "        trial.report(np.mean(scores), step=fold_idx)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# --- XGBR ---\n",
    "def objective_xgbr(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(**param)\n",
    "    score = cross_val_score(model, X_train3, y_train3, cv=10,\n",
    "                            scoring=euclidean_scorer, n_jobs=-1).mean()\n",
    "    return score\n",
    "\n",
    "\n",
    "def objective_adaboost_r(trial):\n",
    "    # Suggest hyperparameters\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 10),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    }\n",
    "\n",
    "    # Create base estimator with tree-specific parameters\n",
    "    base_estimator = DecisionTreeRegressor(\n",
    "        max_depth=params['max_depth'],\n",
    "        min_samples_split=params['min_samples_split'],\n",
    "        min_samples_leaf=params['min_samples_leaf'],\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "    # Create AdaBoost Regressor\n",
    "    ada = AdaBoostRegressor(\n",
    "        estimator=base_estimator,\n",
    "        n_estimators=params['n_estimators'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "    # Wrap in MultiOutputRegressor for 2D targets\n",
    "    model = MultiOutputRegressor(ada)\n",
    "\n",
    "    # Cross-validation with euclidean_scorer\n",
    "    scores = cross_val_score(\n",
    "        model,\n",
    "        X_train3,\n",
    "        y_train3,\n",
    "        cv=cv,\n",
    "        scoring=euclidean_scorer,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    return -scores.mean()\n",
    "\n",
    "\n",
    "\n",
    "def objective_knnr(trial):\n",
    "    n_neighbors = trial.suggest_int('n_neighbors', 1, 50)\n",
    "    p = trial.suggest_int('p', 1, 5)\n",
    "    metric = trial.suggest_categorical('metric', ['minkowski', 'euclidean', 'manhattan'])\n",
    "\n",
    "    knn = KNeighborsRegressor(\n",
    "        n_neighbors=n_neighbors,\n",
    "        weights=knn_weight,\n",
    "        p=p,\n",
    "        metric=metric\n",
    "    )\n",
    "\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(cv.split(X_train3)):\n",
    "        # ✅ Use .iloc for row indexing\n",
    "        X_train_fold = X_train3.iloc[train_idx]\n",
    "        X_valid_fold = X_train3.iloc[valid_idx]\n",
    "        y_train_fold = y_train3.iloc[train_idx]\n",
    "        y_valid_fold = y_train3.iloc[valid_idx]\n",
    "\n",
    "        knn.fit(X_train_fold, y_train_fold)\n",
    "        preds = knn.predict(X_valid_fold)\n",
    "\n",
    "        # ✅ Compute mean Euclidean distance manually\n",
    "        fold_score = np.mean(np.linalg.norm(preds - y_valid_fold.to_numpy(), axis=1))\n",
    "        scores.append(fold_score)\n",
    "\n",
    "        # 🧠 Report intermediate result for pruning\n",
    "        trial.report(fold_score, step=fold_idx)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # ✅ Return positive mean Euclidean error (to minimize)\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4a16b5b4-9ba3-4ca7-a9b0-98b05c7e35c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import numpy as np\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "\n",
    "def euclidean_distance(y_true, y_pred):\n",
    "    return np.mean(np.linalg.norm(y_true - y_pred, axis=1))\n",
    "\n",
    "def objective_lgbr(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 30),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    base_model = lgb.LGBMRegressor(**param)\n",
    "    model = MultiOutputRegressor(base_model)\n",
    "\n",
    "    scores = []\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(cv.split(X_train3)):\n",
    "        X_train_fold = X_train3.iloc[train_idx]\n",
    "        X_valid_fold = X_train3.iloc[valid_idx]\n",
    "        y_train_fold = y_train3.iloc[train_idx]\n",
    "        y_valid_fold = y_train3.iloc[valid_idx]\n",
    "\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        preds = model.predict(X_valid_fold)\n",
    "\n",
    "        # ✅ Use the raw distance function directly\n",
    "        score = euclidean_distance(np.array(y_valid_fold), preds)\n",
    "        scores.append(score)\n",
    "\n",
    "        trial.report(np.mean(scores), step=fold_idx)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a6db9e28-5db4-4acf-ab0d-0f4a1cb4e3fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 10:19:08,872] A new study created in memory with name: no-name-9654a197-2686-4ed8-bd72-b378e169b387\n",
      "[I 2025-12-22 10:19:17,358] Trial 0 finished with value: 5.621290376106874 and parameters: {'n_neighbors': 32, 'p': 1, 'metric': 'manhattan'}. Best is trial 0 with value: 5.621290376106874.\n",
      "[I 2025-12-22 10:19:25,767] Trial 1 finished with value: 6.491518990896006 and parameters: {'n_neighbors': 47, 'p': 1, 'metric': 'minkowski'}. Best is trial 0 with value: 5.621290376106874.\n",
      "[I 2025-12-22 10:19:27,753] Trial 2 finished with value: 6.4881429163881945 and parameters: {'n_neighbors': 38, 'p': 3, 'metric': 'euclidean'}. Best is trial 0 with value: 5.621290376106874.\n",
      "[I 2025-12-22 10:20:47,800] Trial 3 finished with value: 7.740233242414523 and parameters: {'n_neighbors': 46, 'p': 4, 'metric': 'minkowski'}. Best is trial 0 with value: 5.621290376106874.\n",
      "[I 2025-12-22 10:20:56,223] Trial 4 finished with value: 5.226058829404782 and parameters: {'n_neighbors': 26, 'p': 3, 'metric': 'manhattan'}. Best is trial 4 with value: 5.226058829404782.\n",
      "[I 2025-12-22 10:21:04,190] Trial 5 pruned. \n",
      "[I 2025-12-22 10:22:23,859] Trial 6 finished with value: 5.248430343745516 and parameters: {'n_neighbors': 8, 'p': 4, 'metric': 'minkowski'}. Best is trial 4 with value: 5.226058829404782.\n",
      "[I 2025-12-22 10:22:31,814] Trial 7 pruned. \n",
      "[I 2025-12-22 10:22:33,964] Trial 8 finished with value: 5.6050666574394405 and parameters: {'n_neighbors': 22, 'p': 3, 'metric': 'euclidean'}. Best is trial 4 with value: 5.226058829404782.\n",
      "[I 2025-12-22 10:22:41,977] Trial 9 pruned. \n",
      "[I 2025-12-22 10:22:50,364] Trial 10 finished with value: 4.691807353113094 and parameters: {'n_neighbors': 19, 'p': 2, 'metric': 'manhattan'}. Best is trial 10 with value: 4.691807353113094.\n",
      "[I 2025-12-22 10:22:58,742] Trial 11 finished with value: 4.603084741794148 and parameters: {'n_neighbors': 18, 'p': 2, 'metric': 'manhattan'}. Best is trial 11 with value: 4.603084741794148.\n",
      "[I 2025-12-22 10:23:07,086] Trial 12 finished with value: 4.418086032994689 and parameters: {'n_neighbors': 16, 'p': 2, 'metric': 'manhattan'}. Best is trial 12 with value: 4.418086032994689.\n",
      "[I 2025-12-22 10:23:15,414] Trial 13 finished with value: 4.036202122585441 and parameters: {'n_neighbors': 12, 'p': 2, 'metric': 'manhattan'}. Best is trial 13 with value: 4.036202122585441.\n",
      "[I 2025-12-22 10:23:23,902] Trial 14 finished with value: 2.7035331097038786 and parameters: {'n_neighbors': 1, 'p': 2, 'metric': 'manhattan'}. Best is trial 14 with value: 2.7035331097038786.\n",
      "[I 2025-12-22 10:23:32,221] Trial 15 finished with value: 2.945622460371971 and parameters: {'n_neighbors': 3, 'p': 2, 'metric': 'manhattan'}. Best is trial 14 with value: 2.7035331097038786.\n",
      "[I 2025-12-22 10:23:40,559] Trial 16 finished with value: 2.8231227500454312 and parameters: {'n_neighbors': 2, 'p': 1, 'metric': 'manhattan'}. Best is trial 14 with value: 2.7035331097038786.\n",
      "[I 2025-12-22 10:23:48,864] Trial 17 finished with value: 2.7035331097038786 and parameters: {'n_neighbors': 1, 'p': 1, 'metric': 'manhattan'}. Best is trial 14 with value: 2.7035331097038786.\n",
      "[I 2025-12-22 10:23:50,751] Trial 18 finished with value: 4.2557971008643225 and parameters: {'n_neighbors': 8, 'p': 1, 'metric': 'euclidean'}. Best is trial 14 with value: 2.7035331097038786.\n",
      "[I 2025-12-22 10:23:59,246] Trial 19 finished with value: 3.44981524266505 and parameters: {'n_neighbors': 7, 'p': 1, 'metric': 'manhattan'}. Best is trial 14 with value: 2.7035331097038786.\n",
      "[I 2025-12-22 10:24:07,530] Trial 20 finished with value: 2.7035331097038786 and parameters: {'n_neighbors': 1, 'p': 2, 'metric': 'manhattan'}. Best is trial 14 with value: 2.7035331097038786.\n",
      "[I 2025-12-22 10:24:15,830] Trial 21 finished with value: 2.7035331097038786 and parameters: {'n_neighbors': 1, 'p': 2, 'metric': 'manhattan'}. Best is trial 14 with value: 2.7035331097038786.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best KNNR hyperparameters: {'n_neighbors': 1, 'p': 2, 'metric': 'manhattan'}\n",
      "Cross-val error (per fold): [3.21959083 2.54966429 2.20399344 3.01237036 2.51166039 2.63144688\n",
      " 2.68485908 2.5103877  2.71337842 2.99797972]\n",
      "Mean Cross-val error: 2.7035331097038786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['knn_coordinate_pred_UJI_reg.pkl']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optuna.pruners import MedianPruner\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import joblib\n",
    "\n",
    "SEED = 42\n",
    "pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=0)\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "\n",
    "# --- KNNR ---\n",
    "study1 = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "study1.optimize(objective_knnr, n_trials=100, timeout=300)\n",
    "\n",
    "best_params1r = study1.best_params\n",
    "print(\"Best KNNR hyperparameters:\", best_params1r)\n",
    "\n",
    "knn_coordinate_final = KNeighborsRegressor(\n",
    "    n_neighbors=best_params1r['n_neighbors'],\n",
    "    weights=knn_weight,\n",
    "    p=best_params1r['p'],\n",
    "    metric=best_params1r['metric']\n",
    ")\n",
    "\n",
    "cv_scores_knn = cross_val_score(knn_coordinate_final, X_train3, y_train3,\n",
    "                                cv=cv, scoring=euclidean_scorer, n_jobs=-1)\n",
    "\n",
    "print(\"Cross-val error (per fold):\", -cv_scores_knn)\n",
    "print(\"Mean Cross-val error:\", -cv_scores_knn.mean())\n",
    "\n",
    "knn_coordinate_final.fit(X_train3, y_train3)\n",
    "joblib.dump(knn_coordinate_final, 'knn_coordinate_pred_UJI_reg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2ea8b133-a4cb-4fb1-be9a-0494ac3b4763",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 10:24:27,326] A new study created in memory with name: no-name-20ad9374-7037-4cea-9ddd-bbb955019569\n",
      "[I 2025-12-22 10:24:31,965] Trial 0 finished with value: 51.92473054295916 and parameters: {'max_depth': 5, 'n_estimators': 199, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 0 with value: 51.92473054295916.\n",
      "[I 2025-12-22 10:24:42,741] Trial 1 finished with value: 11.661956422109231 and parameters: {'max_depth': 23, 'n_estimators': 363, 'min_samples_split': 3, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 1 with value: 11.661956422109231.\n",
      "[I 2025-12-22 10:24:51,833] Trial 2 finished with value: 15.078250785255218 and parameters: {'max_depth': 42, 'n_estimators': 415, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 1 with value: 11.661956422109231.\n",
      "[I 2025-12-22 10:25:47,335] Trial 3 finished with value: 6.492195631108585 and parameters: {'max_depth': 29, 'n_estimators': 239, 'min_samples_split': 9, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 3 with value: 6.492195631108585.\n",
      "[I 2025-12-22 10:25:55,381] Trial 4 finished with value: 11.672520803131077 and parameters: {'max_depth': 23, 'n_estimators': 258, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 3 with value: 6.492195631108585.\n",
      "[I 2025-12-22 10:25:56,397] Trial 5 pruned. \n",
      "[I 2025-12-22 10:25:57,264] Trial 6 pruned. \n",
      "[I 2025-12-22 10:25:57,643] Trial 7 pruned. \n",
      "[I 2025-12-22 10:25:58,416] Trial 8 pruned. \n",
      "[I 2025-12-22 10:26:06,610] Trial 9 finished with value: 11.064066535758384 and parameters: {'max_depth': 42, 'n_estimators': 269, 'min_samples_split': 4, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 3 with value: 6.492195631108585.\n",
      "[I 2025-12-22 10:26:21,222] Trial 10 finished with value: 10.638172771519534 and parameters: {'max_depth': 17, 'n_estimators': 62, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 3 with value: 6.492195631108585.\n",
      "[I 2025-12-22 10:26:34,752] Trial 11 finished with value: 10.651983328129932 and parameters: {'max_depth': 17, 'n_estimators': 50, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 3 with value: 6.492195631108585.\n",
      "[I 2025-12-22 10:27:11,317] Trial 12 finished with value: 6.257792030987668 and parameters: {'max_depth': 29, 'n_estimators': 152, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 12 with value: 6.257792030987668.\n",
      "[I 2025-12-22 10:27:52,460] Trial 13 finished with value: 6.458712207186077 and parameters: {'max_depth': 29, 'n_estimators': 178, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 12 with value: 6.257792030987668.\n",
      "[I 2025-12-22 10:28:30,758] Trial 14 finished with value: 6.426552120128292 and parameters: {'max_depth': 29, 'n_estimators': 163, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 12 with value: 6.257792030987668.\n",
      "[I 2025-12-22 10:28:33,156] Trial 15 pruned. \n",
      "[I 2025-12-22 10:29:07,726] Trial 16 finished with value: 8.085512804128971 and parameters: {'max_depth': 21, 'n_estimators': 151, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 12 with value: 6.257792030987668.\n",
      "[I 2025-12-22 10:29:57,922] Trial 17 finished with value: 6.02800381938027 and parameters: {'max_depth': 33, 'n_estimators': 213, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 17 with value: 6.02800381938027.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RFR hyperparameters: {'max_depth': 33, 'n_estimators': 213, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': None}\n",
      "Cross-val error (per fold): [6.05570855 6.00059472 5.99977584 6.42165627 5.94001102 5.79581183\n",
      " 5.72849071 5.996608   6.20874009 6.13264116]\n",
      "Mean Cross-val error: 6.028003819384088\n"
     ]
    }
   ],
   "source": [
    "# --- RFR ---\n",
    "study2 = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "study2.optimize(objective_rfr, n_trials=100, timeout=300)\n",
    "\n",
    "best_params2r = study2.best_params\n",
    "print(\"Best RFR hyperparameters:\", best_params2r)\n",
    "\n",
    "rf_coordinate_final = RandomForestRegressor(**best_params2r, random_state=SEED, n_jobs=-1)\n",
    "\n",
    "cv_scores_rf = cross_val_score(rf_coordinate_final, X_train3, y_train3,\n",
    "                               cv=cv, scoring=euclidean_scorer, n_jobs=-1)\n",
    "\n",
    "print(\"Cross-val error (per fold):\", -cv_scores_rf)\n",
    "print(\"Mean Cross-val error:\", -cv_scores_rf.mean())\n",
    "\n",
    "rf_coordinate_final.fit(X_train3, y_train3)\n",
    "joblib.dump(rf_coordinate_final, 'rf_coordinate_pred_UJI_reg.pkl')\n",
    "rf_loaded2 = joblib.load('rf_coordinate_pred_UJI_reg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7052210d-3940-4f52-9521-242d2b8b16df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 10:30:56,555] A new study created in memory with name: no-name-281647be-2dc6-4587-9f4d-e867bc5f41bb\n",
      "[I 2025-12-22 10:31:20,073] Trial 0 finished with value: -14.218829436800252 and parameters: {'n_estimators': 140, 'learning_rate': 0.09471329741601134, 'max_depth': 10, 'min_child_weight': 3.3765423665090455, 'gamma': 2.4755748570320484, 'subsample': 0.5950627747823166, 'colsample_bytree': 0.550554276008838}. Best is trial 0 with value: -14.218829436800252.\n",
      "[I 2025-12-22 10:31:28,567] Trial 1 finished with value: -208025.95365659223 and parameters: {'n_estimators': 169, 'learning_rate': 0.016475079512341867, 'max_depth': 7, 'min_child_weight': 2.949239038280356, 'gamma': 3.101827640924191, 'subsample': 0.5076619733136116, 'colsample_bytree': 0.5968889812192223}. Best is trial 0 with value: -14.218829436800252.\n",
      "[I 2025-12-22 10:31:34,149] Trial 2 finished with value: -356900.2932571105 and parameters: {'n_estimators': 88, 'learning_rate': 0.025437499711535055, 'max_depth': 5, 'min_child_weight': 3.4309177764345384, 'gamma': 2.1605991501701736, 'subsample': 0.9788204367962452, 'colsample_bytree': 0.5832817178911377}. Best is trial 0 with value: -14.218829436800252.\n",
      "[I 2025-12-22 10:31:40,283] Trial 3 finished with value: -735904.3939349706 and parameters: {'n_estimators': 108, 'learning_rate': 0.014192562155121815, 'max_depth': 8, 'min_child_weight': 3.13865954406655, 'gamma': 4.944785925735194, 'subsample': 0.877614183464599, 'colsample_bytree': 0.7142541076746123}. Best is trial 0 with value: -14.218829436800252.\n",
      "[I 2025-12-22 10:31:45,124] Trial 4 finished with value: -2003.3892097135417 and parameters: {'n_estimators': 62, 'learning_rate': 0.11324016999541557, 'max_depth': 4, 'min_child_weight': 6.82296236255745, 'gamma': 3.3633938241499335, 'subsample': 0.5979913089688396, 'colsample_bytree': 0.953489801855042}. Best is trial 0 with value: -14.218829436800252.\n",
      "[I 2025-12-22 10:31:54,231] Trial 5 finished with value: -520795.05631502613 and parameters: {'n_estimators': 182, 'learning_rate': 0.010328408884438561, 'max_depth': 4, 'min_child_weight': 1.18800063097, 'gamma': 0.8748196691032284, 'subsample': 0.9053430459110379, 'colsample_bytree': 0.5137981300392653}. Best is trial 0 with value: -14.218829436800252.\n",
      "[I 2025-12-22 10:32:05,688] Trial 6 finished with value: -13.553625447052628 and parameters: {'n_estimators': 64, 'learning_rate': 0.23690582660420195, 'max_depth': 8, 'min_child_weight': 5.54643901787882, 'gamma': 3.429548793075776, 'subsample': 0.6227684008598154, 'colsample_bytree': 0.8707649796507996}. Best is trial 6 with value: -13.553625447052628.\n",
      "[I 2025-12-22 10:32:10,214] Trial 7 finished with value: -631562.4205139924 and parameters: {'n_estimators': 54, 'learning_rate': 0.030932536604162036, 'max_depth': 5, 'min_child_weight': 5.094070685704321, 'gamma': 2.8281121810518055, 'subsample': 0.6722074188201068, 'colsample_bytree': 0.5692611564071426}. Best is trial 6 with value: -13.553625447052628.\n",
      "[I 2025-12-22 10:32:21,239] Trial 8 finished with value: -17.228827097794632 and parameters: {'n_estimators': 158, 'learning_rate': 0.09578168977390716, 'max_depth': 5, 'min_child_weight': 5.696353823880468, 'gamma': 4.033636824874403, 'subsample': 0.6996273985658119, 'colsample_bytree': 0.8856698669036966}. Best is trial 6 with value: -13.553625447052628.\n",
      "[I 2025-12-22 10:32:26,900] Trial 9 finished with value: -1102849.9841735647 and parameters: {'n_estimators': 92, 'learning_rate': 0.012306900799350411, 'max_depth': 8, 'min_child_weight': 6.118054067572627, 'gamma': 2.4380613855780178, 'subsample': 0.5057780051811613, 'colsample_bytree': 0.5091167619610621}. Best is trial 6 with value: -13.553625447052628.\n",
      "[I 2025-12-22 10:33:14,914] Trial 10 finished with value: -9.152742560138128 and parameters: {'n_estimators': 129, 'learning_rate': 0.390159217589389, 'max_depth': 10, 'min_child_weight': 9.995431429213784, 'gamma': 0.920604744020789, 'subsample': 0.8080412501563655, 'colsample_bytree': 0.8141439997185562}. Best is trial 10 with value: -9.152742560138128.\n",
      "[I 2025-12-22 10:34:04,030] Trial 11 finished with value: -9.826747735144487 and parameters: {'n_estimators': 126, 'learning_rate': 0.48324863881097513, 'max_depth': 10, 'min_child_weight': 9.076530775205793, 'gamma': 0.2868299047694647, 'subsample': 0.8235985721406999, 'colsample_bytree': 0.8169011755574256}. Best is trial 10 with value: -9.152742560138128.\n",
      "[I 2025-12-22 10:34:54,267] Trial 12 finished with value: -9.952238833319013 and parameters: {'n_estimators': 131, 'learning_rate': 0.47643681068544963, 'max_depth': 10, 'min_child_weight': 9.187823379534947, 'gamma': 0.030819155300539303, 'subsample': 0.8083571991261517, 'colsample_bytree': 0.7487851540508018}. Best is trial 10 with value: -9.152742560138128.\n",
      "[I 2025-12-22 10:35:37,185] Trial 13 finished with value: -9.481758485784676 and parameters: {'n_estimators': 114, 'learning_rate': 0.4540217258936569, 'max_depth': 10, 'min_child_weight': 9.904890613014292, 'gamma': 1.2827334506773571, 'subsample': 0.8004901386368048, 'colsample_bytree': 0.8250829678185786}. Best is trial 10 with value: -9.152742560138128.\n",
      "[I 2025-12-22 10:36:33,880] Trial 14 finished with value: -8.430191641508767 and parameters: {'n_estimators': 199, 'learning_rate': 0.22828772618461082, 'max_depth': 9, 'min_child_weight': 9.955837600134783, 'gamma': 1.35154134255925, 'subsample': 0.7640404969454379, 'colsample_bytree': 0.6805669224490156}. Best is trial 14 with value: -8.430191641508767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBR hyperparameters: {'n_estimators': 199, 'learning_rate': 0.22828772618461082, 'max_depth': 9, 'min_child_weight': 9.955837600134783, 'gamma': 1.35154134255925, 'subsample': 0.7640404969454379, 'colsample_bytree': 0.6805669224490156}\n",
      "Cross-val error (per fold): [7.91315456 8.62134873 8.40869031 7.91081202 8.5952806  8.04496092\n",
      " 8.3398514  9.18895453 8.64257863 8.63628472]\n",
      "Mean Cross-val error: 8.430191641508767\n"
     ]
    }
   ],
   "source": [
    "# --- XGBR ---\n",
    "study3 = optuna.create_study(direction='maximize')\n",
    "study3.optimize(objective_xgbr, n_trials=100, timeout=300)\n",
    "\n",
    "best_params3r = study3.best_params\n",
    "print(\"Best XGBR hyperparameters:\", best_params3r)\n",
    "\n",
    "xgb_coordinate_final = XGBRegressor(**best_params3r, random_state=42, n_jobs=-1)\n",
    "\n",
    "cv_scores_xgb = cross_val_score(xgb_coordinate_final, X_train3, y_train3,\n",
    "                                cv=10, scoring=euclidean_scorer, n_jobs=-1)\n",
    "\n",
    "print(\"Cross-val error (per fold):\", -cv_scores_xgb)\n",
    "print(\"Mean Cross-val error:\", -cv_scores_xgb.mean())\n",
    "\n",
    "xgb_coordinate_final.fit(X_train3, y_train3)\n",
    "joblib.dump(xgb_coordinate_final, 'xgb_coordinate_pred_UJI_reg.pkl')\n",
    "xgb_loaded2 = joblib.load('xgb_coordinate_pred_UJI_reg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "96c8f59a-8114-418c-a871-89a168cf6494",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 10:37:40,587] A new study created in memory with name: no-name-7ca2570b-050d-444b-a7bb-38db5f70429c\n",
      "[I 2025-12-22 10:38:09,660] Trial 0 finished with value: 11.812339619931821 and parameters: {'n_estimators': 166, 'learning_rate': 0.02110467653627152, 'num_leaves': 33, 'max_depth': 17, 'min_child_samples': 21}. Best is trial 0 with value: 11.812339619931821.\n",
      "[I 2025-12-22 10:38:23,392] Trial 1 finished with value: 55.22499181817176 and parameters: {'n_estimators': 66, 'learning_rate': 0.01577617305892405, 'num_leaves': 65, 'max_depth': 8, 'min_child_samples': 92}. Best is trial 0 with value: 11.812339619931821.\n",
      "[I 2025-12-22 10:38:44,493] Trial 2 finished with value: 10.765814838481795 and parameters: {'n_estimators': 107, 'learning_rate': 0.05130227272304724, 'num_leaves': 82, 'max_depth': 11, 'min_child_samples': 94}. Best is trial 2 with value: 10.765814838481795.\n",
      "[I 2025-12-22 10:39:16,005] Trial 3 finished with value: 18.805764506833093 and parameters: {'n_estimators': 120, 'learning_rate': 0.018019017544321735, 'num_leaves': 51, 'max_depth': 25, 'min_child_samples': 9}. Best is trial 2 with value: 10.765814838481795.\n",
      "[I 2025-12-22 10:39:23,572] Trial 4 finished with value: 73.71376907354262 and parameters: {'n_estimators': 95, 'learning_rate': 0.011329890884850796, 'num_leaves': 36, 'max_depth': 3, 'min_child_samples': 28}. Best is trial 2 with value: 10.765814838481795.\n",
      "[I 2025-12-22 10:39:35,279] Trial 5 finished with value: 10.799436674260324 and parameters: {'n_estimators': 85, 'learning_rate': 0.12866401624739204, 'num_leaves': 57, 'max_depth': 7, 'min_child_samples': 56}. Best is trial 2 with value: 10.765814838481795.\n",
      "[I 2025-12-22 10:39:57,653] Trial 6 finished with value: 7.07867708182291 and parameters: {'n_estimators': 79, 'learning_rate': 0.13457998320604114, 'num_leaves': 60, 'max_depth': 25, 'min_child_samples': 11}. Best is trial 6 with value: 7.07867708182291.\n",
      "[I 2025-12-22 10:39:58,949] Trial 7 pruned. \n",
      "[I 2025-12-22 10:40:00,958] Trial 8 pruned. \n",
      "[I 2025-12-22 10:40:02,219] Trial 9 pruned. \n",
      "[I 2025-12-22 10:40:55,664] Trial 10 finished with value: 6.439544797526463 and parameters: {'n_estimators': 152, 'learning_rate': 0.3782808138867044, 'num_leaves': 100, 'max_depth': 30, 'min_child_samples': 5}. Best is trial 10 with value: 6.439544797526463.\n",
      "[I 2025-12-22 10:41:48,435] Trial 11 finished with value: 6.909352837402286 and parameters: {'n_estimators': 154, 'learning_rate': 0.473076737850898, 'num_leaves': 97, 'max_depth': 29, 'min_child_samples': 5}. Best is trial 10 with value: 6.439544797526463.\n",
      "[I 2025-12-22 10:42:42,403] Trial 12 finished with value: 6.749683867625729 and parameters: {'n_estimators': 157, 'learning_rate': 0.44909075221738354, 'num_leaves': 99, 'max_depth': 30, 'min_child_samples': 6}. Best is trial 10 with value: 6.439544797526463.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LGBR hyperparameters: {'n_estimators': 152, 'learning_rate': 0.3782808138867044, 'num_leaves': 100, 'max_depth': 30, 'min_child_samples': 5}\n",
      "Cross-val error (per fold): [6.33372377 6.55609725 6.48956162 6.60598323 6.36125154 6.0239405\n",
      " 6.35336224 6.42113322 6.62691499 6.6234796 ]\n",
      "Mean Cross-val error: 6.439544797526463\n"
     ]
    }
   ],
   "source": [
    "# --- LGBR ---\n",
    "study5 = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "study5.optimize(objective_lgbr, n_trials=100, timeout=300)\n",
    "\n",
    "best_params5r = study5.best_params\n",
    "print(\"Best LGBR hyperparameters:\", best_params5r)\n",
    "\n",
    "# Train final model using best params\n",
    "base_model_final = lgb.LGBMRegressor(**best_params5r, random_state=SEED, n_jobs=-1)\n",
    "lgb_coordinate_final = MultiOutputRegressor(base_model_final)\n",
    "\n",
    "# Cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=SEED)  # define cv here if not already\n",
    "cv_scores_lgbm = cross_val_score(\n",
    "    lgb_coordinate_final, X_train3, y_train3,\n",
    "    cv=cv, scoring=euclidean_scorer, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Positive error conversion\n",
    "cv_errors_lgbm = -cv_scores_lgbm\n",
    "\n",
    "# Print results\n",
    "print(\"Cross-val error (per fold):\", cv_errors_lgbm)\n",
    "print(\"Mean Cross-val error:\", cv_errors_lgbm.mean())\n",
    "\n",
    "# Train on full training set\n",
    "lgb_coordinate_final.fit(X_train3, y_train3)\n",
    "\n",
    "# Save and reload\n",
    "joblib.dump(lgb_coordinate_final, 'multioutput_lgbm_regressor.pkl')\n",
    "multi_model_final = joblib.load('multioutput_lgbm_regressor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "721f1f8b-4b47-4f24-aad3-339a57209ec7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 10:43:21,386] A new study created in memory with name: no-name-00f8c09d-f9e4-4df1-bcbe-25db5be6ae37\n",
      "[I 2025-12-22 10:44:05,424] Trial 0 finished with value: 68.59085473899675 and parameters: {'n_estimators': 300, 'learning_rate': 0.7889272465813666, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 7}. Best is trial 0 with value: 68.59085473899675.\n",
      "[I 2025-12-22 10:45:09,116] Trial 1 finished with value: 42.25013399218279 and parameters: {'n_estimators': 154, 'learning_rate': 0.8255052444122164, 'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 1 with value: 42.25013399218279.\n",
      "[I 2025-12-22 10:46:03,915] Trial 2 finished with value: 68.49787730649525 and parameters: {'n_estimators': 82, 'learning_rate': 0.5783767594149442, 'max_depth': 3, 'min_samples_split': 8, 'min_samples_leaf': 8}. Best is trial 1 with value: 42.25013399218279.\n",
      "[I 2025-12-22 10:46:47,179] Trial 3 finished with value: 26.563399386304162 and parameters: {'n_estimators': 63, 'learning_rate': 0.9173869932408834, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 5}. Best is trial 3 with value: 26.563399386304162.\n",
      "[I 2025-12-22 10:47:25,428] Trial 4 finished with value: 68.25401861849238 and parameters: {'n_estimators': 275, 'learning_rate': 0.9086123964161665, 'max_depth': 3, 'min_samples_split': 6, 'min_samples_leaf': 6}. Best is trial 3 with value: 26.563399386304162.\n",
      "[I 2025-12-22 10:48:25,356] Trial 5 finished with value: 32.9860529295879 and parameters: {'n_estimators': 292, 'learning_rate': 0.8434190955142437, 'max_depth': 6, 'min_samples_split': 6, 'min_samples_leaf': 8}. Best is trial 3 with value: 26.563399386304162.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AdaBoost Regressor hyperparameters: {'n_estimators': 63, 'learning_rate': 0.9173869932408834, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 5}\n",
      "AdaBoost Cross-val error (per fold): [27.53819935 25.11810067 27.35843198 26.91715345 25.69773763 24.74156483\n",
      " 25.83654384 31.61982931 25.54809545 25.25833735]\n",
      "AdaBoost Mean Cross-val error: 26.563399386304162\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import joblib\n",
    "\n",
    "# --- AdaBoost Regressor Optimization ---\n",
    "study_ab_r = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "study_ab_r.optimize(objective_adaboost_r, n_trials=50, timeout=300)\n",
    "\n",
    "best_params_ab_r = study_ab_r.best_params\n",
    "print(\"Best AdaBoost Regressor hyperparameters:\", best_params_ab_r)\n",
    "\n",
    "# --- Extract AdaBoost-specific parameters ---\n",
    "ada_params = {\n",
    "    'n_estimators': best_params_ab_r.pop('n_estimators'),\n",
    "    'learning_rate': best_params_ab_r.pop('learning_rate'),\n",
    "    'random_state': SEED\n",
    "}\n",
    "\n",
    "# Base estimator for AdaBoost\n",
    "base_estimator = DecisionTreeRegressor(**best_params_ab_r, random_state=SEED)\n",
    "\n",
    "# Final AdaBoost Regressor wrapped with MultiOutputRegressor for 2D output\n",
    "ab_coordinate_final = MultiOutputRegressor(\n",
    "    AdaBoostRegressor(\n",
    "        estimator=base_estimator,  # Note: use `estimator` (for scikit-learn >= 1.2)\n",
    "        **ada_params\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- Cross-validation (using your custom euclidean_scorer) ---\n",
    "cv_scores_ab = cross_val_score(\n",
    "    ab_coordinate_final,\n",
    "    X_train3,\n",
    "    y_train3,\n",
    "    cv=cv,\n",
    "    scoring=euclidean_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"AdaBoost Cross-val error (per fold):\", -cv_scores_ab)\n",
    "print(\"AdaBoost Mean Cross-val error:\", -cv_scores_ab.mean())\n",
    "\n",
    "# --- Train on full training set and save model ---\n",
    "ab_coordinate_final.fit(X_train3, y_train3)\n",
    "joblib.dump(ab_coordinate_final, 'adaboost_coordinate_pred_UJI_reg.pkl')\n",
    "\n",
    "# Optional: Load the model back\n",
    "ab_loaded = joblib.load('adaboost_coordinate_pred_UJI_reg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cd1cf620-f3d2-434b-b7fd-723cbda20f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "models3 = [\n",
    "        (\"KNN\", knn_coordinate_final),\n",
    "        (\"RF\", rf_coordinate_final),\n",
    "        (\"LGB\", lgb_coordinate_final),\n",
    "        (\"XGB\", xgb_coordinate_final),\n",
    "        (\"AB\", ab_coordinate_final)\n",
    "    ]\n",
    "\n",
    "accuracy_knn = cv_scores_knn.mean()\n",
    "accuracy_rf = cv_scores_rf.mean()\n",
    "accuracy_lgb = cv_scores_lgbm.mean()\n",
    "accuracy_xgb = cv_scores_xgb.mean()\n",
    "accuracy_ab = cv_scores_ab.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "44924bdd-87cd-496f-b83b-25237ec357b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ENSEMBLE MEMORY FOOTPRINT =====\n",
      "KNN: Disk = 56.83 MB | RAM = 56.83 MB\n",
      "RF: Disk = 67.47 MB | RAM = 67.46 MB\n",
      "LGB: Disk = 2.84 MB | RAM = 2.84 MB\n",
      "XGB: Disk = 1.93 MB | RAM = 1.93 MB\n",
      "AB: Disk = 0.48 MB | RAM = 0.46 MB\n",
      "------------------------------------\n",
      "TOTAL ENSEMBLE DISK SIZE: 129.55 MB\n",
      "TOTAL ENSEMBLE RAM SIZE : 129.52 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pickle\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "# ===== Utility functions =====\n",
    "def model_disk_size_mb(model):\n",
    "    \"\"\"Serialized model size on disk (deployment footprint)\"\"\"\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".joblib\") as tmp:\n",
    "        joblib.dump(model, tmp.name)\n",
    "        size_mb = os.path.getsize(tmp.name) / (1024 ** 2)\n",
    "    os.remove(tmp.name)\n",
    "    return size_mb\n",
    "\n",
    "\n",
    "def model_ram_size_mb(model):\n",
    "    \"\"\"In-memory size (RAM footprint)\"\"\"\n",
    "    return sys.getsizeof(pickle.dumps(model)) / (1024 ** 2)\n",
    "\n",
    "\n",
    "# ===== Compute footprint =====\n",
    "disk_sizes = {}\n",
    "ram_sizes = {}\n",
    "\n",
    "for name, model in models3:\n",
    "    disk_sizes[name] = model_disk_size_mb(model)\n",
    "    ram_sizes[name] = model_ram_size_mb(model)\n",
    "\n",
    "# ===== Total ensemble footprint =====\n",
    "total_disk_mb = sum(disk_sizes.values())\n",
    "total_ram_mb = sum(ram_sizes.values())\n",
    "\n",
    "print(\"===== ENSEMBLE MEMORY FOOTPRINT =====\")\n",
    "for name in disk_sizes:\n",
    "    print(f\"{name}: Disk = {disk_sizes[name]:.2f} MB | RAM = {ram_sizes[name]:.2f} MB\")\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(f\"TOTAL ENSEMBLE DISK SIZE: {total_disk_mb:.2f} MB\")\n",
    "print(f\"TOTAL ENSEMBLE RAM SIZE : {total_ram_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c986b1ae-f202-43b5-b75e-80339dbaae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.base import clone\n",
    "import numpy as np\n",
    "\n",
    "def ieo_cvv_ensemble_selection_reg(models, X_train, y_train, X_test, y_test, cv=10, patience=3):\n",
    "    print(\"Initializing Iterative Ensemble Optimization with CV Voting (IEO-CVV) using Mean Euclidean Error...\")\n",
    "\n",
    "    def mean_euclidean_error(y_true, y_pred):\n",
    "        return np.mean(np.linalg.norm(y_true - y_pred, axis=1))\n",
    "\n",
    "    # --- Step 1: Cross-validated predictions and MEE scores ---\n",
    "    model_scores = {}\n",
    "    model_preds = {}\n",
    "\n",
    "    for name, model in models:\n",
    "        print(f\"Evaluating {name}...\")\n",
    "        y_pred_cv = cross_val_predict(model, X_train, y_train, cv=cv, n_jobs=-1)\n",
    "        mee = mean_euclidean_error(y_train, y_pred_cv)\n",
    "        model_scores[name] = -mee\n",
    "        model_preds[name] = y_pred_cv\n",
    "        print(f\"{name} CV Mean Euclidean Error: {mee:.4f}\")\n",
    "\n",
    "    # Sort models by performance\n",
    "    sorted_models = sorted(models, key=lambda m: model_scores[m[0]], reverse=True)\n",
    "\n",
    "    # --- Weighted average ensemble prediction (CV stage) ---\n",
    "    def ensemble_predict_cv(selected_model_names):\n",
    "        preds = np.array([model_preds[name] for name in selected_model_names])\n",
    "        weights = np.array([model_scores[name] for name in selected_model_names])\n",
    "        weights = weights / weights.sum()\n",
    "        return np.tensordot(weights, preds, axes=(0, 0))\n",
    "\n",
    "    # --- Step 2: Initialize with top 2 models ---\n",
    "    ensemble = sorted_models[:2]\n",
    "    ensemble_names = [name for name, _ in ensemble]\n",
    "    y_ensemble_cv = ensemble_predict_cv(ensemble_names)\n",
    "    best_score = -mean_euclidean_error(y_train, y_ensemble_cv)\n",
    "    print(f\"Initial Ensemble: {ensemble_names} - Train MEE: {-best_score:.4f}\")\n",
    "\n",
    "    # --- Step 3: Iterative forward selection with backward pruning ---\n",
    "    patience_counter = 0\n",
    "    remaining_models = sorted_models[2:]\n",
    "\n",
    "    while remaining_models and patience_counter < patience:\n",
    "        improved = False\n",
    "\n",
    "        name, model = remaining_models.pop(0)\n",
    "        temp_ensemble = ensemble + [(name, model)]\n",
    "        temp_names = [n for n, _ in temp_ensemble]\n",
    "\n",
    "        y_pred_temp = ensemble_predict_cv(temp_names)\n",
    "        temp_score = -mean_euclidean_error(y_train, y_pred_temp)\n",
    "\n",
    "        if temp_score > best_score:\n",
    "            ensemble = temp_ensemble\n",
    "            best_score = temp_score\n",
    "            patience_counter = 0\n",
    "            print(f\"Added {name} - Improved MEE: {-temp_score:.4f}\")\n",
    "\n",
    "            # --- Backward pruning ---\n",
    "            pruned = True\n",
    "            while pruned and len(ensemble) > 1:\n",
    "                pruned = False\n",
    "                for n, _ in ensemble:\n",
    "                    if n == name:\n",
    "                        continue\n",
    "                    temp_pruned = [(x, y) for x, y in ensemble if x != n]\n",
    "                    temp_names_pruned = [x for x, _ in temp_pruned]\n",
    "                    y_pred_pruned = ensemble_predict_cv(temp_names_pruned)\n",
    "                    pruned_score = -mean_euclidean_error(y_train, y_pred_pruned)\n",
    "                    if pruned_score >= best_score:\n",
    "                        print(f\"Pruned {n} - MEE improved/maintained: {-pruned_score:.4f}\")\n",
    "                        ensemble = temp_pruned\n",
    "                        best_score = pruned_score\n",
    "                        pruned = True\n",
    "                        break\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Ignored {name} - No improvement: {-temp_score:.4f} (Patience: {patience_counter}/{patience})\")\n",
    "\n",
    "    # --- Step 4: Final weights based on CV performance ---\n",
    "    final_names = [n for n, _ in ensemble]\n",
    "    weights = {name: model_scores[name] for name in final_names}\n",
    "    total = sum(weights.values())\n",
    "    weights = {k: v / total for k, v in weights.items()}\n",
    "\n",
    "    print(\"Final Ensemble Members:\", final_names)\n",
    "    print(\"Final Ensemble Train MEE:\", -best_score)\n",
    "\n",
    "    # --- Step 5: Retrain models and evaluate on test set ---\n",
    "    print(\"\\nRetraining final models and evaluating on test set...\")\n",
    "    for name, model in ensemble:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    def ensemble_predict_test(selected_models, X):\n",
    "        preds = np.array([model.predict(X) for _, model in selected_models])\n",
    "        wts = np.array([model_scores[name] for name, _ in selected_models])\n",
    "        wts = wts / wts.sum()\n",
    "        return np.tensordot(wts, preds, axes=(0, 0))\n",
    "\n",
    "    y_pred_test = ensemble_predict_test(ensemble, X_test)\n",
    "    test_mee = mean_euclidean_error(y_test, y_pred_test)\n",
    "    print(\"Final Ensemble Test MEE:\", test_mee)\n",
    "\n",
    "    return ensemble, -best_score, weights, test_mee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e3a26414-28b9-4d58-b7cd-f41eae5ca15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(ensemble, X, model_scores):\n",
    "    \"\"\"\n",
    "    Weighted ensemble prediction for regression.\n",
    "    Each model's predictions are weighted by its CV-based score.\n",
    "    \"\"\"\n",
    "    # Collect predictions from all models\n",
    "    preds = np.array([m.predict(X) for _, m in ensemble])\n",
    "\n",
    "    # Compute normalized weights based on model scores\n",
    "    weights = np.array([model_scores[name] for name, _ in ensemble])\n",
    "    weights = weights / weights.sum()\n",
    "\n",
    "    # Weighted average across models\n",
    "    weighted_preds = np.tensordot(weights, preds, axes=(0, 0))\n",
    "\n",
    "    return weighted_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "01bc27f6-f4ae-4cc5-a5ec-3d2ae6c17b08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Iterative Ensemble Optimization with CV Voting (IEO-CVV) using Mean Euclidean Error...\n",
      "Evaluating KNN...\n",
      "KNN CV Mean Euclidean Error: 4.7699\n",
      "Evaluating RF...\n",
      "RF CV Mean Euclidean Error: 8.8831\n",
      "Evaluating LGB...\n",
      "LGB CV Mean Euclidean Error: 6.6553\n",
      "Evaluating XGB...\n",
      "XGB CV Mean Euclidean Error: 8.9097\n",
      "Evaluating AB...\n",
      "AB CV Mean Euclidean Error: 24.6724\n",
      "Initial Ensemble: ['KNN', 'LGB'] - Train MEE: 5.3500\n",
      "Ignored RF - No improvement: 6.3312 (Patience: 1/3)\n",
      "Ignored XGB - No improvement: 6.0702 (Patience: 2/3)\n",
      "Ignored AB - No improvement: 17.7234 (Patience: 3/3)\n",
      "Final Ensemble Members: ['KNN', 'LGB']\n",
      "Final Ensemble Train MEE: 5.349992486038268\n",
      "\n",
      "Retraining final models and evaluating on test set...\n",
      "Final Ensemble Test MEE: 5.339831806774781\n",
      "Training time: 2373.42 seconds\n"
     ]
    }
   ],
   "source": [
    "# Run greedy ensemble selection for regression (using Mean Euclidean Distance)\n",
    "ensemble_r2, train_score_r2, ensemble_weights_r2, test_score_r2 = ieo_cvv_ensemble_selection_reg(\n",
    "    models3,\n",
    "    X_train3, y_train3,   # training data\n",
    "    X_test3, y_test3,     # test data\n",
    "    cv=10,                # cross-validation folds\n",
    ")\n",
    "\n",
    "end_time5 = time.time()\n",
    "training_time5 = end_time5 - start_time5\n",
    "print(f\"Training time: {training_time5:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "83bb984b-802d-4677-8918-4674d7760715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.65 seconds\n"
     ]
    }
   ],
   "source": [
    "# Predict target coordinates using the final ensemble\n",
    "start_time6 = time.time()\n",
    "\n",
    "y_target_predictions2 = ensemble_predict(ensemble_r2, X_target, ensemble_weights_r2)\n",
    "\n",
    "end_time6 = time.time()\n",
    "training_time6 = end_time6 - start_time6\n",
    "print(f\"Training time: {training_time6:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d6cf757d-bd9f-4871-8f58-0acd6a37ba2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Mean Euclidean Distance (Target set): 11.822313732055287\n"
     ]
    }
   ],
   "source": [
    "# Evaluate ensemble predictions on target set using Euclidean distance\n",
    "euclidean_errors = np.linalg.norm(y_target_coordinate - y_target_predictions2, axis=1)\n",
    "med_ensemble = euclidean_errors.mean()\n",
    "\n",
    "print(\"Ensemble Mean Euclidean Distance (Target set):\", med_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1001f032-a409-4439-a4c4-72551ab9162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coordinate = pd.DataFrame(y_target_predictions2, columns=['x', 'y'])\n",
    "df_floor = pd.DataFrame(y_floor_predictions2, columns=['Floor'])\n",
    "df_building = pd.DataFrame(y_building_predictions2, columns=['Building'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6acc0505-70eb-4ce7-9237-21653a7c5403",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined1 = pd.concat([df_coordinate, df_floor], axis=1)\n",
    "df_combined = pd.concat([df_combined1, df_building], axis=1)\n",
    "df_combined.to_excel('UJIIndoorLoc Prediction.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "713e717c-b0d2-488d-a31b-9eb53d5ab17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_NEW = DT[['LONGITUDE', 'LATITUDE', 'FLOOR_ID','BUILDINGID']]\n",
    "DT_NEW.to_excel('New_DT_coordinates_UJIIndoorLoc.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491159c3-495a-440b-a223-3cbfa21dec79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
