{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6da4d6c-12c6-4b01-8b1d-9d6762a15d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0f6dbee-b67b-4cec-ade6-226b34638962",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_features = 589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "798d2e9b-f804-4c10-a8d1-c9ee39345994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 1: Load data ===\n",
    "df = pd.read_excel('3. Dataset_UTS.xlsx')\n",
    "# Load test data\n",
    "DT = pd.read_excel('3. Datatest_UTS.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56be2b7c-097b-4f32-811e-eedee0dbcbd6",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b6b989b-a97b-411c-9b56-774b650b581e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_20072\\3564184693.py:104: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[rssi_columns] = df[rssi_columns].applymap(lambda x: x - shift if x < 0 else 0)\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_20072\\3564184693.py:108: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[rssi_columns] = df[rssi_columns].div(row_max, axis=0).fillna(0)\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_20072\\3564184693.py:113: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[rssi_columns] = df[rssi_columns].applymap(\n"
     ]
    }
   ],
   "source": [
    "# Simplest\n",
    "'''# === Step 1: Select RSSI columns ===\n",
    "rssi_columns = df.columns[:raw_features]  # assuming RSSI is in the first `raw_features` columns\n",
    "\n",
    "# === Step 2: Replace 100 (no signal) with 0 ===\n",
    "df[rssi_columns] = df[rssi_columns].replace(100, -110)\n",
    "df'''\n",
    "\n",
    "# Linear Normalization to [0,1] Using Dataset Minimum\n",
    "\n",
    "'''rssi_columns = df.columns[:raw_features]  # assuming RSSI is in the first `raw_features` columns\n",
    "\n",
    "# === Step 2: Replace 100 (no signal) with 0 ===\n",
    "df[rssi_columns] = df[rssi_columns].replace(100, 0)\n",
    "\n",
    "# === Step 3: Compute minimum RSSI value (exclude 0s from consideration)\n",
    "# Only consider values < 0 (i.e., actual signal readings), ignore 0 (no signal)\n",
    "rssi_min = df[rssi_columns][df[rssi_columns] < 0].min().min()\n",
    "\n",
    "# === Step 4: Apply linear normalization\n",
    "# Keep 0s as 0 (representing no signal), and normalize only values < 0\n",
    "df[rssi_columns] = df[rssi_columns].applymap(\n",
    "    lambda x: 0 if x >= 0 else (x - rssi_min) / -rssi_min\n",
    ")\n",
    "df'''\n",
    "\n",
    "# Powered Normalization Using Dataset Minimum\n",
    "\n",
    "'''# === Step 1: Select RSSI columns ===\n",
    "rssi_columns = df.columns[:raw_features]  # assuming RSSI is in the first raw_features columns\n",
    "\n",
    "# === Step 2: Replace 100 (no signal) with 0 ===\n",
    "df[rssi_columns] = df[rssi_columns].replace(100, 0)\n",
    "\n",
    "# === Step 3: Compute dataset-wide minimum RSSI (only for real signal values, excluding 0) ===\n",
    "rssi_min = df[rssi_columns][df[rssi_columns] < 0].min().min()\n",
    "\n",
    "# === Step 4: Apply powered normalization (exclude 0 from normalization) ===\n",
    "gamma = 2  # you can change this exponent as needed\n",
    "\n",
    "df[rssi_columns] = df[rssi_columns].applymap(\n",
    "    lambda x: ((x - rssi_min) / -rssi_min) ** gamma if x < 0 else 0\n",
    ")\n",
    "\n",
    "df'''\n",
    "\n",
    "# Min-Max Normalization Using Global Min/Max\n",
    "\n",
    "'''# === Step 1: Select RSSI columns ===\n",
    "rssi_columns = df.columns[:raw_features]  # assuming RSSI is in the first `raw_features` columns\n",
    "\n",
    "# === Step 2: Replace 100 (no signal) with 0 ===\n",
    "df[rssi_columns] = df[rssi_columns].replace(100, 0)\n",
    "\n",
    "# === Step 3: Compute dataset-wide min and max RSSI values (excluding 0s) ===\n",
    "rssi_min = df[rssi_columns][df[rssi_columns] < 0].min().min()\n",
    "rssi_max = df[rssi_columns][df[rssi_columns] < 0].max().max()\n",
    "\n",
    "# === Step 4: Apply Min-Max Normalization to [0, 1] (exclude 0 from normalization) ===\n",
    "df[rssi_columns] = df[rssi_columns].applymap(\n",
    "    lambda x: (x - rssi_min) / (rssi_max - rssi_min) if x < 0 else 0\n",
    ")\n",
    "\n",
    "df'''\n",
    "\n",
    "# Powered Transformation After Per-Fingerprint Normalization\n",
    "\n",
    "'''# === Step 1: Select RSSI columns ===\n",
    "rssi_columns = df.columns[:raw_features]  # assuming RSSI is in the first `raw_features` columns\n",
    "\n",
    "# === Step 2: Replace 100 (no signal) with 0 ===\n",
    "df[rssi_columns] = df[rssi_columns].replace(100, 0)\n",
    "\n",
    "# === Step 3: Convert RSSI to positive by shifting (only for non-zero values)\n",
    "rssi_min = df[rssi_columns][df[rssi_columns] < 0].min().min()\n",
    "shift = rssi_min - 1\n",
    "\n",
    "# Shift only values less than 0, leave 0s unchanged\n",
    "df[rssi_columns] = df[rssi_columns].applymap(lambda x: x - shift if x < 0 else 0)\n",
    "\n",
    "# === Step 4: Per-row (fingerprint) normalization (excluding 0s) ===\n",
    "# Normalize each row based on the max of non-zero entries\n",
    "row_max = df[rssi_columns].replace(0, pd.NA).max(axis=1)\n",
    "\n",
    "df[rssi_columns] = df[rssi_columns].div(row_max, axis=0).fillna(0)\n",
    "\n",
    "# === Step 5: Apply powered transformation (e.g., square) only to non-zero values\n",
    "gamma = 2  # You can set this to any exponent like e, 1.5, etc.\n",
    "df[rssi_columns] = df[rssi_columns].applymap(lambda x: x**gamma if x > 0 else 0)\n",
    "\n",
    "df'''\n",
    "\n",
    "# Sigmoid-Scaled Row Normalization\n",
    "\n",
    "# === Step 1: Select RSSI columns ===\n",
    "rssi_columns = df.columns[:raw_features]  # assuming RSSI is in the first raw_features columns\n",
    "\n",
    "# === Step 2: Replace 100 (no signal) with 0 ===\n",
    "df[rssi_columns] = df[rssi_columns].replace(100, 0)\n",
    "\n",
    "# === Step 3: Shift RSSI to positive (only non-zero) ===\n",
    "rssi_min = df[rssi_columns][df[rssi_columns] < 0].min().min()\n",
    "shift = rssi_min - 1  # shift negative RSSI to positive\n",
    "df[rssi_columns] = df[rssi_columns].applymap(lambda x: x - shift if x < 0 else 0)\n",
    "\n",
    "# === Step 4: Row-wise normalization (exclude zeros) ===\n",
    "row_max = df[rssi_columns].replace(0, pd.NA).max(axis=1)\n",
    "df[rssi_columns] = df[rssi_columns].div(row_max, axis=0).fillna(0)\n",
    "\n",
    "# === Step 5: Apply sigmoid transformation to non-zero values ===\n",
    "alpha = 10  # steepness of sigmoid\n",
    "beta = 0.5  # midpoint\n",
    "df[rssi_columns] = df[rssi_columns].applymap(\n",
    "    lambda x: 1 / (1 + np.exp(-alpha * (x - beta))) if x > 0 else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03b2eac0-1dae-4c07-9e6c-f5f6afc619fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_20072\\1906004430.py:92: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  DT[rssi_columns] = DT[rssi_columns].applymap(lambda x: x - shift if x < 0 else 0)\n",
      "C:\\Users\\KMITL\\AppData\\Local\\Temp\\ipykernel_20072\\1906004430.py:101: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  DT[rssi_columns] = DT[rssi_columns].applymap(\n"
     ]
    }
   ],
   "source": [
    "'''# === Step 1: Select RSSI columns ===\n",
    "rssi_columns = DT.columns[:raw_features]  # assuming RSSI is in the first `raw_features` columns\n",
    "\n",
    "# === Step 2: Replace 100 (no signal) with 0 ===\n",
    "DT[rssi_columns] = DT[rssi_columns].replace(100, -110)\n",
    "DT'''\n",
    "\n",
    "# Linear Normalization to [0,1] Using Dataset Minimum\n",
    "\n",
    "'''# === Step 1: Replace 100 (no signal) with 0 ===\n",
    "DT[rssi_columns] = DT[rssi_columns].replace(100, 0)\n",
    "\n",
    "# === Step 2: Reuse the dataset-wide minimum RSSI value from training ===\n",
    "# Ensure `rssi_min` is defined from the training set beforehand\n",
    "# rssi_min = df[rssi_columns][df[rssi_columns] < 0].min().min()\n",
    "\n",
    "# === Step 3: Apply the linear normalization formula (excluding 0s) ===\n",
    "DT[rssi_columns] = DT[rssi_columns].applymap(\n",
    "    lambda x: 0 if x >= 0 else (x - rssi_min) / -rssi_min\n",
    ")\n",
    "DT'''\n",
    "\n",
    "# Powered Normalization Using Dataset Minimum\n",
    "\n",
    "'''# === Step 1: Replace 100 (no signal) with 0 ===\n",
    "DT[rssi_columns] = DT[rssi_columns].replace(100, 0)\n",
    "\n",
    "# === Step 2: Reuse the dataset-wide minimum RSSI value from training ===\n",
    "# Make sure `rssi_min` is defined and matches the one used during training:\n",
    "# Example:\n",
    "# rssi_min = -104\n",
    "\n",
    "# === Step 3: Apply powered normalization (exclude 0 from normalization) ===\n",
    "gamma = 2  # Power factor (can also use math.e or another value)\n",
    "\n",
    "DT[rssi_columns] = DT[rssi_columns].applymap(\n",
    "    lambda x: ((x - rssi_min) / -rssi_min) ** gamma if x < 0 else 0\n",
    ")\n",
    "\n",
    "DT'''\n",
    "\n",
    "# Min-Max Normalization Using Global Min/Max\n",
    "\n",
    "'''# === Step 1: Replace 100 (no signal) with 0 ===\n",
    "DT[rssi_columns] = DT[rssi_columns].replace(100, 0)\n",
    "\n",
    "# === Step 2: Reuse dataset-wide min and max RSSI values from training ===\n",
    "# Ensure rssi_min and rssi_max were saved during training:\n",
    "# Example:\n",
    "# rssi_min = -104\n",
    "# rssi_max = -30\n",
    "\n",
    "# === Step 3: Apply Min-Max Normalization to [0, 1] (exclude 0 from normalization) ===\n",
    "DT[rssi_columns] = DT[rssi_columns].applymap(\n",
    "    lambda x: (x - rssi_min) / (rssi_max - rssi_min) if x < 0 else 0\n",
    ")\n",
    "\n",
    "DT'''\n",
    "\n",
    "# Powered Transformation After Per-Fingerprint Normalization\n",
    "\n",
    "'''# === Step 1: Replace 100 (no signal) with 0 ===\n",
    "DT[rssi_columns] = DT[rssi_columns].replace(100, 0)\n",
    "\n",
    "# === Step 2: Reuse dataset-wide minimum RSSI value from training to shift into positive range\n",
    "# Make sure `rssi_min` is defined (e.g., rssi_min = -104)\n",
    "shift = rssi_min - 1\n",
    "\n",
    "# Shift only real RSSI values (< 0), leave 0s untouched\n",
    "DT[rssi_columns] = DT[rssi_columns].applymap(lambda x: x - shift if x < 0 else 0)\n",
    "\n",
    "# === Step 3: Normalize each fingerprint row-wise to [0, 1], excluding 0s ===\n",
    "row_max = DT[rssi_columns].replace(0, pd.NA).max(axis=1)\n",
    "DT[rssi_columns] = DT[rssi_columns].div(row_max, axis=0).fillna(0)\n",
    "\n",
    "# === Step 4: Apply powered transformation only to non-zero values ===\n",
    "#gamma = 2  # You can adjust this exponent as needed\n",
    "DT[rssi_columns] = DT[rssi_columns].applymap(lambda x: x**gamma if x > 0 else 0)\n",
    "\n",
    "DT'''\n",
    "\n",
    "\n",
    "# Sigmoid-Scaled Row Normalization\n",
    "\n",
    "\n",
    "# === Step 1: Replace 100 (no signal) with 0 ===\n",
    "DT[rssi_columns] = DT[rssi_columns].replace(100, 0)\n",
    "\n",
    "# === Step 2: Shift RSSI using dataset-wide min (from training) ===\n",
    "\n",
    "shift = rssi_min - 1  # shift negative RSSI to positive\n",
    "DT[rssi_columns] = DT[rssi_columns].applymap(lambda x: x - shift if x < 0 else 0)\n",
    "\n",
    "# === Step 3: Row-wise normalization (exclude zeros) ===\n",
    "row_max = DT[rssi_columns].replace(0, np.nan).max(axis=1)\n",
    "DT[rssi_columns] = DT[rssi_columns].div(row_max, axis=0).fillna(0)\n",
    "\n",
    "# === Step 4: Apply sigmoid transformation ===\n",
    "alpha = 10  # steepness of the sigmoid\n",
    "beta = 0.5 # midpoint\n",
    "DT[rssi_columns] = DT[rssi_columns].applymap(\n",
    "    lambda x: 1 / (1 + np.exp(-alpha * (x - beta))) if x > 0 else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f68dde54-2cfb-4447-845e-c58e784c63f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:raw_features]\n",
    "X_target = DT.iloc[:,:raw_features]\n",
    "\n",
    "y_floor = df.iloc[:, -3]\n",
    "y_coordinate = df.iloc[:, [-5,-4]]\n",
    "y_target_floor = DT.iloc[:,-2]\n",
    "y_target_coordinate = DT.iloc[:, [-4,-3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8b04e4c-8e84-4e4a-b883-ca5266920e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit on training labels and transform both sets\n",
    "y_floor_encoded = label_encoder.fit_transform(y_floor)\n",
    "y_target_floor_encoded = label_encoder.transform(y_target_floor)\n",
    "\n",
    "# Ensure y_floor_encoded is a pandas Series with same index as X\n",
    "y_floor_series = pd.Series(y_floor_encoded, index=X.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d2a198-15ac-45b5-aa07-2c068a5b26f9",
   "metadata": {},
   "source": [
    "# Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac95d093-f8b9-4f5a-8dbe-7c1e78bcc10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KMITL\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\KMITL\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "# Confidence weighting using Pearson correlation\n",
    "\n",
    "# Now compute correlations\n",
    "correlations = X.apply(lambda x: x.corr(y_floor_series), axis=0)\n",
    "\n",
    "# Step 6.2: Replace NaN with 0\n",
    "correlations = correlations.fillna(0)\n",
    "\n",
    "# Step 6.3: Compute confidence weights\n",
    "# You can replace this with SHAP / MI / permutation importance if available\n",
    "confidence_weights = correlations.abs()\n",
    "\n",
    "# Optional: scale weights to [0, 1]\n",
    "confidence_weights = confidence_weights / confidence_weights.max()\n",
    "\n",
    "# Step 6.4: Multiply each feature by its confidence weight\n",
    "X_weighted = X.mul(confidence_weights, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c08012b8-e899-4c9e-8bf9-37d57b497d1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WAP001</th>\n",
       "      <th>WAP002</th>\n",
       "      <th>WAP003</th>\n",
       "      <th>WAP004</th>\n",
       "      <th>WAP005</th>\n",
       "      <th>WAP006</th>\n",
       "      <th>WAP007</th>\n",
       "      <th>WAP008</th>\n",
       "      <th>WAP009</th>\n",
       "      <th>WAP010</th>\n",
       "      <th>...</th>\n",
       "      <th>WAP580</th>\n",
       "      <th>WAP581</th>\n",
       "      <th>WAP582</th>\n",
       "      <th>WAP583</th>\n",
       "      <th>WAP584</th>\n",
       "      <th>WAP585</th>\n",
       "      <th>WAP586</th>\n",
       "      <th>WAP587</th>\n",
       "      <th>WAP588</th>\n",
       "      <th>WAP589</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9103</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.681034</td>\n",
       "      <td>0.774469</td>\n",
       "      <td>0.727328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9104</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.681034</td>\n",
       "      <td>0.774469</td>\n",
       "      <td>0.727328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9105</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.681034</td>\n",
       "      <td>0.774469</td>\n",
       "      <td>0.727328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9106</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.681034</td>\n",
       "      <td>0.774469</td>\n",
       "      <td>0.727328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9107</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.681034</td>\n",
       "      <td>0.774469</td>\n",
       "      <td>0.727328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9108 rows × 557 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      WAP001  WAP002  WAP003  WAP004    WAP005  WAP006  WAP007  WAP008  \\\n",
       "0        0.0     0.0     0.0     0.0  0.000000     0.0     0.0     0.0   \n",
       "1        0.0     0.0     0.0     0.0  0.000000     0.0     0.0     0.0   \n",
       "2        0.0     0.0     0.0     0.0  0.000000     0.0     0.0     0.0   \n",
       "3        0.0     0.0     0.0     0.0  0.000000     0.0     0.0     0.0   \n",
       "4        0.0     0.0     0.0     0.0  0.000000     0.0     0.0     0.0   \n",
       "...      ...     ...     ...     ...       ...     ...     ...     ...   \n",
       "9103     0.0     0.0     0.0     0.0  0.325121     0.0     0.0     0.0   \n",
       "9104     0.0     0.0     0.0     0.0  0.325121     0.0     0.0     0.0   \n",
       "9105     0.0     0.0     0.0     0.0  0.325121     0.0     0.0     0.0   \n",
       "9106     0.0     0.0     0.0     0.0  0.325121     0.0     0.0     0.0   \n",
       "9107     0.0     0.0     0.0     0.0  0.325121     0.0     0.0     0.0   \n",
       "\n",
       "      WAP009  WAP010  ...  WAP580  WAP581  WAP582  WAP583  WAP584  WAP585  \\\n",
       "0        0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1        0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2        0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3        0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4        0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...      ...     ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "9103     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9104     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9105     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9106     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9107     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      WAP586    WAP587    WAP588    WAP589  \n",
       "0        0.0  0.000000  0.000000  0.000000  \n",
       "1        0.0  0.000000  0.000000  0.000000  \n",
       "2        0.0  0.000000  0.000000  0.000000  \n",
       "3        0.0  0.000000  0.000000  0.000000  \n",
       "4        0.0  0.000000  0.000000  0.000000  \n",
       "...      ...       ...       ...       ...  \n",
       "9103     0.0  0.681034  0.774469  0.727328  \n",
       "9104     0.0  0.681034  0.774469  0.727328  \n",
       "9105     0.0  0.681034  0.774469  0.727328  \n",
       "9106     0.0  0.681034  0.774469  0.727328  \n",
       "9107     0.0  0.681034  0.774469  0.727328  \n",
       "\n",
       "[9108 rows x 557 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confidence weighting using Pearson correlation\n",
    "\n",
    "# Step 6.5: Remove all-zero columns\n",
    "X_weighted_cleaned = X_weighted.loc[:, (X_weighted != 0).any(axis=0)]\n",
    "\n",
    "# Step 6.6: Update feature count\n",
    "raw_features = X_weighted_cleaned.shape[1]\n",
    "\n",
    "# Result: X_weighted_cleaned has transformed, normalized, and confidence-weighted features\n",
    "X_weighted_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4846bbc3-b516-436c-83e6-8cb79bd8234d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WAP001</th>\n",
       "      <th>WAP002</th>\n",
       "      <th>WAP003</th>\n",
       "      <th>WAP004</th>\n",
       "      <th>WAP005</th>\n",
       "      <th>WAP006</th>\n",
       "      <th>WAP007</th>\n",
       "      <th>WAP008</th>\n",
       "      <th>WAP009</th>\n",
       "      <th>WAP010</th>\n",
       "      <th>...</th>\n",
       "      <th>WAP580</th>\n",
       "      <th>WAP581</th>\n",
       "      <th>WAP582</th>\n",
       "      <th>WAP583</th>\n",
       "      <th>WAP584</th>\n",
       "      <th>WAP585</th>\n",
       "      <th>WAP586</th>\n",
       "      <th>WAP587</th>\n",
       "      <th>WAP588</th>\n",
       "      <th>WAP589</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040505</td>\n",
       "      <td>0.022717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>388 rows × 557 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     WAP001  WAP002  WAP003  WAP004  WAP005  WAP006  WAP007  WAP008  WAP009  \\\n",
       "0       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "383     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "384     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "385     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "386     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "387     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "     WAP010  ...  WAP580  WAP581    WAP582    WAP583  WAP584  WAP585  WAP586  \\\n",
       "0       0.0  ...     0.0     0.0  0.000000  0.000000     0.0     0.0     0.0   \n",
       "1       0.0  ...     0.0     0.0  0.000000  0.000000     0.0     0.0     0.0   \n",
       "2       0.0  ...     0.0     0.0  0.000000  0.000000     0.0     0.0     0.0   \n",
       "3       0.0  ...     0.0     0.0  0.000000  0.000000     0.0     0.0     0.0   \n",
       "4       0.0  ...     0.0     0.0  0.000000  0.000000     0.0     0.0     0.0   \n",
       "..      ...  ...     ...     ...       ...       ...     ...     ...     ...   \n",
       "383     0.0  ...     0.0     0.0  0.040505  0.022717     0.0     0.0     0.0   \n",
       "384     0.0  ...     0.0     0.0  0.000000  0.000000     0.0     0.0     0.0   \n",
       "385     0.0  ...     0.0     0.0  0.000000  0.000000     0.0     0.0     0.0   \n",
       "386     0.0  ...     0.0     0.0  0.000000  0.000000     0.0     0.0     0.0   \n",
       "387     0.0  ...     0.0     0.0  0.000000  0.000000     0.0     0.0     0.0   \n",
       "\n",
       "     WAP587  WAP588  WAP589  \n",
       "0       0.0     0.0     0.0  \n",
       "1       0.0     0.0     0.0  \n",
       "2       0.0     0.0     0.0  \n",
       "3       0.0     0.0     0.0  \n",
       "4       0.0     0.0     0.0  \n",
       "..      ...     ...     ...  \n",
       "383     0.0     0.0     0.0  \n",
       "384     0.0     0.0     0.0  \n",
       "385     0.0     0.0     0.0  \n",
       "386     0.0     0.0     0.0  \n",
       "387     0.0     0.0     0.0  \n",
       "\n",
       "[388 rows x 557 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confidence weighting using Pearson correlation\n",
    "\n",
    "# Step 2: Apply the same weights to X_target\n",
    "X_target_weighted = X_target.mul(confidence_weights, axis=1)\n",
    "\n",
    "# Step 3: Keep only the columns that were kept in training\n",
    "X_target_weighted_cleaned = X_target_weighted[X_weighted_cleaned.columns]\n",
    "\n",
    "X_target_weighted_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f6f7694-5dc4-46af-aaba-79735ce4ef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_weighted_cleaned\n",
    "X_target = X_target_weighted_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "558afdb4-fb7e-44e3-9e8e-c0f9c14ef8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "#df = pd.read_excel('3. Dataset_UTS.xlsx')\n",
    "#X = df.iloc[:, :raw_features]\n",
    "\n",
    "# Flatten all values into a single array\n",
    "all_values = X.values.flatten()\n",
    "\n",
    "# Remove NaNs and values equal to 100\n",
    "all_values = all_values[~pd.isnull(all_values)]\n",
    "all_values = all_values[all_values != 0]  # <-- Exclude 100\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Histogram\n",
    "ax.hist(all_values, bins=60, color='lightcoral', edgecolor='black', zorder=1)\n",
    "\n",
    "# Grid behind\n",
    "ax.grid(True, linestyle='--', linewidth=0.6, zorder=1)\n",
    "\n",
    "# Titles and labels\n",
    "ax.set_title(\"UTSIndoorLoc\", fontsize=24)\n",
    "ax.set_xlabel(\"Transformed RSS\", fontsize=20)\n",
    "ax.set_ylabel(\"Frequency\", fontsize=20)\n",
    "\n",
    "# ✅ Adjust tick font sizes\n",
    "ax.tick_params(axis='both', labelsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Transformed UTSIndoorLoc distribution.svg', dpi = 600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65af06da-c1f7-43a9-b733-3ad15a4accee",
   "metadata": {},
   "source": [
    "# Floor Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73782837-a155-4cdf-bb36-59ce12ca2169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import random\n",
    "import numpy as np\n",
    "from optuna.exceptions import TrialPruned\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a334f7dd-5796-42c1-8d55-521a06b7b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train-test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_floor_series, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Fix random seeds for reproducibility ---\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "613d4724-9f79-495e-8f51-4c68d18c72f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "start_time1 = time.time()\n",
    "\n",
    "# --- Custom weight function for KNN ---\n",
    "def knn_weight(d):\n",
    "    return 1 / (d + 1e-6) ** 2\n",
    "\n",
    "\n",
    "# --- Objective function for KNN ---\n",
    "def objective_wknn(trial):\n",
    "    knn = KNeighborsClassifier(\n",
    "        n_neighbors=trial.suggest_int('n_neighbors', 1, 50),\n",
    "        p=trial.suggest_int('p', 1, 5),\n",
    "        metric=trial.suggest_categorical('metric', ['minkowski', 'euclidean', 'manhattan']),\n",
    "        weights=knn_weight\n",
    "    )\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        knn.fit(X_tr, y_tr)\n",
    "        y_pred = knn.predict(X_val)\n",
    "        scores.append(accuracy_score(y_val, y_pred))\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# --- Objective function for RandomForest ---\n",
    "def objective_rf(trial):\n",
    "    rf = RandomForestClassifier(\n",
    "        max_depth=trial.suggest_categorical('max_depth', [None] + list(range(5, 51))),\n",
    "        n_estimators=trial.suggest_int('n_estimators', 50, 500),\n",
    "        min_samples_split=trial.suggest_int('min_samples_split', 2, 10),\n",
    "        min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        max_features=trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        rf.fit(X_tr, y_tr)\n",
    "        y_pred = rf.predict(X_val)\n",
    "        scores.append(accuracy_score(y_val, y_pred))\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# --- Objective function for LightGBM ---\n",
    "def objective_lgb(trial):\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=10000,\n",
    "        learning_rate=trial.suggest_float('learning_rate', 0.01, 0.5, log=True),\n",
    "        num_leaves=trial.suggest_int('num_leaves', 20, 100),\n",
    "        max_depth=trial.suggest_int('max_depth', 3, 30),\n",
    "        min_child_samples=trial.suggest_int('min_child_samples', 5, 100),\n",
    "        objective='multiclass',\n",
    "        num_class=len(np.unique(y_train)),\n",
    "        random_state=SEED,\n",
    "        verbosity=-1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='multi_error',\n",
    "            callbacks=[lgb.early_stopping(100)]\n",
    "        )\n",
    "\n",
    "        y_pred = model.predict(X_val)\n",
    "        scores.append(accuracy_score(y_val, y_pred))\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# --- Objective function for XGBoost ---\n",
    "def objective_xgb(trial):\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=trial.suggest_int('n_estimators', 50, 200),\n",
    "        learning_rate=trial.suggest_float('learning_rate', 0.01, 0.5, log=True),\n",
    "        max_depth=trial.suggest_int('max_depth', 3, 10),\n",
    "        min_child_weight=trial.suggest_float('min_child_weight', 1, 10),\n",
    "        gamma=trial.suggest_float('gamma', 0, 5),\n",
    "        subsample=trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        colsample_bytree=trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "        scores.append(accuracy_score(y_val, y_pred))\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# --- Objective function for GaussianNB ---\n",
    "def objective_gnb(trial):\n",
    "    model = GaussianNB(\n",
    "        var_smoothing=trial.suggest_float('var_smoothing', 1e-12, 1e-6, log=True)\n",
    "    )\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "        scores.append(accuracy_score(y_val, y_pred))\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# --- Objective function for AdaBoost ---\n",
    "def objective_adaboost(trial):\n",
    "    base_estimator = DecisionTreeClassifier(\n",
    "        criterion=trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss']),\n",
    "        max_depth=trial.suggest_int('max_depth', 1, 10),\n",
    "        min_samples_split=trial.suggest_int('min_samples_split', 2, 20),\n",
    "        min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "    model = AdaBoostClassifier(\n",
    "        estimator=base_estimator,\n",
    "        n_estimators=trial.suggest_int('n_estimators', 50, 300),\n",
    "        learning_rate=trial.suggest_float('learning_rate', 0.01, 1.0, log=True),\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "        scores.append(accuracy_score(y_val, y_pred))\n",
    "\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cb03469-3c92-4cc2-95f6-61c4ac454f36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-20 15:16:22,968] A new study created in memory with name: no-name-1e19ac5f-6618-4c04-904d-e7f74376bb4a\n",
      "[I 2025-12-20 15:16:32,164] Trial 0 finished with value: 0.9965691383444758 and parameters: {'n_neighbors': 5, 'p': 4, 'metric': 'manhattan'}. Best is trial 0 with value: 0.9965691383444758.\n",
      "[I 2025-12-20 15:16:37,047] Trial 1 finished with value: 0.9906675403637706 and parameters: {'n_neighbors': 26, 'p': 2, 'metric': 'euclidean'}. Best is trial 0 with value: 0.9965691383444758.\n",
      "[I 2025-12-20 15:16:46,480] Trial 2 finished with value: 0.9962945074767948 and parameters: {'n_neighbors': 8, 'p': 4, 'metric': 'manhattan'}. Best is trial 0 with value: 0.9965691383444758.\n",
      "[I 2025-12-20 15:17:49,555] Trial 3 finished with value: 0.9884712466094809 and parameters: {'n_neighbors': 5, 'p': 5, 'metric': 'minkowski'}. Best is trial 0 with value: 0.9965691383444758.\n",
      "[I 2025-12-20 15:17:58,676] Trial 4 finished with value: 0.9947847438175103 and parameters: {'n_neighbors': 22, 'p': 1, 'metric': 'manhattan'}. Best is trial 0 with value: 0.9965691383444758.\n",
      "[I 2025-12-20 15:18:05,006] Trial 5 finished with value: 0.9925888266567998 and parameters: {'n_neighbors': 13, 'p': 5, 'metric': 'euclidean'}. Best is trial 0 with value: 0.9965691383444758.\n",
      "[I 2025-12-20 15:19:12,216] Trial 6 finished with value: 0.9861379669407325 and parameters: {'n_neighbors': 19, 'p': 4, 'metric': 'minkowski'}. Best is trial 0 with value: 0.9965691383444758.\n",
      "[I 2025-12-20 15:19:17,296] Trial 7 finished with value: 0.9905301778557327 and parameters: {'n_neighbors': 23, 'p': 2, 'metric': 'euclidean'}. Best is trial 0 with value: 0.9965691383444758.\n",
      "[I 2025-12-20 15:19:27,153] Trial 8 finished with value: 0.9942356703789379 and parameters: {'n_neighbors': 34, 'p': 5, 'metric': 'manhattan'}. Best is trial 0 with value: 0.9965691383444758.\n",
      "[I 2025-12-20 15:19:33,553] Trial 9 finished with value: 0.9950593746851913 and parameters: {'n_neighbors': 2, 'p': 4, 'metric': 'euclidean'}. Best is trial 0 with value: 0.9965691383444758.\n",
      "[I 2025-12-20 15:19:43,302] Trial 10 finished with value: 0.9936865969403655 and parameters: {'n_neighbors': 46, 'p': 3, 'metric': 'manhattan'}. Best is trial 0 with value: 0.9965691383444758.\n",
      "[I 2025-12-20 15:19:51,892] Trial 11 finished with value: 0.9962945074767948 and parameters: {'n_neighbors': 8, 'p': 4, 'metric': 'manhattan'}. Best is trial 0 with value: 0.9965691383444758.\n",
      "[I 2025-12-20 15:20:00,466] Trial 12 finished with value: 0.9953338172560828 and parameters: {'n_neighbors': 11, 'p': 3, 'metric': 'manhattan'}. Best is trial 0 with value: 0.9965691383444758.\n",
      "[I 2025-12-20 15:20:11,996] Trial 13 finished with value: 0.99739246605715 and parameters: {'n_neighbors': 1, 'p': 4, 'metric': 'manhattan'}. Best is trial 13 with value: 0.99739246605715.\n",
      "[I 2025-12-20 15:20:20,752] Trial 14 finished with value: 0.99739246605715 and parameters: {'n_neighbors': 2, 'p': 3, 'metric': 'manhattan'}. Best is trial 13 with value: 0.99739246605715.\n",
      "[I 2025-12-20 15:20:29,360] Trial 15 finished with value: 0.99739246605715 and parameters: {'n_neighbors': 1, 'p': 2, 'metric': 'manhattan'}. Best is trial 13 with value: 0.99739246605715.\n",
      "[I 2025-12-20 15:21:32,859] Trial 16 finished with value: 0.9886084208207292 and parameters: {'n_neighbors': 15, 'p': 3, 'metric': 'minkowski'}. Best is trial 13 with value: 0.99739246605715.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best KNN hyperparameters: {'n_neighbors': 1, 'p': 4, 'metric': 'manhattan'}\n",
      "KNN Cross-val Accuracies: [0.99862826 1.         0.99862826 1.         0.99588477 0.99725652\n",
      " 0.99862637 0.99862637 1.         0.99725275]\n",
      "KNN Mean CV Accuracy: 0.9984903299718116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['knn_floor_UTS.joblib']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN\n",
    "# --- Run the optimization ---\n",
    "study1 = optuna.create_study(direction='maximize')\n",
    "study1.optimize(objective_wknn, n_trials=100, timeout=300)\n",
    "\n",
    "# --- Get best parameters ---\n",
    "best_params1 = study1.best_params\n",
    "print(\"Best KNN hyperparameters:\", best_params1)\n",
    "\n",
    "# --- Train final model ---\n",
    "knn_floor_final = KNeighborsClassifier(\n",
    "    n_neighbors=best_params1['n_neighbors'],\n",
    "    weights=knn_weight,\n",
    "    p=best_params1['p'],\n",
    "    metric=best_params1['metric']\n",
    ")\n",
    "\n",
    "# --- Cross-validation accuracy ---\n",
    "cv_scores_knn = cross_val_score(knn_floor_final, X_train, y_train, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "print(\"KNN Cross-val Accuracies:\", cv_scores_knn)\n",
    "print(\"KNN Mean CV Accuracy:\", cv_scores_knn.mean())\n",
    "\n",
    "# --- Train on full training set ---\n",
    "knn_floor_final.fit(X_train, y_train)\n",
    "joblib.dump(knn_floor_final, 'knn_floor_UTS.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0fffb7bd-c1fa-4486-ac88-3cc17a7c8e4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-20 15:21:48,825] A new study created in memory with name: no-name-b50f4cc4-68a0-46ca-8ae1-f816469d375b\n",
      "[I 2025-12-20 15:22:03,221] Trial 0 finished with value: 0.992040506405386 and parameters: {'max_depth': 28, 'n_estimators': 359, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 0 with value: 0.992040506405386.\n",
      "[I 2025-12-20 15:22:16,573] Trial 1 finished with value: 0.7705189365373915 and parameters: {'max_depth': 5, 'n_estimators': 243, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.992040506405386.\n",
      "[I 2025-12-20 15:22:26,205] Trial 2 finished with value: 0.9938243360419827 and parameters: {'max_depth': 18, 'n_estimators': 107, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 2 with value: 0.9938243360419827.\n",
      "[I 2025-12-20 15:23:49,071] Trial 3 finished with value: 0.9803737314680653 and parameters: {'max_depth': 30, 'n_estimators': 339, 'min_samples_split': 6, 'min_samples_leaf': 10, 'max_features': None}. Best is trial 2 with value: 0.9938243360419827.\n",
      "[I 2025-12-20 15:25:07,378] Trial 4 finished with value: 0.9936869735339448 and parameters: {'max_depth': 34, 'n_estimators': 305, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 2 with value: 0.9938243360419827.\n",
      "[I 2025-12-20 15:27:05,325] Trial 5 finished with value: 0.9910794395910948 and parameters: {'max_depth': 46, 'n_estimators': 475, 'min_samples_split': 5, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 2 with value: 0.9938243360419827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest hyperparameters: {'max_depth': 18, 'n_estimators': 107, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2'}\n",
      "RF Cross-val Accuracies: [1.         0.99314129 1.         0.99176955 0.99176955 0.99314129\n",
      " 0.99587912 0.99175824 0.99862637 0.99587912]\n",
      "RF Mean CV Accuracy: 0.9951964530668235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rf_floor_pred_UTS.joblib']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "# --- Run the optimization ---\n",
    "study2 = optuna.create_study(direction='maximize')\n",
    "study2.optimize(objective_rf, n_trials=100, timeout=300)\n",
    "\n",
    "# --- Train final model with best hyperparameters ---\n",
    "best_params2 = study2.best_params\n",
    "print(\"Best Random Forest hyperparameters:\", best_params2)\n",
    "\n",
    "rf_floor_final = RandomForestClassifier(\n",
    "    **best_params2,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# --- Cross-validation accuracy ---\n",
    "cv_scores_rf = cross_val_score(rf_floor_final, X_train, y_train, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "print(\"RF Cross-val Accuracies:\", cv_scores_rf)\n",
    "print(\"RF Mean CV Accuracy:\", cv_scores_rf.mean())\n",
    "\n",
    "# --- Train on full training set ---\n",
    "rf_floor_final.fit(X_train, y_train)\n",
    "joblib.dump(rf_floor_final, 'rf_floor_pred_UTS.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "883687af-9cde-43dc-b3c7-dde772b12747",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-20 15:27:18,728] A new study created in memory with name: no-name-4190ba18-0b18-4843-8cd7-077788bb7f48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid_0's multi_error: 0.000685871\tvalid_0's multi_logloss: 0.00664267\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[73]\tvalid_0's multi_error: 0.000686342\tvalid_0's multi_logloss: 0.00878383\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[148]\tvalid_0's multi_error: 0.000686342\tvalid_0's multi_logloss: 0.00245642\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[133]\tvalid_0's multi_error: 0.00274537\tvalid_0's multi_logloss: 0.00581658\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-20 15:56:30,771] Trial 0 finished with value: 0.998764678911607 and parameters: {'learning_rate': 0.06073158710801616, 'num_leaves': 83, 'max_depth': 13, 'min_child_samples': 82}. Best is trial 0 with value: 0.998764678911607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[59]\tvalid_0's multi_error: 0.00137268\tvalid_0's multi_logloss: 0.015407\n",
      "Best LightGBM hyperparameters: {'learning_rate': 0.06073158710801616, 'num_leaves': 83, 'max_depth': 13, 'min_child_samples': 82}\n",
      "LGB Cross-val Accuracies: [0.99862826 0.99862826 1.         1.         1.         0.99862826\n",
      " 0.99862637 1.         0.99862637 1.        ]\n",
      "LGB Mean CV Accuracy: 0.99931375209153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lgb_floor_pred_UTS.joblib']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Optuna study\n",
    "study3 = optuna.create_study(direction='maximize')\n",
    "study3.optimize(objective_lgb, n_trials=100, timeout=300)\n",
    "\n",
    "# Best params\n",
    "best_params3 = study3.best_params\n",
    "print(\"Best LightGBM hyperparameters:\", best_params3)\n",
    "\n",
    "best_params3.update({\n",
    "    'random_state': SEED,\n",
    "    'n_jobs': -1,\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': len(np.unique(y_train))\n",
    "})\n",
    "\n",
    "# Final model\n",
    "lgb_floor_final = lgb.LGBMClassifier(**best_params3)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores_lgb = cross_val_score(lgb_floor_final, X_train, y_train, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "print(\"LGB Cross-val Accuracies:\", cv_scores_lgb)\n",
    "print(\"LGB Mean CV Accuracy:\", cv_scores_lgb.mean())\n",
    "\n",
    "# Train final model\n",
    "lgb_floor_final.fit(X_train, y_train)\n",
    "\n",
    "# Save\n",
    "joblib.dump(lgb_floor_final, 'lgb_floor_pred_UTS.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ffcdd3d-78a1-40d3-8e6c-51fa2c368e75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-20 15:57:34,791] A new study created in memory with name: no-name-97da2c6e-6613-477c-acd9-5e4a19f50372\n",
      "[I 2025-12-20 15:58:49,896] Trial 0 finished with value: 0.9986275988487534 and parameters: {'n_estimators': 150, 'learning_rate': 0.020356790820459825, 'max_depth': 10, 'min_child_weight': 1.0796362989803954, 'gamma': 1.1827041138524246, 'subsample': 0.7599385810747621, 'colsample_bytree': 0.6630540937234843}. Best is trial 0 with value: 0.9986275988487534.\n",
      "[I 2025-12-20 15:59:50,055] Trial 1 finished with value: 0.998215887918219 and parameters: {'n_estimators': 114, 'learning_rate': 0.039638184784598914, 'max_depth': 6, 'min_child_weight': 9.305004671877706, 'gamma': 2.747674942769478, 'subsample': 0.8445325778486336, 'colsample_bytree': 0.8841526747660291}. Best is trial 0 with value: 0.9986275988487534.\n",
      "[I 2025-12-20 16:00:26,474] Trial 2 finished with value: 0.9982160762150085 and parameters: {'n_estimators': 103, 'learning_rate': 0.1343356294543574, 'max_depth': 5, 'min_child_weight': 4.054576008851628, 'gamma': 4.117450692548361, 'subsample': 0.775189051636134, 'colsample_bytree': 0.5174686907123054}. Best is trial 0 with value: 0.9986275988487534.\n",
      "[I 2025-12-20 16:00:52,930] Trial 3 finished with value: 0.9956078832333949 and parameters: {'n_estimators': 118, 'learning_rate': 0.3563954276680645, 'max_depth': 9, 'min_child_weight': 3.8449521222927086, 'gamma': 4.849039831850506, 'subsample': 0.5997423919030724, 'colsample_bytree': 0.8412183292989934}. Best is trial 0 with value: 0.9986275988487534.\n",
      "[I 2025-12-20 16:01:34,791] Trial 4 finished with value: 0.9983530621294673 and parameters: {'n_estimators': 127, 'learning_rate': 0.04894824869967176, 'max_depth': 3, 'min_child_weight': 5.828966867536283, 'gamma': 1.195725381573833, 'subsample': 0.579824506421831, 'colsample_bytree': 0.9904662873088952}. Best is trial 0 with value: 0.9986275988487534.\n",
      "[I 2025-12-20 16:02:06,087] Trial 5 finished with value: 0.9983532504262568 and parameters: {'n_estimators': 83, 'learning_rate': 0.07676330535418684, 'max_depth': 9, 'min_child_weight': 5.289565533108313, 'gamma': 1.2422006958760816, 'subsample': 0.537173719610693, 'colsample_bytree': 0.6615454564349126}. Best is trial 0 with value: 0.9986275988487534.\n",
      "[I 2025-12-20 16:03:41,459] Trial 6 finished with value: 0.9927259067196534 and parameters: {'n_estimators': 178, 'learning_rate': 0.014012217322794028, 'max_depth': 10, 'min_child_weight': 9.849194720274674, 'gamma': 2.4550312087633057, 'subsample': 0.6169225991669716, 'colsample_bytree': 0.7505631474726552}. Best is trial 0 with value: 0.9986275988487534.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost hyperparameters: {'n_estimators': 150, 'learning_rate': 0.020356790820459825, 'max_depth': 10, 'min_child_weight': 1.0796362989803954, 'gamma': 1.1827041138524246, 'subsample': 0.7599385810747621, 'colsample_bytree': 0.6630540937234843}\n",
      "XGB Cross-val Accuracies: [0.99725652 0.99862826 1.         1.         0.99862826 0.99862826\n",
      " 0.99725275 1.         0.99725275 1.        ]\n",
      "XGB Mean CV Accuracy: 0.998764678394308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KMITL\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:158: UserWarning: [16:04:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgb_floor_pred_UTS.joblib']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoost\n",
    "# --- Run optimization ---\n",
    "study4 = optuna.create_study(direction='maximize')\n",
    "study4.optimize(objective_xgb, n_trials=100, timeout=300)\n",
    "\n",
    "# --- Best parameters ---\n",
    "best_params4 = study4.best_params\n",
    "print(\"Best XGBoost hyperparameters:\", best_params4)\n",
    "\n",
    "# --- Train final model with best params ---\n",
    "xgb_floor_final = XGBClassifier(**best_params4, use_label_encoder=False, eval_metric='mlogloss', random_state=SEED, n_jobs=-1)\n",
    "\n",
    "# --- Cross-validation accuracy ---\n",
    "cv_scores_xgb = cross_val_score(xgb_floor_final, X_train, y_train, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "print(\"XGB Cross-val Accuracies:\", cv_scores_xgb)\n",
    "print(\"XGB Mean CV Accuracy:\", cv_scores_xgb.mean())\n",
    "\n",
    "# --- Train on full training set ---\n",
    "xgb_floor_final.fit(X_train, y_train)\n",
    "joblib.dump(xgb_floor_final, 'xgb_floor_pred_UTS.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59b1bc28-b36f-4f27-8663-b6149bb04f99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-20 16:04:56,385] A new study created in memory with name: no-name-2df27f02-b763-43c3-9f2c-135d88557308\n",
      "[I 2025-12-20 16:04:58,019] Trial 0 finished with value: 0.948394534497384 and parameters: {'var_smoothing': 2.1740831410217663e-10}. Best is trial 0 with value: 0.948394534497384.\n",
      "[I 2025-12-20 16:04:59,700] Trial 1 finished with value: 0.9486690712166702 and parameters: {'var_smoothing': 9.382764133288907e-10}. Best is trial 1 with value: 0.9486690712166702.\n",
      "[I 2025-12-20 16:05:01,363] Trial 2 finished with value: 0.946747784923641 and parameters: {'var_smoothing': 3.743872025091259e-11}. Best is trial 1 with value: 0.9486690712166702.\n",
      "[I 2025-12-20 16:05:03,064] Trial 3 finished with value: 0.9493550364213064 and parameters: {'var_smoothing': 5.102959173103421e-08}. Best is trial 3 with value: 0.9493550364213064.\n",
      "[I 2025-12-20 16:05:05,069] Trial 4 finished with value: 0.9452383978579357 and parameters: {'var_smoothing': 1.2557633073710208e-11}. Best is trial 3 with value: 0.9493550364213064.\n",
      "[I 2025-12-20 16:05:06,802] Trial 5 finished with value: 0.9486690712166702 and parameters: {'var_smoothing': 6.618954553746775e-10}. Best is trial 3 with value: 0.9493550364213064.\n",
      "[I 2025-12-20 16:05:08,545] Trial 6 finished with value: 0.9492177680616634 and parameters: {'var_smoothing': 1.1855054013547923e-08}. Best is trial 3 with value: 0.9493550364213064.\n",
      "[I 2025-12-20 16:05:10,214] Trial 7 finished with value: 0.948394534497384 and parameters: {'var_smoothing': 1.1769817349735035e-10}. Best is trial 3 with value: 0.9493550364213064.\n",
      "[I 2025-12-20 16:05:11,968] Trial 8 finished with value: 0.9463362622898961 and parameters: {'var_smoothing': 2.9514914417352075e-11}. Best is trial 3 with value: 0.9493550364213064.\n",
      "[I 2025-12-20 16:05:13,715] Trial 9 finished with value: 0.9486688829198806 and parameters: {'var_smoothing': 6.983296885045866e-09}. Best is trial 3 with value: 0.9493550364213064.\n",
      "[I 2025-12-20 16:05:15,389] Trial 10 finished with value: 0.9471594958541754 and parameters: {'var_smoothing': 7.446597044181447e-07}. Best is trial 3 with value: 0.9493550364213064.\n",
      "[I 2025-12-20 16:05:17,386] Trial 11 finished with value: 0.9493550364213064 and parameters: {'var_smoothing': 3.491611835099087e-08}. Best is trial 3 with value: 0.9493550364213064.\n",
      "[I 2025-12-20 16:05:19,193] Trial 12 finished with value: 0.9477083809959582 and parameters: {'var_smoothing': 2.6808041765282897e-07}. Best is trial 3 with value: 0.9493550364213064.\n",
      "[I 2025-12-20 16:05:20,939] Trial 13 finished with value: 0.9438659025582943 and parameters: {'var_smoothing': 1.127373595400742e-12}. Best is trial 3 with value: 0.9493550364213064.\n",
      "[I 2025-12-20 16:05:22,645] Trial 14 finished with value: 0.9488063395763133 and parameters: {'var_smoothing': 6.056964133155657e-08}. Best is trial 3 with value: 0.9493550364213064.\n",
      "[I 2025-12-20 16:05:24,286] Trial 15 finished with value: 0.9489432313423771 and parameters: {'var_smoothing': 7.50135770217257e-09}. Best is trial 3 with value: 0.9493550364213064.\n",
      "[I 2025-12-20 16:05:25,989] Trial 16 finished with value: 0.9492181446552428 and parameters: {'var_smoothing': 5.665015814579029e-08}. Best is trial 3 with value: 0.9493550364213064.\n",
      "[I 2025-12-20 16:05:27,956] Trial 17 finished with value: 0.9488063395763133 and parameters: {'var_smoothing': 6.1559783300535e-08}. Best is trial 3 with value: 0.9493550364213064.\n",
      "[I 2025-12-20 16:05:29,668] Trial 18 finished with value: 0.948531897005422 and parameters: {'var_smoothing': 3.454074222454638e-09}. Best is trial 3 with value: 0.9493550364213064.\n",
      "[I 2025-12-20 16:05:31,366] Trial 19 finished with value: 0.9481199977780979 and parameters: {'var_smoothing': 3.860630246859217e-07}. Best is trial 3 with value: 0.9493550364213064.\n",
      "[I 2025-12-20 16:05:33,093] Trial 20 finished with value: 0.9494923047809495 and parameters: {'var_smoothing': 2.408153430684379e-08}. Best is trial 20 with value: 0.9494923047809495.\n",
      "[I 2025-12-20 16:05:34,850] Trial 21 finished with value: 0.9497668415002358 and parameters: {'var_smoothing': 3.161810966631307e-08}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:05:36,590] Trial 22 finished with value: 0.9490810645923892 and parameters: {'var_smoothing': 1.4551374147701267e-07}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:05:38,565] Trial 23 finished with value: 0.9492177680616634 and parameters: {'var_smoothing': 1.8242520756421094e-08}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:05:40,176] Trial 24 finished with value: 0.948531897005422 and parameters: {'var_smoothing': 3.4855466228159077e-09}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:05:41,970] Trial 25 finished with value: 0.9492183329520323 and parameters: {'var_smoothing': 1.3842785831377397e-07}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:05:43,681] Trial 26 finished with value: 0.9483947227941737 and parameters: {'var_smoothing': 2.252369519534646e-09}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:05:45,396] Trial 27 finished with value: 0.9474340325734616 and parameters: {'var_smoothing': 8.924383169037021e-07}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:05:47,227] Trial 28 finished with value: 0.9494923047809495 and parameters: {'var_smoothing': 2.7900118917608764e-08}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:05:49,005] Trial 29 finished with value: 0.9488063395763133 and parameters: {'var_smoothing': 3.9977341086692333e-10}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:05:50,474] Trial 30 finished with value: 0.9494923047809495 and parameters: {'var_smoothing': 2.1520366195015536e-08}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:05:52,012] Trial 31 finished with value: 0.9492177680616634 and parameters: {'var_smoothing': 2.038868011600764e-08}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:05:54,081] Trial 32 finished with value: 0.9485319911538168 and parameters: {'var_smoothing': 2.143862693729851e-09}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:05:55,882] Trial 33 finished with value: 0.9485319911538168 and parameters: {'var_smoothing': 1.1955343924082284e-07}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:05:57,687] Trial 34 finished with value: 0.9494923047809495 and parameters: {'var_smoothing': 2.2857873144982773e-08}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:05:59,464] Trial 35 finished with value: 0.9492177680616634 and parameters: {'var_smoothing': 7.868902747526547e-09}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:06:01,614] Trial 36 finished with value: 0.9477083809959582 and parameters: {'var_smoothing': 2.941065745545707e-07}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:06:03,359] Trial 37 finished with value: 0.9486690712166702 and parameters: {'var_smoothing': 8.090521234964234e-10}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:06:05,160] Trial 38 finished with value: 0.9497668415002358 and parameters: {'var_smoothing': 3.247491971144011e-08}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:06:06,912] Trial 39 finished with value: 0.9496300438825669 and parameters: {'var_smoothing': 1.7223522073338216e-09}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:06:08,770] Trial 40 finished with value: 0.948394534497384 and parameters: {'var_smoothing': 1.5200304215574126e-10}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:06:10,587] Trial 41 finished with value: 0.9490809704439943 and parameters: {'var_smoothing': 1.4008865287309014e-09}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:06:12,619] Trial 42 finished with value: 0.9492177680616634 and parameters: {'var_smoothing': 9.619678348497907e-09}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:06:14,302] Trial 43 finished with value: 0.9486688829198806 and parameters: {'var_smoothing': 5.0737888585900185e-09}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:06:15,997] Trial 44 finished with value: 0.9485318028570271 and parameters: {'var_smoothing': 3.3401898246796683e-10}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:06:17,752] Trial 45 finished with value: 0.9493550364213064 and parameters: {'var_smoothing': 3.760860098345051e-08}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:06:19,548] Trial 46 finished with value: 0.9477086634411427 and parameters: {'var_smoothing': 5.100103308571545e-11}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:06:21,175] Trial 47 finished with value: 0.9492177680616634 and parameters: {'var_smoothing': 1.3826331478646355e-08}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:06:22,939] Trial 48 finished with value: 0.9485319911538168 and parameters: {'var_smoothing': 9.439244013450381e-08}. Best is trial 21 with value: 0.9497668415002358.\n",
      "[I 2025-12-20 16:06:24,892] Trial 49 finished with value: 0.9493555071632805 and parameters: {'var_smoothing': 1.4956821354993045e-09}. Best is trial 21 with value: 0.9497668415002358.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Naive Bayes hyperparameters: {'var_smoothing': 3.161810966631307e-08}\n",
      "GNB Cross-val Accuracies: [0.9478738  0.9478738  0.94513032 0.95610425 0.95473251 0.94650206\n",
      " 0.93406593 0.9532967  0.93818681 0.95604396]\n",
      "GNB Mean CV Accuracy: 0.947981014184718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['gnb_floor_pred_UTS.joblib']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gaussian Naive Bayes\n",
    "# --- Run the optimization ---\n",
    "study7 = optuna.create_study(direction='maximize')\n",
    "study7.optimize(objective_gnb, n_trials=50, timeout=200)\n",
    "\n",
    "# --- Train final model with best hyperparameters ---\n",
    "best_params7 = study7.best_params\n",
    "print(\"Best Naive Bayes hyperparameters:\", best_params7)\n",
    "\n",
    "gnb_floor_final = GaussianNB(**best_params7)\n",
    "\n",
    "# --- Cross-validation accuracy ---\n",
    "cv_scores_gnb = cross_val_score(gnb_floor_final, X_train, y_train, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "print(\"GNB Cross-val Accuracies:\", cv_scores_gnb)\n",
    "print(\"GNB Mean CV Accuracy:\", cv_scores_gnb.mean())\n",
    "\n",
    "# --- Train on full training set ---\n",
    "gnb_floor_final.fit(X_train, y_train)\n",
    "joblib.dump(gnb_floor_final, 'gnb_floor_pred_UTS.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc06de20-a6e6-4f2f-b016-5361dc4673e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-20 16:06:28,831] A new study created in memory with name: no-name-b5a0d95a-94c5-4f56-9faa-2184f04d4bfd\n",
      "[I 2025-12-20 16:09:31,851] Trial 0 finished with value: 0.9887462540707412 and parameters: {'criterion': 'log_loss', 'max_depth': 4, 'min_samples_split': 16, 'min_samples_leaf': 17, 'n_estimators': 253, 'learning_rate': 0.1565957542919959}. Best is trial 0 with value: 0.9887462540707412.\n",
      "[I 2025-12-20 16:15:47,836] Trial 1 finished with value: 0.9984903304891104 and parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 11, 'min_samples_leaf': 4, 'n_estimators': 282, 'learning_rate': 0.20954975837372336}. Best is trial 1 with value: 0.9984903304891104.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 11, 'min_samples_leaf': 4, 'n_estimators': 282, 'learning_rate': 0.20954975837372336}\n",
      "AdaBoost Cross-val Accuracies: [1.         0.99862826 1.         1.         0.99862826 0.99725652\n",
      " 0.99862637 1.         0.99587912 1.        ]\n",
      "AdaBoost Mean CV Accuracy: 0.9989018526055563\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['adaboost_floor_pred_UTS.joblib']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import joblib\n",
    "\n",
    "# --- Run the optimization ---\n",
    "study_ab = optuna.create_study(direction='maximize')\n",
    "study_ab.optimize(objective_adaboost, n_trials=100, timeout=300)\n",
    "\n",
    "# --- Train final model with best hyperparameters ---\n",
    "best_params_ab = study_ab.best_params\n",
    "print(\"Best hyperparameters:\", best_params_ab)\n",
    "\n",
    "# Extract AdaBoost-specific parameters\n",
    "ada_params = {\n",
    "    'n_estimators': best_params_ab.pop('n_estimators'),\n",
    "    'learning_rate': best_params_ab.pop('learning_rate'),\n",
    "    'random_state': SEED\n",
    "}\n",
    "\n",
    "# Remaining params go to base estimator (DecisionTree)\n",
    "base_estimator = DecisionTreeClassifier(**best_params_ab, random_state=SEED)\n",
    "\n",
    "# Build final AdaBoost model\n",
    "ab_floor_final = AdaBoostClassifier(\n",
    "    estimator=base_estimator,\n",
    "    **ada_params\n",
    ")\n",
    "\n",
    "# --- Cross-validation accuracy ---\n",
    "cv_scores_ab = cross_val_score(ab_floor_final, X_train, y_train, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "print(\"AdaBoost Cross-val Accuracies:\", cv_scores_ab)\n",
    "print(\"AdaBoost Mean CV Accuracy:\", cv_scores_ab.mean())\n",
    "\n",
    "# --- Train on full training set ---\n",
    "ab_floor_final.fit(X_train, y_train)\n",
    "joblib.dump(ab_floor_final, 'adaboost_floor_pred_UTS.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2902001-8cc1-43d7-83a1-2b7146631e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "        (\"KNN\", knn_floor_final),\n",
    "        (\"RF\", rf_floor_final),\n",
    "        (\"LGB\", lgb_floor_final),\n",
    "        (\"XGB\", xgb_floor_final),\n",
    "        (\"NB\", gnb_floor_final),\n",
    "        (\"AB\", ab_floor_final)\n",
    "    ]\n",
    "\n",
    "accuracy_knn = cv_scores_knn.mean()\n",
    "accuracy_rf = cv_scores_rf.mean()\n",
    "accuracy_lgb = cv_scores_lgb.mean()\n",
    "accuracy_xgb = cv_scores_xgb.mean()\n",
    "accuracy_gnb = cv_scores_gnb.mean()\n",
    "accuracy_ab = cv_scores_ab.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c4e04a1-6cba-4562-9544-82e1e42b23fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IEO-CVV\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def ieo_cvv_ensemble_selection(models, X_train, y_train, X_test, y_test, cv=10, patience=3):\n",
    "    print(\"Initializing Iterative Ensemble Optimization with CV Voting (IEO-CVV)...\")\n",
    "\n",
    "    # --- Step 1: Compute cross-val scores and predictions ---\n",
    "    model_scores = {}\n",
    "    model_preds = {}\n",
    "\n",
    "    for name, model in models:\n",
    "        print(f\"Evaluating {name}...\")\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "        acc = cv_scores.mean()\n",
    "        model_scores[name] = acc\n",
    "\n",
    "        y_pred_cv = cross_val_predict(model, X_train, y_train, cv=cv, method='predict', n_jobs=-1)\n",
    "        model_preds[name] = y_pred_cv\n",
    "\n",
    "        print(f\"{name} CV Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Sort models by CV accuracy\n",
    "    sorted_models = sorted(models, key=lambda m: model_scores[m[0]], reverse=True)\n",
    "\n",
    "    # --- Ensemble voting function (CV-based predictions) ---\n",
    "    def ensemble_predict_cv(selected_model_names):\n",
    "        preds = np.array([model_preds[name] for name in selected_model_names])\n",
    "        weights = np.array([model_scores[name] for name in selected_model_names])\n",
    "        weighted_preds = []\n",
    "\n",
    "        for i in range(preds.shape[1]):\n",
    "            classes = np.unique(preds[:, i])\n",
    "            vote_score = {c: 0.0 for c in classes}\n",
    "            for j, c in enumerate(preds[:, i]):\n",
    "                vote_score[c] += weights[j]\n",
    "            final_class = max(vote_score, key=vote_score.get)\n",
    "            weighted_preds.append(final_class)\n",
    "\n",
    "        return np.array(weighted_preds)\n",
    "\n",
    "    # --- Step 2: Initialize ensemble with top 2 models ---\n",
    "    ensemble = sorted_models[:2]  # Changed here: top 2 models\n",
    "    ensemble_names = [name for name, _ in ensemble]\n",
    "    best_acc = accuracy_score(y_train, ensemble_predict_cv(ensemble_names))\n",
    "    print(f\"Initial Ensemble: {[n for n, _ in ensemble]} - Train Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "    # --- Step 3: Iterative forward selection with backward pruning ---\n",
    "    patience_counter = 0\n",
    "    remaining_models = sorted_models[2:]\n",
    "\n",
    "    while remaining_models and patience_counter < patience:\n",
    "        improved = False\n",
    "\n",
    "        name, model = remaining_models.pop(0)\n",
    "        temp_ensemble = ensemble + [(name, model)]\n",
    "        temp_names = [n for n, _ in temp_ensemble]\n",
    "\n",
    "        y_pred_temp = ensemble_predict_cv(temp_names)\n",
    "        new_acc = accuracy_score(y_train, y_pred_temp)\n",
    "\n",
    "        if new_acc > best_acc:\n",
    "            ensemble = temp_ensemble\n",
    "            best_acc = new_acc\n",
    "            improved = True\n",
    "            patience_counter = 0\n",
    "            print(f\"Added {name} - Improved Train Accuracy: {new_acc:.4f}\")\n",
    "\n",
    "            # --- Backward pruning ---\n",
    "            pruned = True\n",
    "            while pruned and len(ensemble) > 1:\n",
    "                pruned = False\n",
    "                for n, m in ensemble:\n",
    "                    if n == name:\n",
    "                        continue\n",
    "                    temp_ensemble_pruned = [(x, y) for x, y in ensemble if x != n]\n",
    "                    temp_names_pruned = [x for x, _ in temp_ensemble_pruned]\n",
    "                    y_pred_pruned = ensemble_predict_cv(temp_names_pruned)\n",
    "                    pruned_acc = accuracy_score(y_train, y_pred_pruned)\n",
    "\n",
    "                    if pruned_acc >= best_acc:\n",
    "                        print(f\"Pruned {n} - Accuracy maintained/improved: {pruned_acc:.4f}\")\n",
    "                        ensemble = temp_ensemble_pruned\n",
    "                        best_acc = pruned_acc\n",
    "                        pruned = True\n",
    "                        break\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Ignored {name} - No improvement: {new_acc:.4f} (Patience: {patience_counter}/{patience})\")\n",
    "\n",
    "    # --- Final weights based on normalized CV accuracies ---\n",
    "    final_names = [n for n, _ in ensemble]\n",
    "    weights = {name: model_scores[name] for name in final_names}\n",
    "    total = sum(weights.values())\n",
    "    weights = {k: v / total for k, v in weights.items()}\n",
    "\n",
    "    print(\"Final Ensemble Members:\", final_names)\n",
    "    print(\"Final Ensemble Train Accuracy:\", best_acc)\n",
    "\n",
    "    # --- Step 4: Retrain final models and test ensemble ---\n",
    "    print(\"\\nRetraining final models and evaluating on test set...\")\n",
    "    for name, model in ensemble:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    def ensemble_predict_test(selected_models, X):\n",
    "        preds = np.array([m.predict(X).ravel() for _, m in selected_models])\n",
    "        weights = np.array([model_scores[name] for name, _ in selected_models])\n",
    "        weighted_preds = []\n",
    "        for i in range(X.shape[0]):\n",
    "            classes = np.unique(preds[:, i])\n",
    "            vote_score = {c: 0.0 for c in classes}\n",
    "            for j, c in enumerate(preds[:, i]):\n",
    "                vote_score[c] += weights[j]\n",
    "            final_class = max(vote_score, key=vote_score.get)\n",
    "            weighted_preds.append(final_class)\n",
    "        return np.array(weighted_preds)\n",
    "\n",
    "    y_pred_test = ensemble_predict_test(ensemble, X_test)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    print(\"Final Ensemble Test Accuracy:\", test_acc)\n",
    "\n",
    "    return ensemble, best_acc, weights, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9ff48f4-a61f-4ab5-baf8-447206d66df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(ensemble, X, model_scores):\n",
    "    preds = np.array([m.predict(X).ravel() for _, m in ensemble])\n",
    "    weights = np.array([model_scores[name] for name, _ in ensemble])\n",
    "\n",
    "    weighted_preds = []\n",
    "    for i in range(X.shape[0]):\n",
    "        classes, counts = np.unique(preds[:, i], return_counts=True)\n",
    "        # Weighted voting\n",
    "        vote_score = {c: 0 for c in classes}\n",
    "        for j, c in enumerate(preds[:, i]):\n",
    "            vote_score[c] += weights[j]\n",
    "        weighted_preds.append(max(vote_score, key=vote_score.get))\n",
    "\n",
    "    return np.array(weighted_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca33ff9-1acd-4908-945e-6cbefb662468",
   "metadata": {},
   "source": [
    "# IEO-CVV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23debaca-9419-4a1f-b1c1-58a715eb7f35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Iterative Ensemble Optimization with CV Voting (IEO-CVV)...\n",
      "Evaluating KNN...\n",
      "KNN CV Accuracy: 0.9985\n",
      "Evaluating RF...\n",
      "RF CV Accuracy: 0.9952\n",
      "Evaluating LGB...\n",
      "LGB CV Accuracy: 0.9993\n",
      "Evaluating XGB...\n",
      "XGB CV Accuracy: 0.9988\n",
      "Evaluating NB...\n",
      "NB CV Accuracy: 0.9480\n",
      "Evaluating AB...\n",
      "AB CV Accuracy: 0.9989\n",
      "Initial Ensemble: ['LGB', 'AB'] - Train Accuracy: 0.9993\n",
      "Ignored XGB - No improvement: 0.9993 (Patience: 1/3)\n",
      "Added KNN - Improved Train Accuracy: 0.9996\n",
      "Ignored RF - No improvement: 0.9996 (Patience: 1/3)\n",
      "Ignored NB - No improvement: 0.9996 (Patience: 2/3)\n",
      "Final Ensemble Members: ['LGB', 'AB', 'KNN']\n",
      "Final Ensemble Train Accuracy: 0.99958825144112\n",
      "\n",
      "Retraining final models and evaluating on test set...\n",
      "Final Ensemble Test Accuracy: 1.0\n",
      "Training time: 4256.57 seconds\n"
     ]
    }
   ],
   "source": [
    "ensemble2, best_acc2, ensemble_weights2, test_acc2 = ieo_cvv_ensemble_selection(models, X_train, y_train, X_test, y_test)\n",
    "\n",
    "end_time1 = time.time()\n",
    "\n",
    "training_time1 = end_time1 - start_time1\n",
    "print(f\"Training time: {training_time1:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5877243-ca37-4278-b555-afcd4ff7d5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.29 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time3 = time.time()\n",
    "\n",
    "y_floor_predictions2 = ensemble_predict(ensemble2, X_target, ensemble_weights2)\n",
    "\n",
    "end_time3 = time.time()\n",
    "training_time3 = end_time3 - start_time3\n",
    "print(f\"Training time: {training_time3:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a39957ee-f019-439f-8b7f-a327bce2ce3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Floor using Ensemble Accuracy: 0.9355670103092784\n"
     ]
    }
   ],
   "source": [
    "accuracy_floor_ensemble2 = accuracy_score(y_target_floor_encoded, y_floor_predictions2)\n",
    "print(\"Floor using Ensemble Accuracy:\", accuracy_floor_ensemble2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8413c44-2c1d-4526-98e1-31185c099f93",
   "metadata": {},
   "source": [
    "# Location Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "51b6f48d-657d-47a5-8913-4f4d53bbab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances,make_scorer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53922b9e-880f-4157-8c0e-e7917874a180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data split ---\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y_coordinate, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "def mean_euclidean_distance(y_true, y_pred):\n",
    "    return np.mean(np.linalg.norm(y_pred - y_true, axis=1))\n",
    "\n",
    "euclidean_scorer = make_scorer(mean_euclidean_distance, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "080a81ea-7ffa-4b61-8257-2e4be904ef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import time\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "start_time2 = time.time()\n",
    "\n",
    "# =========================\n",
    "# Random Forest Regressor\n",
    "# =========================\n",
    "def objective_rfr(trial):\n",
    "    rf = RandomForestRegressor(\n",
    "        max_depth=trial.suggest_categorical('max_depth', [None] + list(range(5, 51))),\n",
    "        n_estimators=trial.suggest_int('n_estimators', 50, 500),\n",
    "        min_samples_split=trial.suggest_int('min_samples_split', 2, 10),\n",
    "        min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        max_features=trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, valid_idx in cv.split(X_train2):\n",
    "        X_tr, X_val = X_train2.iloc[train_idx], X_train2.iloc[valid_idx]\n",
    "        y_tr, y_val = y_train2.iloc[train_idx], y_train2.iloc[valid_idx]\n",
    "\n",
    "        rf.fit(X_tr, y_tr)\n",
    "        preds = rf.predict(X_val)\n",
    "        scores.append(np.mean(np.linalg.norm(preds - y_val.to_numpy(), axis=1)))\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# XGBoost Regressor\n",
    "# =========================\n",
    "def objective_xgbr(trial):\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=trial.suggest_int('n_estimators', 50, 200),\n",
    "        learning_rate=trial.suggest_float('learning_rate', 0.01, 0.5, log=True),\n",
    "        max_depth=trial.suggest_int('max_depth', 3, 10),\n",
    "        min_child_weight=trial.suggest_float('min_child_weight', 1, 10),\n",
    "        gamma=trial.suggest_float('gamma', 0, 5),\n",
    "        subsample=trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        colsample_bytree=trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    return cross_val_score(\n",
    "        model,\n",
    "        X_train2,\n",
    "        y_train2,\n",
    "        cv=10,\n",
    "        scoring=euclidean_scorer,\n",
    "        n_jobs=-1\n",
    "    ).mean()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# AdaBoost Regressor\n",
    "# =========================\n",
    "def objective_adaboost_r(trial):\n",
    "    base_estimator = DecisionTreeRegressor(\n",
    "        max_depth=trial.suggest_int('max_depth', 2, 10),\n",
    "        min_samples_split=trial.suggest_int('min_samples_split', 2, 10),\n",
    "        min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "    ada = AdaBoostRegressor(\n",
    "        estimator=base_estimator,\n",
    "        n_estimators=trial.suggest_int('n_estimators', 50, 300),\n",
    "        learning_rate=trial.suggest_float('learning_rate', 0.01, 1.0),\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "    model = MultiOutputRegressor(ada)\n",
    "\n",
    "    scores = cross_val_score(\n",
    "        model,\n",
    "        X_train2,\n",
    "        y_train2,\n",
    "        cv=cv,\n",
    "        scoring=euclidean_scorer,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    return -scores.mean()  # minimize Euclidean error\n",
    "\n",
    "\n",
    "# =========================\n",
    "# KNN Regressor\n",
    "# =========================\n",
    "def objective_knnr(trial):\n",
    "    knn = KNeighborsRegressor(\n",
    "        n_neighbors=trial.suggest_int('n_neighbors', 1, 50),\n",
    "        p=trial.suggest_int('p', 1, 5),\n",
    "        metric=trial.suggest_categorical('metric', ['minkowski', 'euclidean', 'manhattan']),\n",
    "        weights=knn_weight\n",
    "    )\n",
    "\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, valid_idx in cv.split(X_train2):\n",
    "        X_tr, X_val = X_train2.iloc[train_idx], X_train2.iloc[valid_idx]\n",
    "        y_tr, y_val = y_train2.iloc[train_idx], y_train2.iloc[valid_idx]\n",
    "\n",
    "        knn.fit(X_tr, y_tr)\n",
    "        preds = knn.predict(X_val)\n",
    "        scores.append(np.mean(np.linalg.norm(preds - y_val.to_numpy(), axis=1)))\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# LightGBM Regressor\n",
    "# =========================\n",
    "def euclidean_distance(y_true, y_pred):\n",
    "    return np.mean(np.linalg.norm(y_true - y_pred, axis=1))\n",
    "\n",
    "\n",
    "def objective_lgbr(trial):\n",
    "    base_model = lgb.LGBMRegressor(\n",
    "        n_estimators=trial.suggest_int('n_estimators', 50, 200),\n",
    "        learning_rate=trial.suggest_float('learning_rate', 0.01, 0.5, log=True),\n",
    "        num_leaves=trial.suggest_int('num_leaves', 20, 100),\n",
    "        max_depth=trial.suggest_int('max_depth', 3, 30),\n",
    "        min_child_samples=trial.suggest_int('min_child_samples', 5, 100),\n",
    "        random_state=42,\n",
    "        verbosity=-1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model = MultiOutputRegressor(base_model)\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, valid_idx in cv.split(X_train2):\n",
    "        X_tr, X_val = X_train2.iloc[train_idx], X_train2.iloc[valid_idx]\n",
    "        y_tr, y_val = y_train2.iloc[train_idx], y_train2.iloc[valid_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_val)\n",
    "        scores.append(euclidean_distance(y_val.to_numpy(), preds))\n",
    "\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8f8d7ffc-a68e-4e65-bac5-26f504f2bdbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-20 16:27:19,983] A new study created in memory with name: no-name-3cd33a93-f7e3-4d49-9c39-cf78ea3fe0d3\n",
      "[I 2025-12-20 16:27:23,054] Trial 0 finished with value: 0.267206898193753 and parameters: {'n_neighbors': 2, 'p': 1, 'metric': 'minkowski'}. Best is trial 0 with value: 0.267206898193753.\n",
      "[I 2025-12-20 16:27:24,260] Trial 1 finished with value: 0.45230791017025807 and parameters: {'n_neighbors': 39, 'p': 5, 'metric': 'euclidean'}. Best is trial 0 with value: 0.267206898193753.\n",
      "[I 2025-12-20 16:27:25,276] Trial 2 finished with value: 0.4221345589220147 and parameters: {'n_neighbors': 19, 'p': 3, 'metric': 'euclidean'}. Best is trial 0 with value: 0.267206898193753.\n",
      "[I 2025-12-20 16:27:28,412] Trial 3 finished with value: 0.42360414217850895 and parameters: {'n_neighbors': 50, 'p': 5, 'metric': 'manhattan'}. Best is trial 0 with value: 0.267206898193753.\n",
      "[I 2025-12-20 16:27:31,661] Trial 4 finished with value: 0.2720850617526024 and parameters: {'n_neighbors': 3, 'p': 1, 'metric': 'minkowski'}. Best is trial 0 with value: 0.267206898193753.\n",
      "[I 2025-12-20 16:27:32,635] Trial 5 finished with value: 0.3318632002467812 and parameters: {'n_neighbors': 3, 'p': 2, 'metric': 'euclidean'}. Best is trial 0 with value: 0.267206898193753.\n",
      "[I 2025-12-20 16:28:01,792] Trial 6 finished with value: 0.48161948688126993 and parameters: {'n_neighbors': 49, 'p': 3, 'metric': 'minkowski'}. Best is trial 0 with value: 0.267206898193753.\n",
      "[I 2025-12-20 16:28:31,258] Trial 7 finished with value: 0.4724014009513919 and parameters: {'n_neighbors': 30, 'p': 4, 'metric': 'minkowski'}. Best is trial 0 with value: 0.267206898193753.\n",
      "[I 2025-12-20 16:28:32,266] Trial 8 finished with value: 0.4148965724551893 and parameters: {'n_neighbors': 17, 'p': 2, 'metric': 'minkowski'}. Best is trial 0 with value: 0.267206898193753.\n",
      "[I 2025-12-20 16:28:35,435] Trial 9 finished with value: 0.41267682981469783 and parameters: {'n_neighbors': 44, 'p': 1, 'metric': 'minkowski'}. Best is trial 0 with value: 0.267206898193753.\n",
      "[I 2025-12-20 16:28:38,726] Trial 10 finished with value: 0.3330095466745039 and parameters: {'n_neighbors': 12, 'p': 2, 'metric': 'manhattan'}. Best is trial 0 with value: 0.267206898193753.\n",
      "[I 2025-12-20 16:28:41,824] Trial 11 finished with value: 0.2523712941531459 and parameters: {'n_neighbors': 1, 'p': 1, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:28:44,913] Trial 12 finished with value: 0.2523712941531459 and parameters: {'n_neighbors': 1, 'p': 1, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:28:48,179] Trial 13 finished with value: 0.3278592975791534 and parameters: {'n_neighbors': 11, 'p': 1, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:28:51,323] Trial 14 finished with value: 0.375338208066008 and parameters: {'n_neighbors': 27, 'p': 2, 'metric': 'manhattan'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:28:54,472] Trial 15 finished with value: 0.31572637853620805 and parameters: {'n_neighbors': 9, 'p': 1, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:29:23,774] Trial 16 finished with value: 0.4537941550676991 and parameters: {'n_neighbors': 20, 'p': 4, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:29:24,832] Trial 17 finished with value: 0.44208495973953454 and parameters: {'n_neighbors': 34, 'p': 2, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:29:27,928] Trial 18 finished with value: 0.3060258760209592 and parameters: {'n_neighbors': 7, 'p': 3, 'metric': 'manhattan'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:29:29,085] Trial 19 finished with value: 0.4071777803980493 and parameters: {'n_neighbors': 15, 'p': 1, 'metric': 'euclidean'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:29:30,090] Trial 20 finished with value: 0.31531662402508676 and parameters: {'n_neighbors': 1, 'p': 2, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:29:33,203] Trial 21 finished with value: 0.289924084075328 and parameters: {'n_neighbors': 5, 'p': 1, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:29:36,428] Trial 22 finished with value: 0.2523712941531459 and parameters: {'n_neighbors': 1, 'p': 1, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:29:39,543] Trial 23 finished with value: 0.3129794764050937 and parameters: {'n_neighbors': 8, 'p': 1, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:29:40,530] Trial 24 finished with value: 0.40314272562605613 and parameters: {'n_neighbors': 14, 'p': 2, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:29:43,810] Trial 25 finished with value: 0.37133079630543087 and parameters: {'n_neighbors': 24, 'p': 1, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:30:12,889] Trial 26 finished with value: 0.4156795008280335 and parameters: {'n_neighbors': 6, 'p': 4, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:30:13,875] Trial 27 finished with value: 0.38533938552375196 and parameters: {'n_neighbors': 11, 'p': 2, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:30:17,128] Trial 28 finished with value: 0.29946806938441245 and parameters: {'n_neighbors': 6, 'p': 1, 'metric': 'manhattan'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:30:18,116] Trial 29 finished with value: 0.31531662402508676 and parameters: {'n_neighbors': 1, 'p': 3, 'metric': 'euclidean'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:30:21,212] Trial 30 finished with value: 0.2523712941531459 and parameters: {'n_neighbors': 1, 'p': 1, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:30:24,446] Trial 31 finished with value: 0.2523712941531459 and parameters: {'n_neighbors': 1, 'p': 1, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:30:27,544] Trial 32 finished with value: 0.2824321777999227 and parameters: {'n_neighbors': 4, 'p': 1, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:30:30,621] Trial 33 finished with value: 0.31572637853620805 and parameters: {'n_neighbors': 9, 'p': 1, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:30:31,643] Trial 34 finished with value: 0.3441878984473198 and parameters: {'n_neighbors': 4, 'p': 1, 'metric': 'euclidean'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:30:32,786] Trial 35 finished with value: 0.31531662402508676 and parameters: {'n_neighbors': 1, 'p': 2, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:31:01,965] Trial 36 finished with value: 0.4263256942818609 and parameters: {'n_neighbors': 5, 'p': 5, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:31:03,178] Trial 37 finished with value: 0.45230791017025807 and parameters: {'n_neighbors': 39, 'p': 1, 'metric': 'euclidean'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:31:06,274] Trial 38 finished with value: 0.362936072696794 and parameters: {'n_neighbors': 20, 'p': 1, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:31:07,272] Trial 39 finished with value: 0.3318632002467812 and parameters: {'n_neighbors': 3, 'p': 2, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:31:10,568] Trial 40 finished with value: 0.31572637853620805 and parameters: {'n_neighbors': 9, 'p': 3, 'metric': 'manhattan'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:31:13,664] Trial 41 finished with value: 0.2523712941531459 and parameters: {'n_neighbors': 1, 'p': 1, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:31:16,722] Trial 42 finished with value: 0.2720850617526024 and parameters: {'n_neighbors': 3, 'p': 1, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:31:19,777] Trial 43 finished with value: 0.3060258760209592 and parameters: {'n_neighbors': 7, 'p': 1, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:31:23,018] Trial 44 finished with value: 0.2720850617526024 and parameters: {'n_neighbors': 3, 'p': 1, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:31:24,007] Trial 45 finished with value: 0.3969585750005775 and parameters: {'n_neighbors': 13, 'p': 2, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:31:27,287] Trial 46 finished with value: 0.3555423387454387 and parameters: {'n_neighbors': 17, 'p': 1, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:31:28,255] Trial 47 finished with value: 0.38028489946107485 and parameters: {'n_neighbors': 10, 'p': 2, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:31:31,324] Trial 48 finished with value: 0.2523712941531459 and parameters: {'n_neighbors': 1, 'p': 1, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:31:34,562] Trial 49 finished with value: 0.29946806938441245 and parameters: {'n_neighbors': 6, 'p': 4, 'metric': 'manhattan'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:31:35,552] Trial 50 finished with value: 0.3318632002467812 and parameters: {'n_neighbors': 3, 'p': 2, 'metric': 'euclidean'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:31:38,625] Trial 51 finished with value: 0.2523712941531459 and parameters: {'n_neighbors': 1, 'p': 1, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:31:41,709] Trial 52 finished with value: 0.289924084075328 and parameters: {'n_neighbors': 5, 'p': 1, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:31:44,964] Trial 53 finished with value: 0.2523712941531459 and parameters: {'n_neighbors': 1, 'p': 1, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:31:48,057] Trial 54 finished with value: 0.3129794764050937 and parameters: {'n_neighbors': 8, 'p': 1, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:31:51,275] Trial 55 finished with value: 0.2720850617526024 and parameters: {'n_neighbors': 3, 'p': 1, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n",
      "[I 2025-12-20 16:32:20,392] Trial 56 finished with value: 0.4954063160855207 and parameters: {'n_neighbors': 36, 'p': 5, 'metric': 'minkowski'}. Best is trial 11 with value: 0.2523712941531459.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best KNNR hyperparameters: {'n_neighbors': 1, 'p': 1, 'metric': 'minkowski'}\n",
      "Cross-val error (per fold): [0.24501568 0.14689097 0.34607254 0.30549476 0.17582366 0.22103972\n",
      " 0.24588436 0.31520037 0.26053832 0.26175256]\n",
      "Mean Cross-val error: 0.2523712941531459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['knn_coordinate_pred_UTS_reg.joblib']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optuna.pruners import MedianPruner\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import joblib\n",
    "\n",
    "SEED = 42\n",
    "pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=0)\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "\n",
    "# --- KNNR ---\n",
    "study1 = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "study1.optimize(objective_knnr, n_trials=100, timeout=300)\n",
    "\n",
    "best_params1r = study1.best_params\n",
    "print(\"Best KNNR hyperparameters:\", best_params1r)\n",
    "\n",
    "knn_coordinate_final = KNeighborsRegressor(\n",
    "    n_neighbors=best_params1r['n_neighbors'],\n",
    "    weights=knn_weight,\n",
    "    p=best_params1r['p'],\n",
    "    metric=best_params1r['metric']\n",
    ")\n",
    "\n",
    "cv_scores_knn = cross_val_score(knn_coordinate_final, X_train2, y_train2,\n",
    "                                cv=cv, scoring=euclidean_scorer, n_jobs=-1)\n",
    "\n",
    "print(\"Cross-val error (per fold):\", -cv_scores_knn)\n",
    "print(\"Mean Cross-val error:\", -cv_scores_knn.mean())\n",
    "\n",
    "knn_coordinate_final.fit(X_train2, y_train2)\n",
    "joblib.dump(knn_coordinate_final, 'knn_coordinate_pred_UTS_reg.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "08dce246-73af-4b50-b6d7-515cc68a0ff1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-20 16:32:25,223] A new study created in memory with name: no-name-5ecc0301-fb33-4564-85bf-539ac529a209\n",
      "[I 2025-12-20 16:32:29,944] Trial 0 finished with value: 2.746654060282169 and parameters: {'max_depth': 24, 'n_estimators': 224, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 0 with value: 2.746654060282169.\n",
      "[I 2025-12-20 16:32:35,935] Trial 1 finished with value: 8.856840046331959 and parameters: {'max_depth': 8, 'n_estimators': 328, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 0 with value: 2.746654060282169.\n",
      "[I 2025-12-20 16:32:42,655] Trial 2 finished with value: 5.938695744798266 and parameters: {'max_depth': 31, 'n_estimators': 393, 'min_samples_split': 10, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 0 with value: 2.746654060282169.\n",
      "[I 2025-12-20 16:32:47,762] Trial 3 finished with value: 1.8321254856585694 and parameters: {'max_depth': 36, 'n_estimators': 223, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 3 with value: 1.8321254856585694.\n",
      "[I 2025-12-20 16:33:36,100] Trial 4 finished with value: 2.8001101889154727 and parameters: {'max_depth': 39, 'n_estimators': 452, 'min_samples_split': 9, 'min_samples_leaf': 8, 'max_features': None}. Best is trial 3 with value: 1.8321254856585694.\n",
      "[I 2025-12-20 16:33:40,324] Trial 5 finished with value: 5.412406207464436 and parameters: {'max_depth': 49, 'n_estimators': 212, 'min_samples_split': 4, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 3 with value: 1.8321254856585694.\n",
      "[I 2025-12-20 16:33:45,781] Trial 6 finished with value: 1.5622549646599388 and parameters: {'max_depth': 37, 'n_estimators': 250, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 6 with value: 1.5622549646599388.\n",
      "[I 2025-12-20 16:33:52,236] Trial 7 finished with value: 5.061384715998724 and parameters: {'max_depth': 19, 'n_estimators': 339, 'min_samples_split': 8, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 6 with value: 1.5622549646599388.\n",
      "[I 2025-12-20 16:34:13,524] Trial 8 finished with value: 6.085473544438977 and parameters: {'max_depth': 11, 'n_estimators': 242, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 6 with value: 1.5622549646599388.\n",
      "[I 2025-12-20 16:34:18,592] Trial 9 finished with value: 6.6299568840376395 and parameters: {'max_depth': 13, 'n_estimators': 252, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 6 with value: 1.5622549646599388.\n",
      "[I 2025-12-20 16:34:20,750] Trial 10 finished with value: 4.227991232912112 and parameters: {'max_depth': 37, 'n_estimators': 72, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 6 with value: 1.5622549646599388.\n",
      "[I 2025-12-20 16:34:24,085] Trial 11 finished with value: 2.263990526214444 and parameters: {'max_depth': 37, 'n_estimators': 119, 'min_samples_split': 6, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 6 with value: 1.5622549646599388.\n",
      "[I 2025-12-20 16:34:27,784] Trial 12 finished with value: 5.081524931511288 and parameters: {'max_depth': 14, 'n_estimators': 162, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 6 with value: 1.5622549646599388.\n",
      "[I 2025-12-20 16:34:34,507] Trial 13 finished with value: 2.258164428671212 and parameters: {'max_depth': 36, 'n_estimators': 313, 'min_samples_split': 6, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 6 with value: 1.5622549646599388.\n",
      "[I 2025-12-20 16:34:38,733] Trial 14 finished with value: 1.2010047780043986 and parameters: {'max_depth': 38, 'n_estimators': 157, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:34:56,368] Trial 15 finished with value: 2.061491751754537 and parameters: {'max_depth': 38, 'n_estimators': 143, 'min_samples_split': 3, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:34:58,336] Trial 16 finished with value: 9.858078067074675 and parameters: {'max_depth': 7, 'n_estimators': 69, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:35:02,599] Trial 17 finished with value: 2.6049417186458177 and parameters: {'max_depth': 29, 'n_estimators': 174, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:35:33,761] Trial 18 finished with value: 2.746065648282949 and parameters: {'max_depth': 23, 'n_estimators': 282, 'min_samples_split': 5, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:35:41,984] Trial 19 finished with value: 9.031896915749648 and parameters: {'max_depth': 10, 'n_estimators': 496, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:35:49,121] Trial 20 finished with value: 4.547179362094196 and parameters: {'max_depth': 17, 'n_estimators': 383, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:35:53,764] Trial 21 finished with value: 1.8614750168207135 and parameters: {'max_depth': 34, 'n_estimators': 193, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:35:57,177] Trial 22 finished with value: 1.2914149946642102 and parameters: {'max_depth': 43, 'n_estimators': 119, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:36:00,105] Trial 23 finished with value: 1.5524647190935097 and parameters: {'max_depth': 43, 'n_estimators': 108, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:36:02,987] Trial 24 finished with value: 1.558787185808208 and parameters: {'max_depth': 43, 'n_estimators': 96, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:36:04,834] Trial 25 finished with value: 2.9429328498932934 and parameters: {'max_depth': 32, 'n_estimators': 50, 'min_samples_split': 8, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:36:08,199] Trial 26 finished with value: 2.0468551443647742 and parameters: {'max_depth': 43, 'n_estimators': 127, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:36:11,089] Trial 27 finished with value: 2.1233518325732215 and parameters: {'max_depth': 43, 'n_estimators': 103, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:36:28,397] Trial 28 finished with value: 3.2641891147678344 and parameters: {'max_depth': 38, 'n_estimators': 151, 'min_samples_split': 3, 'min_samples_leaf': 10, 'max_features': None}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:36:32,104] Trial 29 finished with value: 3.776524091143517 and parameters: {'max_depth': 22, 'n_estimators': 176, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:36:34,862] Trial 30 finished with value: 3.5887807617958374 and parameters: {'max_depth': 18, 'n_estimators': 89, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:36:37,596] Trial 31 finished with value: 1.558554663388939 and parameters: {'max_depth': 43, 'n_estimators': 93, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:36:40,835] Trial 32 finished with value: 2.3383344642358783 and parameters: {'max_depth': 30, 'n_estimators': 123, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:36:43,405] Trial 33 finished with value: 1.6090680249632006 and parameters: {'max_depth': 48, 'n_estimators': 83, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:36:46,456] Trial 34 finished with value: 8.11188639944372 and parameters: {'max_depth': 9, 'n_estimators': 138, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:36:48,538] Trial 35 finished with value: 2.249295503381348 and parameters: {'max_depth': 27, 'n_estimators': 50, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:36:52,589] Trial 36 finished with value: 6.220618334941039 and parameters: {'max_depth': 12, 'n_estimators': 199, 'min_samples_split': 9, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:36:56,498] Trial 37 finished with value: 1.8203467738292751 and parameters: {'max_depth': 44, 'n_estimators': 175, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:36:59,553] Trial 38 finished with value: 1.607343673009743 and parameters: {'max_depth': 46, 'n_estimators': 110, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:37:04,223] Trial 39 finished with value: 4.238584390912862 and parameters: {'max_depth': 25, 'n_estimators': 223, 'min_samples_split': 7, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:37:21,982] Trial 40 finished with value: 3.3011304615061534 and parameters: {'max_depth': 16, 'n_estimators': 157, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:37:24,887] Trial 41 finished with value: 1.558787185808208 and parameters: {'max_depth': 43, 'n_estimators': 96, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 14 with value: 1.2010047780043986.\n",
      "[I 2025-12-20 16:37:27,308] Trial 42 finished with value: 1.5207740487622252 and parameters: {'max_depth': 45, 'n_estimators': 73, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 14 with value: 1.2010047780043986.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RFR hyperparameters: {'max_depth': 38, 'n_estimators': 157, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Cross-val error (per fold): [1.01924083 1.21446746 1.12743365 1.28361364 1.19506792 1.01025292\n",
      " 1.09495134 1.32201709 1.39824953 1.3447534 ]\n",
      "Mean Cross-val error: 1.2010047780043989\n"
     ]
    }
   ],
   "source": [
    "# --- RFR ---\n",
    "study2 = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "study2.optimize(objective_rfr, n_trials=100, timeout=300)\n",
    "\n",
    "best_params2r = study2.best_params\n",
    "print(\"Best RFR hyperparameters:\", best_params2r)\n",
    "\n",
    "rf_coordinate_final = RandomForestRegressor(**best_params2r, random_state=SEED, n_jobs=-1)\n",
    "\n",
    "cv_scores_rf = cross_val_score(rf_coordinate_final, X_train2, y_train2,\n",
    "                               cv=cv, scoring=euclidean_scorer, n_jobs=-1)\n",
    "\n",
    "print(\"Cross-val error (per fold):\", -cv_scores_rf)\n",
    "print(\"Mean Cross-val error:\", -cv_scores_rf.mean())\n",
    "\n",
    "rf_coordinate_final.fit(X_train2, y_train2)\n",
    "joblib.dump(rf_coordinate_final, 'rf_coordinate_pred_UTS_reg.joblib')\n",
    "rf_loaded2 = joblib.load('rf_coordinate_pred_UTS_reg.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "427e4f0a-3f09-4595-ad65-33c757bf1806",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-20 16:37:32,781] A new study created in memory with name: no-name-9b751cba-b1d6-4df1-afec-055c75a74f2c\n",
      "[I 2025-12-20 16:37:49,260] Trial 0 finished with value: -1.4093385272468595 and parameters: {'n_estimators': 156, 'learning_rate': 0.18933128911643998, 'max_depth': 9, 'min_child_weight': 8.824211293123701, 'gamma': 0.37003921779048266, 'subsample': 0.6319779856630245, 'colsample_bytree': 0.5648132054696995}. Best is trial 0 with value: -1.4093385272468595.\n",
      "[I 2025-12-20 16:38:07,361] Trial 1 finished with value: -6.305468626270587 and parameters: {'n_estimators': 89, 'learning_rate': 0.028424015963969707, 'max_depth': 8, 'min_child_weight': 9.811770284597918, 'gamma': 3.4542172391398145, 'subsample': 0.597858253603539, 'colsample_bytree': 0.8373811363051493}. Best is trial 0 with value: -1.4093385272468595.\n",
      "[I 2025-12-20 16:38:16,413] Trial 2 finished with value: -1.1689885924068002 and parameters: {'n_estimators': 138, 'learning_rate': 0.4813031213155698, 'max_depth': 8, 'min_child_weight': 9.831598122628309, 'gamma': 4.192824817395093, 'subsample': 0.9491975091534064, 'colsample_bytree': 0.7469457490166338}. Best is trial 2 with value: -1.1689885924068002.\n",
      "[I 2025-12-20 16:38:23,073] Trial 3 finished with value: -9.044140577162366 and parameters: {'n_estimators': 199, 'learning_rate': 0.020807091026409957, 'max_depth': 3, 'min_child_weight': 1.9107963888900614, 'gamma': 0.9426974889418915, 'subsample': 0.8859247896822693, 'colsample_bytree': 0.6864990272204914}. Best is trial 2 with value: -1.1689885924068002.\n",
      "[I 2025-12-20 16:38:28,549] Trial 4 finished with value: -5.8931087468706185 and parameters: {'n_estimators': 92, 'learning_rate': 0.06045313880410634, 'max_depth': 5, 'min_child_weight': 9.571297501012191, 'gamma': 4.570760720052436, 'subsample': 0.6442657971337192, 'colsample_bytree': 0.6181761969100344}. Best is trial 2 with value: -1.1689885924068002.\n",
      "[I 2025-12-20 16:38:51,300] Trial 5 finished with value: -5.508387016847824 and parameters: {'n_estimators': 165, 'learning_rate': 0.01976172602720322, 'max_depth': 7, 'min_child_weight': 7.370100830081716, 'gamma': 1.0995998664465407, 'subsample': 0.8117458447927046, 'colsample_bytree': 0.5605840483983148}. Best is trial 2 with value: -1.1689885924068002.\n",
      "[I 2025-12-20 16:39:53,037] Trial 6 finished with value: -7.603486947138158 and parameters: {'n_estimators': 169, 'learning_rate': 0.011550486958259859, 'max_depth': 10, 'min_child_weight': 7.070909814073766, 'gamma': 2.922438591953149, 'subsample': 0.8661794142832209, 'colsample_bytree': 0.9244159214292377}. Best is trial 2 with value: -1.1689885924068002.\n",
      "[I 2025-12-20 16:39:59,640] Trial 7 finished with value: -3.259238644019156 and parameters: {'n_estimators': 82, 'learning_rate': 0.17337530458861417, 'max_depth': 6, 'min_child_weight': 2.0540926805912543, 'gamma': 0.32691067839487165, 'subsample': 0.5102050824094881, 'colsample_bytree': 0.6336381092863574}. Best is trial 2 with value: -1.1689885924068002.\n",
      "[I 2025-12-20 16:40:21,293] Trial 8 finished with value: -3.4917206756817465 and parameters: {'n_estimators': 143, 'learning_rate': 0.04518381018898965, 'max_depth': 8, 'min_child_weight': 2.4367631926462154, 'gamma': 1.655916800359265, 'subsample': 0.5172108922817579, 'colsample_bytree': 0.8693877770100193}. Best is trial 2 with value: -1.1689885924068002.\n",
      "[I 2025-12-20 16:40:25,567] Trial 9 finished with value: -8.521591845758122 and parameters: {'n_estimators': 104, 'learning_rate': 0.04912680745582337, 'max_depth': 3, 'min_child_weight': 1.9041240519091929, 'gamma': 2.6871710112321505, 'subsample': 0.6496257843711399, 'colsample_bytree': 0.6692283344200209}. Best is trial 2 with value: -1.1689885924068002.\n",
      "[I 2025-12-20 16:40:31,907] Trial 10 finished with value: -1.4780820732722013 and parameters: {'n_estimators': 120, 'learning_rate': 0.48594175230662623, 'max_depth': 5, 'min_child_weight': 4.417206274277765, 'gamma': 4.977900516400046, 'subsample': 0.9951821884337873, 'colsample_bytree': 0.7877153383886143}. Best is trial 2 with value: -1.1689885924068002.\n",
      "[I 2025-12-20 16:40:43,463] Trial 11 finished with value: -1.139934938493688 and parameters: {'n_estimators': 144, 'learning_rate': 0.40467622978577356, 'max_depth': 10, 'min_child_weight': 8.229430265754264, 'gamma': 3.910649465461818, 'subsample': 0.7243201008057801, 'colsample_bytree': 0.5005770009647446}. Best is trial 11 with value: -1.139934938493688.\n",
      "[I 2025-12-20 16:40:53,841] Trial 12 finished with value: -1.094971813672067 and parameters: {'n_estimators': 132, 'learning_rate': 0.4977495685961008, 'max_depth': 10, 'min_child_weight': 7.605012706977807, 'gamma': 3.833464077283867, 'subsample': 0.7373830544296397, 'colsample_bytree': 0.505497351375213}. Best is trial 12 with value: -1.094971813672067.\n",
      "[I 2025-12-20 16:41:05,001] Trial 13 finished with value: -1.9054872464077903 and parameters: {'n_estimators': 63, 'learning_rate': 0.21714284285198915, 'max_depth': 10, 'min_child_weight': 7.122535107233587, 'gamma': 3.659293229703964, 'subsample': 0.7374459519857627, 'colsample_bytree': 0.5060939249332235}. Best is trial 12 with value: -1.094971813672067.\n",
      "[I 2025-12-20 16:41:18,487] Trial 14 finished with value: -1.1090347863485022 and parameters: {'n_estimators': 121, 'learning_rate': 0.27844763287446866, 'max_depth': 10, 'min_child_weight': 5.416517569926572, 'gamma': 3.6775911437743805, 'subsample': 0.7393823307537857, 'colsample_bytree': 0.5027137065808286}. Best is trial 12 with value: -1.094971813672067.\n",
      "[I 2025-12-20 16:41:37,001] Trial 15 finished with value: -1.907187354017212 and parameters: {'n_estimators': 117, 'learning_rate': 0.11134414172771537, 'max_depth': 9, 'min_child_weight': 4.6842405254830854, 'gamma': 2.041663995056592, 'subsample': 0.7636219013532564, 'colsample_bytree': 0.5590058293665162}. Best is trial 12 with value: -1.094971813672067.\n",
      "[I 2025-12-20 16:41:51,481] Trial 16 finished with value: -1.0092459557758606 and parameters: {'n_estimators': 186, 'learning_rate': 0.2902872522417269, 'max_depth': 9, 'min_child_weight': 5.805990965518721, 'gamma': 3.168158101550201, 'subsample': 0.8135742037172753, 'colsample_bytree': 0.7190174677854706}. Best is trial 16 with value: -1.0092459557758606.\n",
      "[I 2025-12-20 16:42:13,709] Trial 17 finished with value: -1.2016398290613592 and parameters: {'n_estimators': 197, 'learning_rate': 0.13192551136339267, 'max_depth': 9, 'min_child_weight': 6.173366953629228, 'gamma': 2.862874184032574, 'subsample': 0.8285161142994242, 'colsample_bytree': 0.7338604395611313}. Best is trial 16 with value: -1.0092459557758606.\n",
      "[I 2025-12-20 16:42:26,151] Trial 18 finished with value: -1.0364647675026857 and parameters: {'n_estimators': 181, 'learning_rate': 0.300800852146749, 'max_depth': 7, 'min_child_weight': 3.297977700041811, 'gamma': 3.011251266779715, 'subsample': 0.6947317298679779, 'colsample_bytree': 0.9244120955660651}. Best is trial 16 with value: -1.0092459557758606.\n",
      "[I 2025-12-20 16:42:42,940] Trial 19 finished with value: -2.037726122786367 and parameters: {'n_estimators': 180, 'learning_rate': 0.10629587807730896, 'max_depth': 7, 'min_child_weight': 3.2122228467224283, 'gamma': 2.3082490793158956, 'subsample': 0.6827943362302693, 'colsample_bytree': 0.962193088166506}. Best is trial 16 with value: -1.0092459557758606.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBR hyperparameters: {'n_estimators': 186, 'learning_rate': 0.2902872522417269, 'max_depth': 9, 'min_child_weight': 5.805990965518721, 'gamma': 3.168158101550201, 'subsample': 0.8135742037172753, 'colsample_bytree': 0.7190174677854706}\n",
      "Cross-val error (per fold): [1.03836045 1.06411674 1.06171722 0.78789342 0.98129221 1.01472623\n",
      " 0.9671666  1.00298203 1.09698017 1.07722449]\n",
      "Mean Cross-val error: 1.0092459557758606\n"
     ]
    }
   ],
   "source": [
    "# --- XGBR ---\n",
    "study3 = optuna.create_study(direction='maximize')\n",
    "study3.optimize(objective_xgbr, n_trials=100, timeout=300)\n",
    "\n",
    "best_params3r = study3.best_params\n",
    "print(\"Best XGBR hyperparameters:\", best_params3r)\n",
    "\n",
    "xgb_coordinate_final = XGBRegressor(**best_params3r, random_state=42, n_jobs=-1)\n",
    "\n",
    "cv_scores_xgb = cross_val_score(xgb_coordinate_final, X_train2, y_train2,\n",
    "                                cv=10, scoring=euclidean_scorer, n_jobs=-1)\n",
    "\n",
    "print(\"Cross-val error (per fold):\", -cv_scores_xgb)\n",
    "print(\"Mean Cross-val error:\", -cv_scores_xgb.mean())\n",
    "\n",
    "xgb_coordinate_final.fit(X_train2, y_train2)\n",
    "joblib.dump(xgb_coordinate_final, 'xgb_coordinate_pred_UTS_reg.joblib')\n",
    "xgb_loaded2 = joblib.load('xgb_coordinate_pred_UTS_reg.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0fee3cee-a6dd-4932-a947-8a139d444ff2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-20 16:43:01,357] A new study created in memory with name: no-name-64512313-0685-4347-a241-a3dd6d1cad88\n",
      "[I 2025-12-20 16:43:29,589] Trial 0 finished with value: 6.451988081457669 and parameters: {'n_estimators': 180, 'learning_rate': 0.01092351165681506, 'num_leaves': 64, 'max_depth': 17, 'min_child_samples': 69}. Best is trial 0 with value: 6.451988081457669.\n",
      "[I 2025-12-20 16:43:59,271] Trial 1 finished with value: 2.5452136282510898 and parameters: {'n_estimators': 160, 'learning_rate': 0.03271005603228032, 'num_leaves': 70, 'max_depth': 20, 'min_child_samples': 33}. Best is trial 1 with value: 2.5452136282510898.\n",
      "[I 2025-12-20 16:44:07,643] Trial 2 finished with value: 3.8014677490621125 and parameters: {'n_estimators': 115, 'learning_rate': 0.1769748068191341, 'num_leaves': 76, 'max_depth': 6, 'min_child_samples': 60}. Best is trial 1 with value: 2.5452136282510898.\n",
      "[I 2025-12-20 16:44:39,543] Trial 3 finished with value: 6.103618320820301 and parameters: {'n_estimators': 163, 'learning_rate': 0.01117731280877312, 'num_leaves': 67, 'max_depth': 17, 'min_child_samples': 28}. Best is trial 1 with value: 2.5452136282510898.\n",
      "[I 2025-12-20 16:44:46,768] Trial 4 finished with value: 3.773867785632418 and parameters: {'n_estimators': 72, 'learning_rate': 0.24290879731029416, 'num_leaves': 90, 'max_depth': 9, 'min_child_samples': 96}. Best is trial 1 with value: 2.5452136282510898.\n",
      "[I 2025-12-20 16:44:53,126] Trial 5 finished with value: 7.902855139073033 and parameters: {'n_estimators': 113, 'learning_rate': 0.06302410501412216, 'num_leaves': 44, 'max_depth': 3, 'min_child_samples': 62}. Best is trial 1 with value: 2.5452136282510898.\n",
      "[I 2025-12-20 16:45:00,430] Trial 6 finished with value: 5.988354606001893 and parameters: {'n_estimators': 50, 'learning_rate': 0.061091682079648844, 'num_leaves': 28, 'max_depth': 13, 'min_child_samples': 90}. Best is trial 1 with value: 2.5452136282510898.\n",
      "[I 2025-12-20 16:45:15,974] Trial 7 finished with value: 1.9549395492370363 and parameters: {'n_estimators': 57, 'learning_rate': 0.09018256644482855, 'num_leaves': 88, 'max_depth': 16, 'min_child_samples': 7}. Best is trial 7 with value: 1.9549395492370363.\n",
      "[I 2025-12-20 16:45:35,350] Trial 8 finished with value: 3.1563343372749233 and parameters: {'n_estimators': 192, 'learning_rate': 0.06543185448794657, 'num_leaves': 51, 'max_depth': 14, 'min_child_samples': 91}. Best is trial 7 with value: 1.9549395492370363.\n",
      "[I 2025-12-20 16:45:52,960] Trial 9 finished with value: 0.9067193754395259 and parameters: {'n_estimators': 103, 'learning_rate': 0.48767065410278754, 'num_leaves': 77, 'max_depth': 28, 'min_child_samples': 66}. Best is trial 9 with value: 0.9067193754395259.\n",
      "[I 2025-12-20 16:46:03,455] Trial 10 finished with value: 1.3283094547374215 and parameters: {'n_estimators': 91, 'learning_rate': 0.479463194072149, 'num_leaves': 26, 'max_depth': 30, 'min_child_samples': 41}. Best is trial 9 with value: 0.9067193754395259.\n",
      "[I 2025-12-20 16:46:12,332] Trial 11 finished with value: 1.9993064249218442 and parameters: {'n_estimators': 87, 'learning_rate': 0.35606048210218544, 'num_leaves': 20, 'max_depth': 30, 'min_child_samples': 42}. Best is trial 9 with value: 0.9067193754395259.\n",
      "[I 2025-12-20 16:46:27,015] Trial 12 finished with value: 1.1830272650529747 and parameters: {'n_estimators': 96, 'learning_rate': 0.4745375495921571, 'num_leaves': 38, 'max_depth': 30, 'min_child_samples': 75}. Best is trial 9 with value: 0.9067193754395259.\n",
      "[I 2025-12-20 16:46:45,150] Trial 13 finished with value: 1.6370847224220118 and parameters: {'n_estimators': 135, 'learning_rate': 0.17226869617738427, 'num_leaves': 43, 'max_depth': 25, 'min_child_samples': 76}. Best is trial 9 with value: 0.9067193754395259.\n",
      "[I 2025-12-20 16:47:05,361] Trial 14 finished with value: 0.8831085737054025 and parameters: {'n_estimators': 134, 'learning_rate': 0.46650581464727847, 'num_leaves': 100, 'max_depth': 25, 'min_child_samples': 78}. Best is trial 14 with value: 0.8831085737054025.\n",
      "[I 2025-12-20 16:47:25,054] Trial 15 finished with value: 1.3178930326355378 and parameters: {'n_estimators': 135, 'learning_rate': 0.24600831410695922, 'num_leaves': 100, 'max_depth': 23, 'min_child_samples': 81}. Best is trial 14 with value: 0.8831085737054025.\n",
      "[I 2025-12-20 16:47:49,856] Trial 16 finished with value: 1.4288982120618514 and parameters: {'n_estimators': 151, 'learning_rate': 0.12113894557291681, 'num_leaves': 80, 'max_depth': 25, 'min_child_samples': 55}. Best is trial 14 with value: 0.8831085737054025.\n",
      "[I 2025-12-20 16:48:06,910] Trial 17 finished with value: 1.5706010600148228 and parameters: {'n_estimators': 112, 'learning_rate': 0.30246036441162294, 'num_leaves': 96, 'max_depth': 26, 'min_child_samples': 100}. Best is trial 14 with value: 0.8831085737054025.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LGBR hyperparameters: {'n_estimators': 134, 'learning_rate': 0.46650581464727847, 'num_leaves': 100, 'max_depth': 25, 'min_child_samples': 78}\n",
      "Cross-val error (per fold): [0.80191406 0.81875631 0.94344242 0.91542193 0.81713743 0.77426065\n",
      " 0.79881318 0.91995399 1.07865002 0.96273575]\n",
      "Mean Cross-val error: 0.8831085737054025\n"
     ]
    }
   ],
   "source": [
    "# --- LGBR ---\n",
    "study5 = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "study5.optimize(objective_lgbr, n_trials=100, timeout=300)\n",
    "\n",
    "best_params5r = study5.best_params\n",
    "print(\"Best LGBR hyperparameters:\", best_params5r)\n",
    "\n",
    "# Train final model using best params\n",
    "base_model_final = lgb.LGBMRegressor(**best_params5r, random_state=SEED, n_jobs=-1)\n",
    "lgb_coordinate_final = MultiOutputRegressor(base_model_final)\n",
    "\n",
    "# Cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=SEED) \n",
    "cv_scores_lgbm = cross_val_score(\n",
    "    lgb_coordinate_final, X_train2, y_train2,\n",
    "    cv=cv, scoring=euclidean_scorer, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Positive error conversion\n",
    "cv_errors_lgbm = -cv_scores_lgbm\n",
    "\n",
    "# Print results\n",
    "print(\"Cross-val error (per fold):\", cv_errors_lgbm)\n",
    "print(\"Mean Cross-val error:\", cv_errors_lgbm.mean())\n",
    "\n",
    "# Train on full training set\n",
    "lgb_coordinate_final.fit(X_train2, y_train2)\n",
    "\n",
    "# Save and reload\n",
    "joblib.dump(lgb_coordinate_final, 'multioutput_lgbm_regressor.joblib')\n",
    "multi_model_final = joblib.load('multioutput_lgbm_regressor.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ced8b907-5ba8-4573-bd0c-c29a54ac1237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-20 16:48:24,079] A new study created in memory with name: no-name-73cd811d-200a-4010-9088-940423a3eb73\n",
      "[I 2025-12-20 16:50:20,331] Trial 0 finished with value: 10.645662557715621 and parameters: {'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 10, 'n_estimators': 227, 'learning_rate': 0.25033487095378704}. Best is trial 0 with value: 10.645662557715621.\n",
      "[I 2025-12-20 16:51:31,387] Trial 1 finished with value: 13.052347940778205 and parameters: {'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 7, 'n_estimators': 226, 'learning_rate': 0.12664327393735691}. Best is trial 0 with value: 10.645662557715621.\n",
      "[I 2025-12-20 16:51:45,829] Trial 2 finished with value: 15.369926241279137 and parameters: {'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 4, 'n_estimators': 88, 'learning_rate': 0.7189770070176749}. Best is trial 0 with value: 10.645662557715621.\n",
      "[I 2025-12-20 16:52:49,797] Trial 3 finished with value: 9.076682938088279 and parameters: {'max_depth': 6, 'min_samples_split': 8, 'min_samples_leaf': 6, 'n_estimators': 78, 'learning_rate': 0.5080317499566265}. Best is trial 3 with value: 9.076682938088279.\n",
      "[I 2025-12-20 16:56:42,663] Trial 4 finished with value: 7.45693278738447 and parameters: {'max_depth': 8, 'min_samples_split': 8, 'min_samples_leaf': 1, 'n_estimators': 242, 'learning_rate': 0.48348056951229107}. Best is trial 4 with value: 7.45693278738447.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AdaBoost Regressor hyperparameters: {'max_depth': 8, 'min_samples_split': 8, 'min_samples_leaf': 1, 'n_estimators': 242, 'learning_rate': 0.48348056951229107}\n",
      "AdaBoost Cross-val error (per fold): [7.28061067 7.65687322 7.60353048 7.52242181 7.01543667 7.08163751\n",
      " 7.5561937  7.54419785 7.66132005 7.64710591]\n",
      "AdaBoost Mean Cross-val error: 7.45693278738447\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import joblib\n",
    "\n",
    "# --- AdaBoost Regressor Optimization ---\n",
    "study_ab_r = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "study_ab_r.optimize(objective_adaboost_r, n_trials=50, timeout=300)\n",
    "\n",
    "best_params_ab_r = study_ab_r.best_params\n",
    "print(\"Best AdaBoost Regressor hyperparameters:\", best_params_ab_r)\n",
    "\n",
    "# --- Extract AdaBoost-specific parameters ---\n",
    "ada_params = {\n",
    "    'n_estimators': best_params_ab_r.pop('n_estimators'),\n",
    "    'learning_rate': best_params_ab_r.pop('learning_rate'),\n",
    "    'random_state': SEED\n",
    "}\n",
    "\n",
    "# Base estimator for AdaBoost\n",
    "base_estimator = DecisionTreeRegressor(**best_params_ab_r, random_state=SEED)\n",
    "\n",
    "# Final AdaBoost Regressor wrapped with MultiOutputRegressor for 2D output\n",
    "ab_coordinate_final = MultiOutputRegressor(\n",
    "    AdaBoostRegressor(\n",
    "        estimator=base_estimator,\n",
    "        **ada_params\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- Cross-validation ---\n",
    "cv_scores_ab = cross_val_score(\n",
    "    ab_coordinate_final,\n",
    "    X_train2,\n",
    "    y_train2,\n",
    "    cv=cv,\n",
    "    scoring=euclidean_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"AdaBoost Cross-val error (per fold):\", -cv_scores_ab)\n",
    "print(\"AdaBoost Mean Cross-val error:\", -cv_scores_ab.mean())\n",
    "\n",
    "# --- Train on full training set and save model ---\n",
    "ab_coordinate_final.fit(X_train2, y_train2)\n",
    "joblib.dump(ab_coordinate_final, 'adaboost_coordinate_pred_UTS_reg.pkl')\n",
    "\n",
    "# Optional: Load the model back\n",
    "ab_loaded = joblib.load('adaboost_coordinate_pred_UTS_reg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3615101-b579-4630-9977-c3938667e7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "models2 = [\n",
    "        (\"KNN\", knn_coordinate_final),\n",
    "        (\"RF\", rf_coordinate_final),\n",
    "        (\"LGB\", lgb_coordinate_final),\n",
    "        (\"XGB\", xgb_coordinate_final),\n",
    "        (\"AB\", ab_coordinate_final)\n",
    "    ]\n",
    "\n",
    "accuracy_knn = cv_scores_knn.mean()\n",
    "accuracy_rf = cv_scores_rf.mean()\n",
    "accuracy_lgb = cv_scores_lgbm.mean()\n",
    "accuracy_xgb = cv_scores_xgb.mean()\n",
    "accuracy_ab = cv_scores_ab.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "295ddd6c-0a35-4698-a85b-f54b084305ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(ensemble, X, model_scores):\n",
    "    \"\"\"\n",
    "    Weighted ensemble prediction for regression.\n",
    "    Each model's predictions are weighted by its CV-based score.\n",
    "    \"\"\"\n",
    "    # Collect predictions from all models → shape: (n_models, n_samples, n_outputs)\n",
    "    preds = np.array([m.predict(X) for _, m in ensemble])\n",
    "\n",
    "    # Compute normalized weights based on model scores\n",
    "    weights = np.array([model_scores[name] for name, _ in ensemble])\n",
    "    weights = weights / weights.sum()\n",
    "\n",
    "    # Weighted average across models\n",
    "    weighted_preds = np.tensordot(weights, preds, axes=(0, 0))\n",
    "\n",
    "    return weighted_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5e7448-df3b-4b7c-882b-265dfe3ea9ff",
   "metadata": {},
   "source": [
    "# IEO-CVV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "13e5ff59-821b-4694-9e6a-30299b5417b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.base import clone\n",
    "import numpy as np\n",
    "\n",
    "def ieo_cvv_ensemble_selection_reg(models, X_train, y_train, X_test, y_test, cv=10, patience=3):\n",
    "    print(\"Initializing Iterative Ensemble Optimization with CV Voting (IEO-CVV) using Mean Euclidean Error...\")\n",
    "\n",
    "    def mean_euclidean_error(y_true, y_pred):\n",
    "        return np.mean(np.linalg.norm(y_true - y_pred, axis=1))\n",
    "\n",
    "    # --- Step 1: Cross-validated predictions and MEE scores ---\n",
    "    model_scores = {}\n",
    "    model_preds = {}\n",
    "\n",
    "    for name, model in models:\n",
    "        print(f\"Evaluating {name}...\")\n",
    "        y_pred_cv = cross_val_predict(model, X_train, y_train, cv=cv, n_jobs=-1)\n",
    "        mee = mean_euclidean_error(y_train, y_pred_cv)\n",
    "        model_scores[name] = -mee  # Negative so that higher is better\n",
    "        model_preds[name] = y_pred_cv\n",
    "        print(f\"{name} CV Mean Euclidean Error: {mee:.4f}\")\n",
    "\n",
    "    # Sort models by performance (lowest MEE first)\n",
    "    sorted_models = sorted(models, key=lambda m: model_scores[m[0]], reverse=True)\n",
    "\n",
    "    # --- Weighted average ensemble prediction (CV stage) ---\n",
    "    def ensemble_predict_cv(selected_model_names):\n",
    "        preds = np.array([model_preds[name] for name in selected_model_names])  # shape: (n_models, n_samples, n_outputs)\n",
    "        weights = np.array([model_scores[name] for name in selected_model_names])\n",
    "        weights = weights / weights.sum()\n",
    "        return np.tensordot(weights, preds, axes=(0, 0))\n",
    "\n",
    "    # --- Step 2: Initialize with top 2 models ---\n",
    "    ensemble = sorted_models[:2]\n",
    "    ensemble_names = [name for name, _ in ensemble]\n",
    "    y_ensemble_cv = ensemble_predict_cv(ensemble_names)\n",
    "    best_score = -mean_euclidean_error(y_train, y_ensemble_cv)\n",
    "    print(f\"Initial Ensemble: {ensemble_names} - Train MEE: {-best_score:.4f}\")\n",
    "\n",
    "    # --- Step 3: Iterative forward selection with backward pruning ---\n",
    "    patience_counter = 0\n",
    "    remaining_models = sorted_models[2:]\n",
    "\n",
    "    while remaining_models and patience_counter < patience:\n",
    "        improved = False\n",
    "\n",
    "        name, model = remaining_models.pop(0)\n",
    "        temp_ensemble = ensemble + [(name, model)]\n",
    "        temp_names = [n for n, _ in temp_ensemble]\n",
    "\n",
    "        y_pred_temp = ensemble_predict_cv(temp_names)\n",
    "        temp_score = -mean_euclidean_error(y_train, y_pred_temp)\n",
    "\n",
    "        if temp_score > best_score:\n",
    "            ensemble = temp_ensemble\n",
    "            best_score = temp_score\n",
    "            patience_counter = 0\n",
    "            print(f\"Added {name} - Improved MEE: {-temp_score:.4f}\")\n",
    "\n",
    "            # --- Backward pruning ---\n",
    "            pruned = True\n",
    "            while pruned and len(ensemble) > 1:\n",
    "                pruned = False\n",
    "                for n, _ in ensemble:\n",
    "                    if n == name:\n",
    "                        continue\n",
    "                    temp_pruned = [(x, y) for x, y in ensemble if x != n]\n",
    "                    temp_names_pruned = [x for x, _ in temp_pruned]\n",
    "                    y_pred_pruned = ensemble_predict_cv(temp_names_pruned)\n",
    "                    pruned_score = -mean_euclidean_error(y_train, y_pred_pruned)\n",
    "                    if pruned_score >= best_score:\n",
    "                        print(f\"Pruned {n} - MEE improved/maintained: {-pruned_score:.4f}\")\n",
    "                        ensemble = temp_pruned\n",
    "                        best_score = pruned_score\n",
    "                        pruned = True\n",
    "                        break\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Ignored {name} - No improvement: {-temp_score:.4f} (Patience: {patience_counter}/{patience})\")\n",
    "\n",
    "    # --- Step 4: Final weights based on CV performance ---\n",
    "    final_names = [n for n, _ in ensemble]\n",
    "    weights = {name: model_scores[name] for name in final_names}\n",
    "    total = sum(weights.values())\n",
    "    weights = {k: v / total for k, v in weights.items()}\n",
    "\n",
    "    print(\"Final Ensemble Members:\", final_names)\n",
    "    print(\"Final Ensemble Train MEE:\", -best_score)\n",
    "\n",
    "    # --- Step 5: Retrain models and evaluate on test set ---\n",
    "    print(\"\\nRetraining final models and evaluating on test set...\")\n",
    "    for name, model in ensemble:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    def ensemble_predict_test(selected_models, X):\n",
    "        preds = np.array([model.predict(X) for _, model in selected_models])\n",
    "        wts = np.array([model_scores[name] for name, _ in selected_models])\n",
    "        wts = wts / wts.sum()\n",
    "        return np.tensordot(wts, preds, axes=(0, 0))\n",
    "\n",
    "    y_pred_test = ensemble_predict_test(ensemble, X_test)\n",
    "    test_mee = mean_euclidean_error(y_test, y_pred_test)\n",
    "    print(\"Final Ensemble Test MEE:\", test_mee)\n",
    "\n",
    "    return ensemble, -best_score, weights, test_mee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fdf3333d-27d1-4348-9f50-d1beafb0c9ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Iterative Ensemble Optimization with CV Voting (IEO-CVV) using Mean Euclidean Error...\n",
      "Evaluating KNN...\n",
      "KNN CV Mean Euclidean Error: 0.2858\n",
      "Evaluating RF...\n",
      "RF CV Mean Euclidean Error: 1.1998\n",
      "Evaluating LGB...\n",
      "LGB CV Mean Euclidean Error: 0.8968\n",
      "Evaluating XGB...\n",
      "XGB CV Mean Euclidean Error: 1.0092\n",
      "Evaluating AB...\n",
      "AB CV Mean Euclidean Error: 7.4614\n",
      "Initial Ensemble: ['KNN', 'LGB'] - Train MEE: 0.7195\n",
      "Ignored XGB - No improvement: 0.7558 (Patience: 1/3)\n",
      "Ignored RF - No improvement: 0.8672 (Patience: 2/3)\n",
      "Ignored AB - No improvement: 6.4876 (Patience: 3/3)\n",
      "Final Ensemble Members: ['KNN', 'LGB']\n",
      "Final Ensemble Train MEE: 0.7195246302028794\n",
      "\n",
      "Retraining final models and evaluating on test set...\n",
      "Final Ensemble Test MEE: 0.6574462048339459\n",
      "Training time: 2348.71 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "# Run greedy ensemble selection for regression (using Mean Euclidean Distance)\n",
    "ensemble2, train_score2, ensemble_weights2, test_score2 = ieo_cvv_ensemble_selection_reg(\n",
    "    models2,\n",
    "    X_train2, y_train2,   # training data\n",
    "    X_test2, y_test2,     # test data\n",
    "    cv=10,                # cross-validation folds\n",
    ")\n",
    "\n",
    "end_time2 = time.time()\n",
    "\n",
    "training_time2 = end_time2 - start_time2\n",
    "print(f\"Training time: {training_time2:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1d4c58d3-6501-4029-b4dd-79a5ee519a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.17 seconds\n"
     ]
    }
   ],
   "source": [
    "# Predict target coordinates using the final ensemble\n",
    "start_time4 = time.time()\n",
    "\n",
    "y_target_predictions2 = ensemble_predict(ensemble2, X_target, ensemble_weights2)\n",
    "\n",
    "end_time4 = time.time()\n",
    "\n",
    "training_time4 = end_time4 - start_time4\n",
    "print(f\"Training time: {training_time4:.2f} seconds\")                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "22a08bac-6fa8-4921-a4d4-e3b4b3fd55c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Mean Euclidean Distance (Target set): 8.726189096540397\n"
     ]
    }
   ],
   "source": [
    "# Evaluate ensemble predictions on target set using Euclidean distance\n",
    "euclidean_errors = np.linalg.norm(y_target_coordinate - y_target_predictions2, axis=1)\n",
    "med_ensemble = euclidean_errors.mean()\n",
    "\n",
    "print(\"Ensemble Mean Euclidean Distance (Target set):\", med_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ff13535a-bd54-4d92-b219-e4f695643b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coordinate2 = pd.DataFrame(y_target_predictions2, columns=['x', 'y'])\n",
    "df_floor2 = pd.DataFrame(y_floor_predictions2, columns=['Floor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "da50c51a-370c-4ebf-8d74-152cedccaae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined2 = pd.concat([df_coordinate2, df_floor2], axis=1)\n",
    "df_combined2.to_excel('UTSIndoorLoc Prediction2_TPE-PS.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
